{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"spotoptim","text":""},{"location":"#surrogate-model-based-optimization","title":"Surrogate Model Based Optimization","text":"<ul> <li>Documentation for spotpython see Hyperparameter Tuning Cookbook, a guide for scikit-learn, PyTorch, river, and spotpython.</li> <li>News and updates related to spotpython see SPOTSeven</li> </ul>"},{"location":"about/","title":"Contact/Privacy Policy","text":""},{"location":"about/#address","title":"Address","text":"<p>Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de</p>"},{"location":"about/#privacy-policy","title":"Privacy Policy","text":"<p>We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject.</p> <p>The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled.</p> <p>As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone.</p> <ol> <li>Definitions</li> </ol> <p>The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used.</p> <p>In this data protection declaration, we use, inter alia, the following terms:</p> <p>a)    Personal data</p> <p>Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.</p> <p>b) Data subject</p> <p>Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing.</p> <p>c)    Processing</p> <p>Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>d)    Restriction of processing</p> <p>Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future.</p> <p>e)    Profiling</p> <p>Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.</p> <p>f)     Pseudonymisation</p> <p>Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person.</p> <p>g)    Controller or controller responsible for the processing</p> <p>Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law.</p> <p>h)    Processor</p> <p>Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller.</p> <p>i)      Recipient</p> <p>Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing.</p> <p>j)      Third party</p> <p>Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data.</p> <p>k)    Consent</p> <p>Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.</p> <ol> <li>Name and Address of the controller</li> </ol> <p>Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is:</p> <p>TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab</p> <p>Steinm\u00fcllerallee 1</p> <p>51643 Gummersbach</p> <p>Deutschland</p> <p>Phone: +49 2261 81966391</p> <p>Email: thomas.bartz-beielstein@th-koeln.de</p> <p>Website: www.spotseven.de</p> <ol> <li>Collection of general data and information</li> </ol> <p>The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems.</p> <p>When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject.</p> <ol> <li>Comments function in the blog on the website</li> </ol> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties.</p> <p>If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller.</p> <ol> <li>Routine erasure and blocking of personal data</li> </ol> <p>The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to.</p> <p>If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements.</p> <ol> <li>Rights of the data subject</li> </ol> <p>a) Right of confirmation</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>b) Right of access</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information:</p> <p>the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer.</p> <p>If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller.</p> <p>c) Right to rectification</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement.</p> <p>If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>d) Right to erasure (Right to be forgotten)</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary:</p> <p>The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately.</p> <p>Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases.</p> <p>e) Right of restriction of processing</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies:</p> <p>The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing.</p> <p>f) Right to data portability</p> <p>Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller.</p> <p>Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others.</p> <p>In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee.</p> <p>g) Right to object</p> <p>Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions.</p> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims.</p> <p>If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes.</p> <p>In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest.</p> <p>In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications.</p> <p>h) Automated individual decision-making, including profiling</p> <p>Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent.</p> <p>If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision.</p> <p>If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <p>i) Right to withdraw data protection consent</p> <p>Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time.</p> <p>f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <ol> <li>Data protection provisions about the application and use of Facebook</li> </ol> <p>On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network.</p> <p>A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests.</p> <p>The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland.</p> <p>With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data.</p> <p>Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made.</p> <p>The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook.</p> <ol> <li>Data protection provisions about the application and use of Google+</li> </ol> <p>On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests.</p> <p>The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/.</p> <p>If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject.</p> <p>If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services.</p> <p>Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button.</p> <p>If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website.</p> <p>Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy.</p> <ol> <li>Data protection provisions about the application and use of Jetpack for WordPress</li> </ol> <p>On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website.</p> <p>The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES.</p> <p>Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic.</p> <p>The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs.</p> <p>In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie.</p> <p>With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject.</p> <p>The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/.</p> <ol> <li>Data protection provisions about the application and use of LinkedIn</li> </ol> <p>The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world.</p> <p>The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data.</p> <p>LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made.</p> <p>LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy.</p> <ol> <li>Data protection provisions about the application and use of Twitter</li> </ol> <p>On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets.</p> <p>The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers.</p> <p>If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data.</p> <p>Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made.</p> <p>The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en.</p> <ol> <li>Data protection provisions about the application and use of YouTube</li> </ol> <p>On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal.</p> <p>The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject.</p> <p>YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made.</p> <p>YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google.</p> <ol> <li>Legal basis for the processing</li> </ol> <p>Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR).</p> <ol> <li>The legitimate interests pursued by the controller or by a third party</li> </ol> <p>Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders.</p> <ol> <li>Period for which the personal data will be stored</li> </ol> <p>The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract.</p> <ol> <li>Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data</li> </ol> <p>We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data.</p> <ol> <li>Existence of automated decision-making</li> </ol> <p>As a responsible company, we do not use automatic decision-making or profiling.</p> <p>This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.</p>"},{"location":"download/","title":"Install spotpython","text":"<pre><code>pip install spotoptim\n</code></pre>"},{"location":"examples/","title":"SPOT Examples","text":""},{"location":"examples/#simple-spotpython-run","title":"Simple spotpython run","text":"<pre><code>import numpy as np\nfrom spotpython.spot import spot\nfrom spotpython.fun.objectivefunctions import analytical\nfrom spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n\nfun = analytical().fun_branin\nfun_control = fun_control_init(lower = np.array([-5, 0]),\n                               upper = np.array([10, 15]),\n                               fun_evals=20)\ndesign_control = design_control_init(init_size=10)\nsurrogate_control = surrogate_control_init(n_theta=2)\nS = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\nS.run()\n</code></pre> <pre><code>spotpython tuning: 3.146824136952164 [######----] 55.00% \nspotpython tuning: 3.146824136952164 [######----] 60.00% \nspotpython tuning: 3.146824136952164 [######----] 65.00% \nspotpython tuning: 3.146824136952164 [#######---] 70.00% \nspotpython tuning: 1.1487233101571483 [########--] 75.00% \nspotpython tuning: 1.0236891516766402 [########--] 80.00% \nspotpython tuning: 0.41994270072214057 [########--] 85.00% \nspotpython tuning: 0.40193544341108023 [#########-] 90.00% \nspotpython tuning: 0.3991519598268951 [##########] 95.00% \nspotpython tuning: 0.3991519598268951 [##########] 100.00% Done...\n</code></pre> <pre><code>S.print_results()\n</code></pre> <pre><code>min y: 0.3991519598268951\nx0: 3.1546575195040987\nx1: 2.285931113926263\n</code></pre> <pre><code>S.plot_progress(log_y=True)\n</code></pre> <pre><code>S.surrogate.plot()\n</code></pre>"},{"location":"examples/#further-examples","title":"Further Examples","text":"<p>Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization.</p>"},{"location":"hyperparameter-tuning-cookbook/","title":"Hyperparameter Tuning Cookbook","text":"<p>The following is a cookbook of hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning.</p> <p>Hyperparameter Tuning Cookbook</p>"},{"location":"manuals/acquisition_failure/","title":"Acquisition Failure Handling in SpotOptim","text":"<p>SpotOptim provides sophisticated fallback strategies for handling acquisition function failures during optimization. This ensures robust optimization even when the surrogate model struggles to suggest new points.</p>"},{"location":"manuals/acquisition_failure/#what-is-acquisition-failure","title":"What is Acquisition Failure?","text":"<p>During surrogate-based optimization, the acquisition function suggests new points to evaluate. However, sometimes the suggested point is too close to existing points (within <code>tolerance_x</code> distance), which would provide little new information. When this happens, SpotOptim uses a fallback strategy to propose an alternative point.</p>"},{"location":"manuals/acquisition_failure/#fallback-strategies","title":"Fallback Strategies","text":"<p>SpotOptim supports two fallback strategies, controlled by the <code>acquisition_failure_strategy</code> parameter:</p>"},{"location":"manuals/acquisition_failure/#1-random-space-filling-design-default","title":"1. Random Space-Filling Design (Default)","text":"<p>Strategy name: <code>\"random\"</code></p> <p>This strategy uses Latin Hypercube Sampling (LHS) to generate a new space-filling point. LHS ensures good coverage of the search space by dividing each dimension into equal-probability intervals.</p> <p>When to use:</p> <ul> <li>General-purpose optimization</li> <li>When you want simplicity and good space-filling properties</li> <li>Default choice for most problems</li> </ul> <p>Example:</p> <pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\ndef sphere(X):\n    return np.sum(X**2, axis=1)\n\noptimizer = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=10,\n    acquisition_failure_strategy=\"random\",  # Default\n    verbose=True\n)\n\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/acquisition_failure/#2-morris-mitchell-minimizing-point","title":"2. Morris-Mitchell Minimizing Point","text":"<p>Strategy name: <code>\"mm\"</code></p> <p>This strategy finds a point that maximizes the minimum distance to all existing points. It evaluates 100 candidate points and selects the one with the largest minimum distance to the already-evaluated points, providing excellent space-filling properties.</p> <p>When to use:</p> <ul> <li>When you want to ensure maximum exploration</li> <li>For problems where avoiding clustering of points is critical</li> <li>When the search space has been heavily sampled in some regions</li> </ul> <p>Example:</p> <pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\ndef rosenbrock(X):\n    x = X[:, 0]\n    y = X[:, 1]\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\noptimizer = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=100,\n    n_initial=20,\n    acquisition_failure_strategy=\"mm\",  # Morris-Mitchell\n    verbose=True\n)\n\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/acquisition_failure/#how-it-works","title":"How It Works","text":"<p>The acquisition failure handling is integrated into the optimization process:</p> <ol> <li>Acquisition optimization: SpotOptim uses differential evolution to optimize the acquisition function</li> <li>Distance check: The proposed point is checked against existing points using <code>tolerance_x</code></li> <li>Fallback activation: If the point is too close, <code>_handle_acquisition_failure()</code> is called</li> <li>Strategy execution: The configured fallback strategy generates a new point</li> <li>Evaluation: The fallback point is evaluated and added to the dataset</li> </ol>"},{"location":"manuals/acquisition_failure/#comparison-of-strategies","title":"Comparison of Strategies","text":"Aspect Random (LHS) Morris-Mitchell Computation Very fast Moderate (100 candidates) Space-filling Good Excellent Exploration Balanced Maximum distance Clustering avoidance Good Best Recommended for General use Heavily sampled spaces"},{"location":"manuals/acquisition_failure/#complete-example-comparing-strategies","title":"Complete Example: Comparing Strategies","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef ackley(X):\n    \"\"\"Ackley function - multimodal test function\"\"\"\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    n = X.shape[1]\n\n    sum_sq = np.sum(X**2, axis=1)\n    sum_cos = np.sum(np.cos(c * X), axis=1)\n\n    return -a * np.exp(-b * np.sqrt(sum_sq / n)) - np.exp(sum_cos / n) + a + np.e\n\n# Test with random strategy\nprint(\"=\" * 60)\nprint(\"Testing with Random Space-Filling Strategy\")\nprint(\"=\" * 60)\n\nopt_random = SpotOptim(\n    fun=ackley,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=15,\n    acquisition_failure_strategy=\"random\",\n    tolerance_x=0.1,  # Relatively large tolerance to trigger failures\n    seed=42,\n    verbose=True\n)\n\nresult_random = opt_random.optimize()\n\nprint(f\"\\nRandom Strategy Results:\")\nprint(f\"  Best value: {result_random.fun:.6f}\")\nprint(f\"  Best point: {result_random.x}\")\nprint(f\"  Total evaluations: {result_random.nfev}\")\n\n# Test with Morris-Mitchell strategy\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Testing with Morris-Mitchell Strategy\")\nprint(\"=\" * 60)\n\nopt_mm = SpotOptim(\n    fun=ackley,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=15,\n    acquisition_failure_strategy=\"mm\",\n    tolerance_x=0.1,  # Same tolerance\n    seed=42,\n    verbose=True\n)\n\nresult_mm = opt_mm.optimize()\n\nprint(f\"\\nMorris-Mitchell Strategy Results:\")\nprint(f\"  Best value: {result_mm.fun:.6f}\")\nprint(f\"  Best point: {result_mm.x}\")\nprint(f\"  Total evaluations: {result_mm.nfev}\")\n\n# Compare\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Comparison\")\nprint(\"=\" * 60)\nprint(f\"Random strategy:        {result_random.fun:.6f}\")\nprint(f\"Morris-Mitchell strategy: {result_mm.fun:.6f}\")\nif result_random.fun &lt; result_mm.fun:\n    print(\"\u2192 Random strategy found better solution\")\nelse:\n    print(\"\u2192 Morris-Mitchell strategy found better solution\")\n</code></pre>"},{"location":"manuals/acquisition_failure/#advanced-usage-setting-tolerance","title":"Advanced Usage: Setting Tolerance","text":"<p>The <code>tolerance_x</code> parameter controls when the fallback strategy is triggered. A larger tolerance means points need to be farther apart, triggering the fallback more often:</p> <pre><code># Strict tolerance (smaller value) - fewer fallbacks\noptimizer_strict = SpotOptim(\n    fun=objective,\n    bounds=bounds,\n    tolerance_x=1e-6,  # Very small - almost never triggers fallback\n    acquisition_failure_strategy=\"mm\"\n)\n\n# Relaxed tolerance (larger value) - more fallbacks\noptimizer_relaxed = SpotOptim(\n    fun=objective,\n    bounds=bounds,\n    tolerance_x=0.5,  # Larger - triggers fallback more often\n    acquisition_failure_strategy=\"mm\"\n)\n</code></pre>"},{"location":"manuals/acquisition_failure/#best-practices","title":"Best Practices","text":""},{"location":"manuals/acquisition_failure/#1-use-random-for-most-problems","title":"1. Use Random for Most Problems","text":"<p>The random strategy (default) is sufficient for most optimization problems:</p> <pre><code>optimizer = SpotOptim(\n    fun=objective,\n    bounds=bounds,\n    acquisition_failure_strategy=\"random\"  # Good default choice\n)\n</code></pre>"},{"location":"manuals/acquisition_failure/#2-use-morris-mitchell-for-intensive-sampling","title":"2. Use Morris-Mitchell for Intensive Sampling","text":"<p>When you have a large budget and want maximum exploration:</p> <pre><code>optimizer = SpotOptim(\n    fun=expensive_objective,\n    bounds=bounds,\n    max_iter=200,  # Large budget\n    acquisition_failure_strategy=\"mm\"  # Maximize space coverage\n)\n</code></pre>"},{"location":"manuals/acquisition_failure/#3-monitor-fallback-activations","title":"3. Monitor Fallback Activations","text":"<p>Enable verbose mode to see when fallbacks are triggered:</p> <pre><code>optimizer = SpotOptim(\n    fun=objective,\n    bounds=bounds,\n    acquisition_failure_strategy=\"mm\",\n    verbose=True  # Shows fallback messages\n)\n</code></pre>"},{"location":"manuals/acquisition_failure/#4-adjust-tolerance-based-on-problem-scale","title":"4. Adjust Tolerance Based on Problem Scale","text":"<p>For problems with small search spaces, use smaller tolerance:</p> <pre><code># Small search space\noptimizer_small = SpotOptim(\n    fun=objective,\n    bounds=[(-1, 1), (-1, 1)],\n    tolerance_x=0.01,  # Small tolerance for small space\n    acquisition_failure_strategy=\"random\"\n)\n\n# Large search space\noptimizer_large = SpotOptim(\n    fun=objective,\n    bounds=[(-100, 100), (-100, 100)],\n    tolerance_x=1.0,  # Larger tolerance for large space\n    acquisition_failure_strategy=\"mm\"\n)\n</code></pre>"},{"location":"manuals/acquisition_failure/#technical-details","title":"Technical Details","text":""},{"location":"manuals/acquisition_failure/#morris-mitchell-implementation","title":"Morris-Mitchell Implementation","text":"<p>The Morris-Mitchell strategy:</p> <ol> <li>Generates 100 candidate points using Latin Hypercube Sampling</li> <li>For each candidate, calculates the minimum distance to all existing points</li> <li>Selects the candidate with the maximum minimum distance</li> </ol> <p>This ensures the new point is as far as possible from the densest region of evaluated points.</p>"},{"location":"manuals/acquisition_failure/#random-strategy-implementation","title":"Random Strategy Implementation","text":"<p>The random strategy:</p> <ol> <li>Generates a single point using Latin Hypercube Sampling</li> <li>Ensures the point is within bounds</li> <li>Applies variable type repairs (rounding for int/factor variables)</li> </ol> <p>This is computationally efficient while maintaining good space-filling properties.</p>"},{"location":"manuals/acquisition_failure/#summary","title":"Summary","text":"<ul> <li>Default strategy (<code>\"random\"</code>): Fast, good space-filling, suitable for most problems</li> <li>Morris-Mitchell (<code>\"mm\"</code>): Better space-filling, maximizes minimum distance, ideal for intensive sampling</li> <li>Trigger: Activated when acquisition-proposed point is too close to existing points (within <code>tolerance_x</code>)</li> <li>Control: Set via <code>acquisition_failure_strategy</code> parameter</li> <li>Monitoring: Enable <code>verbose=True</code> to see when fallbacks occur</li> </ul> <p>Choose the strategy that best matches your optimization goals: - Use <code>\"random\"</code> for general-purpose optimization - Use <code>\"mm\"</code> when you want maximum exploration and have a generous function evaluation budget</p>"},{"location":"manuals/diabetes_dataset/","title":"Diabetes Dataset Utilities","text":"<p>SpotOptim provides convenient utilities for working with the sklearn diabetes dataset, including PyTorch <code>Dataset</code> and <code>DataLoader</code> implementations. These utilities simplify data loading, preprocessing, and model training for regression tasks.</p>"},{"location":"manuals/diabetes_dataset/#overview","title":"Overview","text":"<p>The diabetes dataset contains 10 baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements) for 442 diabetes patients. The target is a quantitative measure of disease progression one year after baseline.</p> <p>Module: <code>spotoptim.data.diabetes</code></p> <p>Key Components:</p> <ul> <li><code>DiabetesDataset</code>: PyTorch Dataset class</li> <li><code>get_diabetes_dataloaders()</code>: Convenience function for complete data pipeline</li> </ul>"},{"location":"manuals/diabetes_dataset/#quick-start","title":"Quick Start","text":""},{"location":"manuals/diabetes_dataset/#basic-usage","title":"Basic Usage","text":"<pre><code>from spotoptim.data import get_diabetes_dataloaders\n\n# Load data with default settings\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders()\n\n# Iterate through batches\nfor batch_X, batch_y in train_loader:\n    print(f\"Batch features: {batch_X.shape}\")  # (32, 10)\n    print(f\"Batch targets: {batch_y.shape}\")   # (32, 1)\n    break\n</code></pre>"},{"location":"manuals/diabetes_dataset/#training-a-model","title":"Training a Model","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom spotoptim.data import get_diabetes_dataloaders\nfrom spotoptim.nn.linear_regressor import LinearRegressor\n\n# Load data\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    test_size=0.2,\n    batch_size=32,\n    scale_features=True,\n    random_state=42\n)\n\n# Create model\nmodel = LinearRegressor(\n    input_dim=10,\n    output_dim=1,\n    l1=64,\n    num_hidden_layers=2,\n    activation=\"ReLU\"\n)\n\n# Setup training\ncriterion = nn.MSELoss()\noptimizer = model.get_optimizer(\"Adam\", lr=0.01)\n\n# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for batch_X, batch_y in train_loader:\n        # Forward pass\n        predictions = model(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_train_loss:.4f}\")\n\n# Evaluation\nmodel.eval()\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for batch_X, batch_y in test_loader:\n        predictions = model(batch_X)\n        loss = criterion(predictions, batch_y)\n        test_loss += loss.item()\n\navg_test_loss = test_loss / len(test_loader)\nprint(f\"Test MSE: {avg_test_loss:.4f}\")\n</code></pre>"},{"location":"manuals/diabetes_dataset/#function-reference","title":"Function Reference","text":""},{"location":"manuals/diabetes_dataset/#get_diabetes_dataloaders","title":"get_diabetes_dataloaders()","text":"<p>Loads the sklearn diabetes dataset and returns configured PyTorch DataLoaders.</p> <p>Signature:</p> <pre><code>get_diabetes_dataloaders(\n    test_size=0.2,\n    batch_size=32,\n    shuffle_train=True,\n    shuffle_test=False,\n    random_state=42,\n    scale_features=True,\n    num_workers=0,\n    pin_memory=False\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>test_size</code> float 0.2 Proportion of dataset for testing (0.0 to 1.0) <code>batch_size</code> int 32 Number of samples per batch <code>shuffle_train</code> bool True Whether to shuffle training data <code>shuffle_test</code> bool False Whether to shuffle test data <code>random_state</code> int 42 Random seed for train/test split <code>scale_features</code> bool True Whether to standardize features <code>num_workers</code> int 0 Number of subprocesses for data loading <code>pin_memory</code> bool False Whether to pin memory (useful for GPU) <p>Returns:</p> <ul> <li><code>train_loader</code> (DataLoader): Training data loader</li> <li><code>test_loader</code> (DataLoader): Test data loader</li> <li><code>scaler</code> (StandardScaler or None): Fitted scaler if <code>scale_features=True</code>, else None</li> </ul> <p>Example:</p> <pre><code>from spotoptim.data import get_diabetes_dataloaders\n\n# Custom configuration\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    test_size=0.3,\n    batch_size=64,\n    shuffle_train=True,\n    scale_features=True,\n    random_state=123\n)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\nprint(f\"Scaler mean: {scaler.mean_[:3]}\")  # First 3 features\n</code></pre>"},{"location":"manuals/diabetes_dataset/#diabetesdataset-class","title":"DiabetesDataset Class","text":"<p>PyTorch Dataset implementation for the diabetes dataset.</p> <p>Signature:</p> <pre><code>DiabetesDataset(X, y, transform=None, target_transform=None)\n</code></pre> <p>Parameters:</p> <ul> <li><code>X</code> (np.ndarray): Feature matrix of shape (n_samples, n_features)</li> <li><code>y</code> (np.ndarray): Target values of shape (n_samples,) or (n_samples, 1)</li> <li><code>transform</code> (callable, optional): Transform to apply to features</li> <li><code>target_transform</code> (callable, optional): Transform to apply to targets</li> </ul> <p>Attributes:</p> <ul> <li><code>X</code> (torch.Tensor): Feature tensor (n_samples, n_features)</li> <li><code>y</code> (torch.Tensor): Target tensor (n_samples, 1)</li> <li><code>n_features</code> (int): Number of features (10 for diabetes)</li> <li><code>n_samples</code> (int): Number of samples</li> </ul> <p>Methods:</p> <ul> <li><code>__len__()</code>: Returns number of samples</li> <li><code>__getitem__(idx)</code>: Returns tuple (features, target) for given index</li> </ul>"},{"location":"manuals/diabetes_dataset/#manual-dataset-creation","title":"Manual Dataset Creation","text":"<pre><code>from spotoptim.data import DiabetesDataset\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader\n\n# Load raw data\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create datasets\ntrain_dataset = DiabetesDataset(X_train, y_train)\ntest_dataset = DiabetesDataset(X_test, y_test)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Inspect dataset\nprint(f\"Dataset size: {len(train_dataset)}\")\nprint(f\"Features shape: {train_dataset.X.shape}\")\nprint(f\"Targets shape: {train_dataset.y.shape}\")\n\n# Get a sample\nfeatures, target = train_dataset[0]\nprint(f\"Sample features: {features.shape}\")  # (10,)\nprint(f\"Sample target: {target.shape}\")      # (1,)\n</code></pre>"},{"location":"manuals/diabetes_dataset/#advanced-usage","title":"Advanced Usage","text":""},{"location":"manuals/diabetes_dataset/#custom-transforms","title":"Custom Transforms","text":"<pre><code>from spotoptim.data import DiabetesDataset\nfrom sklearn.datasets import load_diabetes\nimport torch\n\n# Define custom transforms\ndef add_noise(x):\n    \"\"\"Add Gaussian noise to features.\"\"\"\n    return x + torch.randn_like(x) * 0.01\n\ndef log_transform(y):\n    \"\"\"Apply log transform to target.\"\"\"\n    return torch.log1p(y)\n\n# Load data\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Create dataset with transforms\ndataset = DiabetesDataset(\n    X, y,\n    transform=add_noise,\n    target_transform=log_transform\n)\n\n# Transforms are applied when accessing items\nfeatures, target = dataset[0]\n</code></pre>"},{"location":"manuals/diabetes_dataset/#different-traintest-splits","title":"Different Train/Test Splits","text":"<pre><code>from spotoptim.data import get_diabetes_dataloaders\n\n# 70/30 split\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    test_size=0.3,\n    random_state=42\n)\nprint(f\"Training samples: {len(train_loader.dataset)}\")  # ~310\nprint(f\"Test samples: {len(test_loader.dataset)}\")       # ~132\n\n# 90/10 split\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    test_size=0.1,\n    random_state=42\n)\nprint(f\"Training samples: {len(train_loader.dataset)}\")  # ~398\nprint(f\"Test samples: {len(test_loader.dataset)}\")       # ~44\n</code></pre>"},{"location":"manuals/diabetes_dataset/#without-feature-scaling","title":"Without Feature Scaling","text":"<pre><code>from spotoptim.data import get_diabetes_dataloaders\n\n# Load without scaling (useful for tree-based models)\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    scale_features=False\n)\n\nprint(f\"Scaler: {scaler}\")  # None\n\n# Data is in original scale\nfor batch_X, batch_y in train_loader:\n    print(f\"Mean: {batch_X.mean(dim=0)[:3]}\")  # Non-zero values\n    break\n</code></pre>"},{"location":"manuals/diabetes_dataset/#larger-batch-sizes","title":"Larger Batch Sizes","text":"<pre><code>from spotoptim.data import get_diabetes_dataloaders\n\n# Larger batches for faster training (if memory allows)\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    batch_size=128\n)\nprint(f\"Batches per epoch: {len(train_loader)}\")  # Fewer batches\n\n# Smaller batches for more gradient updates\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    batch_size=8\n)\nprint(f\"Batches per epoch: {len(train_loader)}\")  # More batches\n</code></pre>"},{"location":"manuals/diabetes_dataset/#gpu-training-with-pin-memory","title":"GPU Training with Pin Memory","text":"<pre><code>import torch\nfrom spotoptim.data import get_diabetes_dataloaders\n\n# Enable pin_memory for faster GPU transfer\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    batch_size=32,\n    pin_memory=True  # Set to True when using GPU\n)\n\n# Move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Training loop with GPU\nfor batch_X, batch_y in train_loader:\n    # Data is already pinned, faster transfer to GPU\n    batch_X = batch_X.to(device, non_blocking=True)\n    batch_y = batch_y.to(device, non_blocking=True)\n\n    # ... training code ...\n</code></pre>"},{"location":"manuals/diabetes_dataset/#complete-training-example","title":"Complete Training Example","text":"<p>Here\u2019s a complete example showing data loading, model training, and evaluation:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom spotoptim.data import get_diabetes_dataloaders\nfrom spotoptim.nn.linear_regressor import LinearRegressor\n\ndef train_diabetes_model():\n    \"\"\"Train a neural network on the diabetes dataset.\"\"\"\n\n    # Load data\n    train_loader, test_loader, scaler = get_diabetes_dataloaders(\n        test_size=0.2,\n        batch_size=32,\n        scale_features=True,\n        random_state=42\n    )\n\n    # Create model\n    model = LinearRegressor(\n        input_dim=10,\n        output_dim=1,\n        l1=128,\n        num_hidden_layers=3,\n        activation=\"ReLU\"\n    )\n\n    # Setup training\n    criterion = nn.MSELoss()\n    optimizer = model.get_optimizer(\"Adam\", lr=0.001, weight_decay=1e-5)\n\n    # Training configuration\n    num_epochs = 200\n    best_test_loss = float('inf')\n\n    print(\"Starting training...\")\n    print(f\"Training samples: {len(train_loader.dataset)}\")\n    print(f\"Test samples: {len(test_loader.dataset)}\")\n    print(f\"Batches per epoch: {len(train_loader)}\")\n    print(\"-\" * 60)\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n\n        for batch_X, batch_y in train_loader:\n            # Forward pass\n            predictions = model(batch_X)\n            loss = criterion(predictions, batch_y)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n\n        # Evaluation phase\n        model.eval()\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for batch_X, batch_y in test_loader:\n                predictions = model(batch_X)\n                loss = criterion(predictions, batch_y)\n                test_loss += loss.item()\n\n        avg_test_loss = test_loss / len(test_loader)\n\n        # Track best model\n        if avg_test_loss &lt; best_test_loss:\n            best_test_loss = avg_test_loss\n            # Could save model here: torch.save(model.state_dict(), 'best_model.pt')\n\n        # Print progress\n        if (epoch + 1) % 20 == 0:\n            print(f\"Epoch {epoch+1:3d}/{num_epochs}: \"\n                  f\"Train Loss = {avg_train_loss:.4f}, \"\n                  f\"Test Loss = {avg_test_loss:.4f}\")\n\n    print(\"-\" * 60)\n    print(f\"Training complete!\")\n    print(f\"Best test loss: {best_test_loss:.4f}\")\n\n    return model, best_test_loss\n\n# Run training\nif __name__ == \"__main__\":\n    model, best_loss = train_diabetes_model()\n</code></pre>"},{"location":"manuals/diabetes_dataset/#integration-with-spotoptim","title":"Integration with SpotOptim","text":"<p>Use the diabetes dataset for hyperparameter optimization with SpotOptim:</p> <pre><code>import numpy as np\nimport torch\nimport torch.nn as nn\nfrom spotoptim import SpotOptim\nfrom spotoptim.data import get_diabetes_dataloaders\nfrom spotoptim.nn.linear_regressor import LinearRegressor\n\ndef evaluate_model(X):\n    \"\"\"Objective function for SpotOptim.\n\n    Args:\n        X: Array of hyperparameters [lr, l1, num_hidden_layers]\n\n    Returns:\n        Array of validation losses\n    \"\"\"\n    results = []\n\n    for params in X:\n        lr, l1, num_hidden_layers = params\n        lr = 10 ** lr  # Log scale for learning rate\n        l1 = int(l1)\n        num_hidden_layers = int(num_hidden_layers)\n\n        # Load data\n        train_loader, test_loader, _ = get_diabetes_dataloaders(\n            test_size=0.2,\n            batch_size=32,\n            random_state=42\n        )\n\n        # Create model\n        model = LinearRegressor(\n            input_dim=10,\n            output_dim=1,\n            l1=l1,\n            num_hidden_layers=num_hidden_layers,\n            activation=\"ReLU\"\n        )\n\n        # Train briefly\n        criterion = nn.MSELoss()\n        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n\n        num_epochs = 50\n        for epoch in range(num_epochs):\n            model.train()\n            for batch_X, batch_y in train_loader:\n                predictions = model(batch_X)\n                loss = criterion(predictions, batch_y)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        # Evaluate\n        model.eval()\n        test_loss = 0.0\n        with torch.no_grad():\n            for batch_X, batch_y in test_loader:\n                predictions = model(batch_X)\n                loss = criterion(predictions, batch_y)\n                test_loss += loss.item()\n\n        results.append(test_loss / len(test_loader))\n\n    return np.array(results)\n\n# Optimize hyperparameters\noptimizer = SpotOptim(\n    fun=evaluate_model,\n    bounds=[\n        (-4, -2),   # log10(lr): 0.0001 to 0.01\n        (16, 128),  # l1: number of neurons\n        (0, 4)      # num_hidden_layers\n    ],\n    var_type=[\"num\", \"int\", \"int\"],\n    max_iter=30,\n    n_initial=10,\n    seed=42,\n    verbose=True\n)\n\nresult = optimizer.optimize()\nprint(f\"Best hyperparameters found:\")\nprint(f\"  Learning rate: {10**result.x[0]:.6f}\")\nprint(f\"  Hidden neurons (l1): {int(result.x[1])}\")\nprint(f\"  Hidden layers: {int(result.x[2])}\")\nprint(f\"  Best MSE: {result.fun:.4f}\")\n</code></pre>"},{"location":"manuals/diabetes_dataset/#best-practices","title":"Best Practices","text":""},{"location":"manuals/diabetes_dataset/#1-always-use-feature-scaling","title":"1. Always Use Feature Scaling","text":"<pre><code># Good: Features are standardized\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    scale_features=True\n)\n</code></pre> <p>Neural networks typically perform better with normalized inputs.</p>"},{"location":"manuals/diabetes_dataset/#2-set-random-seeds-for-reproducibility","title":"2. Set Random Seeds for Reproducibility","text":"<pre><code># Reproducible train/test splits\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    random_state=42\n)\n\n# Also set PyTorch seed\nimport torch\ntorch.manual_seed(42)\n</code></pre>"},{"location":"manuals/diabetes_dataset/#3-dont-shuffle-test-data","title":"3. Don\u2019t Shuffle Test Data","text":"<pre><code># Good: Test data in consistent order\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    shuffle_train=True,   # Shuffle training data\n    shuffle_test=False    # Don't shuffle test data\n)\n</code></pre> <p>This ensures consistent evaluation metrics across runs.</p>"},{"location":"manuals/diabetes_dataset/#4-choose-appropriate-batch-size","title":"4. Choose Appropriate Batch Size","text":"<pre><code># Small dataset (442 samples) - moderate batch size works well\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    batch_size=32  # Good balance for this dataset\n)\n</code></pre> <p>Too large: Fewer gradient updates per epoch Too small: Noisy gradients, slower training</p>"},{"location":"manuals/diabetes_dataset/#5-save-the-scaler-for-production","title":"5. Save the Scaler for Production","text":"<pre><code>import pickle\nfrom spotoptim.data import get_diabetes_dataloaders\n\n# Train with scaling\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    scale_features=True\n)\n\n# Save scaler for production use\nwith open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Later: Load and use on new data\nwith open('scaler.pkl', 'rb') as f:\n    scaler = pickle.load(f)\n\nnew_data_scaled = scaler.transform(new_data)\n</code></pre>"},{"location":"manuals/diabetes_dataset/#troubleshooting","title":"Troubleshooting","text":""},{"location":"manuals/diabetes_dataset/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Solution: Reduce batch size or disable pin_memory</p> <pre><code>train_loader, test_loader, scaler = get_diabetes_dataloaders(\n    batch_size=16,      # Smaller batches\n    pin_memory=False    # Disable if not using GPU\n)\n</code></pre>"},{"location":"manuals/diabetes_dataset/#issue-different-data-ranges","title":"Issue: Different Data Ranges","text":"<p>Symptom: Model not converging, loss is NaN</p> <p>Solution: Ensure feature scaling is enabled</p> <pre><code>train_loader, test_loader, scaler = get_diabetes_dataloaders(\n    scale_features=True  # Must be True for neural networks\n)\n</code></pre>"},{"location":"manuals/diabetes_dataset/#issue-non-reproducible-results","title":"Issue: Non-Reproducible Results","text":"<p>Solution: Set all random seeds</p> <pre><code>import torch\nimport numpy as np\n\n# Set all seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ntrain_loader, test_loader, scaler = get_diabetes_dataloaders(\n    random_state=42,\n    shuffle_train=False  # Disable shuffle for full reproducibility\n)\n</code></pre>"},{"location":"manuals/diabetes_dataset/#issue-slow-data-loading","title":"Issue: Slow Data Loading","text":"<p>Solution: Use multiple workers (if not on Windows)</p> <pre><code>train_loader, test_loader, scaler = get_diabetes_dataloaders(\n    num_workers=4,      # Use 4 subprocesses\n    pin_memory=True     # Enable for GPU\n)\n</code></pre> <p>Note: On Windows, set <code>num_workers=0</code> to avoid multiprocessing issues.</p>"},{"location":"manuals/diabetes_dataset/#summary","title":"Summary","text":"<p>The diabetes dataset utilities in SpotOptim provide:</p> <ul> <li>Easy data loading: One function call gets complete data pipeline</li> <li>PyTorch integration: Native Dataset and DataLoader support</li> <li>Preprocessing included: Automatic feature scaling and train/test splitting</li> <li>Flexible configuration: Control batch size, splitting, scaling, and more</li> <li>Production ready: Save scalers and ensure reproducibility</li> </ul> <p>For more examples, see: - <code>examples/diabetes_dataset_example.py</code> - <code>notebooks/demos.ipynb</code> - Test suite: <code>tests/test_diabetes_dataset.py</code></p>"},{"location":"manuals/kriging/","title":"Kriging Surrogate Integration Summary","text":""},{"location":"manuals/kriging/#overview","title":"Overview","text":"<p>Implementation of a Kriging (Gaussian Process) surrogate model to SpotOptim, providing an alternative to scikit-learn\u2019s GaussianProcessRegressor.</p>"},{"location":"manuals/kriging/#module-structure","title":"Module Structure","text":"<pre><code>src/spotoptim/surrogate/\n\u251c\u2500\u2500 __init__.py          # Module exports\n\u251c\u2500\u2500 kriging.py           # Kriging implementation (~350 lines)\n\u2514\u2500\u2500 README.md            # Module documentation\n</code></pre>"},{"location":"manuals/kriging/#kriging-class-srcspotoptimsurrogatekrigingpy","title":"Kriging Class (<code>src/spotoptim/surrogate/kriging.py</code>)","text":"<p>Key Features:</p> <ul> <li>Scikit-learn compatible interface (<code>fit()</code>, <code>predict()</code>)</li> <li>Gaussian (RBF) kernel: R = exp(-D)</li> <li>Automatic hyperparameter optimization via maximum likelihood</li> <li>Cholesky decomposition for efficient linear algebra</li> <li>Prediction with uncertainty (<code>return_std=True</code>)</li> <li>Reproducible results via seed parameter</li> </ul> <p>Implementation Details:</p> <ul> <li>lean, well-documented code</li> <li>No external dependencies beyond NumPy, SciPy</li> <li>Simplified from spotpython.surrogate.kriging</li> <li>Focused on core functionality needed for SpotOptim</li> </ul> <p>Parameters:</p> <ul> <li><code>noise</code>: Regularization (nugget effect)</li> <li><code>kernel</code>: Currently \u2018gauss\u2019 (Gaussian/RBF)</li> <li><code>n_theta</code>: Number of length scale parameters</li> <li><code>min_theta</code>, <code>max_theta</code>: Bounds for hyperparameter optimization</li> <li><code>seed</code>: Random seed for reproducibility</li> </ul>"},{"location":"manuals/kriging/#integration-with-spotoptim","title":"Integration with SpotOptim","text":"<p>No Changes Required to SpotOptim Core!</p> <p>The existing <code>surrogate</code> parameter already supports any scikit-learn compatible model:</p> <pre><code>from spotoptim import SpotOptim, Kriging\n\nkriging = Kriging(seed=42)\noptimizer = SpotOptim(\n    fun=objective,\n    bounds=bounds,\n    surrogate=kriging,  # Just pass the Kriging instance\n    seed=42\n)\n</code></pre>"},{"location":"manuals/kriging/#documentation","title":"Documentation","text":"<p>Added Example  to <code>notebooks/demos.ipynb</code></p> <ul> <li>Demonstrates Kriging vs GP comparison</li> <li>Shows custom parameter usage</li> </ul>"},{"location":"manuals/kriging/#usage-examples","title":"Usage Examples","text":""},{"location":"manuals/kriging/#basic-usage","title":"Basic Usage","text":"<pre><code>from spotoptim import SpotOptim, Kriging\n\nkriging = Kriging(noise=1e-6, seed=42)\noptimizer = SpotOptim(fun=objective, bounds=bounds, surrogate=kriging)\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/kriging/#custom-parameters","title":"Custom Parameters","text":"<pre><code>kriging = Kriging(\n    noise=1e-4,\n    min_theta=-2.0,\n    max_theta=3.0,\n    seed=123\n)\n</code></pre>"},{"location":"manuals/kriging/#prediction-with-uncertainty","title":"Prediction with Uncertainty","text":"<pre><code>model = Kriging(seed=42)\nmodel.fit(X_train, y_train)\ny_pred, y_std = model.predict(X_test, return_std=True)\n</code></pre>"},{"location":"manuals/kriging/#technical-details","title":"Technical Details","text":""},{"location":"manuals/kriging/#kriging-vs-gaussianprocessregressor","title":"Kriging vs GaussianProcessRegressor","text":"Aspect Kriging GaussianProcessRegressor Lines of code ~350 Complex internal implementation Dependencies NumPy, SciPy scikit-learn + dependencies Kernel Gaussian (RBF) Multiple types (Matern, RQ, etc.) Hyperparameter opt Differential Evolution L-BFGS-B with restarts Use case Simplified, explicit Production, flexible"},{"location":"manuals/kriging/#algorithm","title":"Algorithm","text":"<ol> <li>Correlation Matrix:</li> </ol> <ul> <li>Compute squared distances: D_ij = \u03a3_k \u03b8_k(x_ik - x_jk)\u00b2</li> <li>Apply kernel: R_ij = exp(-D_ij)</li> <li>Add nugget: R_ii += noise</li> </ul> <ol> <li>Maximum Likelihood:</li> </ol> <ul> <li>Optimize \u03b8 via differential evolution</li> <li>Minimize: (n/2)log(\u03c3\u00b2) + (1/2)log|R|</li> <li>Concentrated likelihood (\u03bc profiled out)</li> </ul> <ol> <li>Prediction:</li> </ol> <ul> <li>Mean: f\u0302(x) = \u03bc\u0302 + \u03c8(x)\u1d40R\u207b\u00b9r</li> <li>Variance: s\u00b2(x) = \u03c3\u0302\u00b2[1 + \u03bb - \u03c8(x)\u1d40R\u207b\u00b9\u03c8(x)]</li> <li>Uses Cholesky decomposition for efficiency</li> </ul>"},{"location":"manuals/kriging/#key-arguments-passed-from-spotoptim","title":"Key Arguments Passed from SpotOptim","text":"<p>SpotOptim passes these to the surrogate via the standard interface:</p> <p>During fit:</p> <pre><code>surrogate.fit(X, y)\n</code></pre> <ul> <li><code>X</code>: Training points (n_initial or accumulated evaluations)</li> <li><code>y</code>: Function values</li> </ul> <p>During predict:</p> <pre><code>mu = surrogate.predict(x)[0]  # For acquisition='y'\nmu, sigma = surrogate.predict(x, return_std=True)  # For acquisition='ei', 'pi'\n</code></pre> <p>Implicit parameters via seed:</p> <ul> <li><code>random_state=seed</code> (for GaussianProcessRegressor)</li> <li><code>seed=seed</code> (for Kriging)</li> </ul>"},{"location":"manuals/kriging/#benefits","title":"Benefits","text":"<ol> <li>Self-contained: No heavy scikit-learn dependency for surrogate</li> <li>Explicit: Clear hyperparameter bounds and optimization</li> <li>Educational: Readable implementation of Kriging/GP</li> <li>Flexible: Easy to extend with new kernels or features</li> <li>Compatible: Works seamlessly with existing SpotOptim API</li> </ol>"},{"location":"manuals/kriging/#future-enhancements","title":"Future Enhancements","text":"<p>Potential additions:</p> <ul> <li>[ ] Additional kernels (Matern, Exponential, Cubic)</li> <li>[ ] Anisotropic hyperparameters (separate \u03b8 per dimension)</li> <li>[ ] Gradient-enhanced predictions</li> <li>[ ] Batch predictions for efficiency</li> <li>[ ] Parallel hyperparameter optimization</li> <li>[ ] ARD (Automatic Relevance Determination)</li> </ul>"},{"location":"manuals/kriging/#conclusion","title":"Conclusion","text":"<p>Implementation of a Kriging surrogate into SpotOptim with:</p> <ul> <li>\u2705 Full scikit-learn compatibility</li> <li>\u2705 Comprehensive test coverage (9 new tests)</li> <li>\u2705 Complete documentation</li> <li>\u2705 Example notebook</li> <li>\u2705 Zero breaking changes</li> <li>\u2705 All 25 tests passing</li> </ul>"},{"location":"manuals/multiobjective/","title":"Multi-Objective Optimization Support in SpotOptim","text":""},{"location":"manuals/multiobjective/#overview","title":"Overview","text":"<p>SpotOptim supports multi-objective optimization functions with automatic detection and flexible scalarization strategies. This implementation follows the same approach as the Spot class from spotPython.</p>"},{"location":"manuals/multiobjective/#what-was-implemented","title":"What Was Implemented","text":""},{"location":"manuals/multiobjective/#1-core-functionality","title":"1. Core Functionality","text":"<p>Parameter:</p> <ul> <li><code>fun_mo2so</code> (callable, optional): Function to convert multi-objective values to single-objective</li> <li>Takes array of shape <code>(n_samples, n_objectives)</code></li> <li>Returns array of shape <code>(n_samples,)</code></li> <li>If <code>None</code>, uses first objective (default behavior)</li> </ul> <p>Attribute:</p> <ul> <li><code>y_mo</code> (ndarray or None): Stores all multi-objective function values</li> <li>Shape: <code>(n_samples, n_objectives)</code> for multi-objective problems</li> <li><code>None</code> for single-objective problems</li> </ul> <p>Methods:</p> <ul> <li><code>_get_shape(y)</code>: Get shape of objective function output</li> <li><code>_store_mo(y_mo)</code>: Store multi-objective values with automatic appending</li> <li><code>_mo2so(y_mo)</code>: Convert multi-objective to single-objective values</li> </ul> <p>The method <code>_evaluate_function(X)</code> automatically detects multi-objective functions. It  calls <code>_mo2so()</code> to convert multi-objective to single-objective. It also stores the original multi-objective values in <code>y_mo</code>. And it returns single-objective values for optimization.</p>"},{"location":"manuals/multiobjective/#usage-examples","title":"Usage Examples","text":""},{"location":"manuals/multiobjective/#example-1-default-behavior-use-first-objective","title":"Example 1: Default Behavior (Use First Objective)","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef bi_objective(X):\n    \"\"\"Two conflicting objectives.\"\"\"\n    obj1 = np.sum(X**2, axis=1)          # Minimize at origin\n    obj2 = np.sum((X - 2)**2, axis=1)    # Minimize at (2, 2)\n    return np.column_stack([obj1, obj2])\n\noptimizer = SpotOptim(\n    fun=bi_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    n_initial=15,\n    seed=42\n)\n\nresult = optimizer.optimize()\n\nprint(f\"Best x: {result.x}\")                    # Near [0, 0]\nprint(f\"Best f(x): {result.fun}\")               # Minimizes obj1\nprint(f\"MO values stored: {optimizer.y_mo.shape}\")  # (30, 2)\n</code></pre>"},{"location":"manuals/multiobjective/#example-2-weighted-sum-scalarization","title":"Example 2: Weighted Sum Scalarization","text":"<pre><code>def weighted_sum(y_mo):\n    \"\"\"Equal weighting of objectives.\"\"\"\n    return 0.5 * y_mo[:, 0] + 0.5 * y_mo[:, 1]\n\noptimizer = SpotOptim(\n    fun=bi_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    n_initial=15,\n    fun_mo2so=weighted_sum,  # Custom conversion\n    seed=42\n)\n\nresult = optimizer.optimize()\nprint(f\"Compromise solution: {result.x}\")  # Near [1, 1]\n</code></pre>"},{"location":"manuals/multiobjective/#example-3-min-max-scalarization","title":"Example 3: Min-Max Scalarization","text":"<pre><code>def min_max(y_mo):\n    \"\"\"Minimize the maximum objective.\"\"\"\n    return np.max(y_mo, axis=1)\n\noptimizer = SpotOptim(\n    fun=bi_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    n_initial=15,\n    fun_mo2so=min_max,\n    seed=42\n)\n\nresult = optimizer.optimize()\n# Finds solution with balanced objective values\n</code></pre>"},{"location":"manuals/multiobjective/#example-4-three-or-more-objectives","title":"Example 4: Three or More Objectives","text":"<pre><code>def three_objectives(X):\n    \"\"\"Three different norms.\"\"\"\n    obj1 = np.sum(X**2, axis=1)           # L2 norm\n    obj2 = np.sum(np.abs(X), axis=1)      # L1 norm\n    obj3 = np.max(np.abs(X), axis=1)      # L-infinity norm\n    return np.column_stack([obj1, obj2, obj3])\n\ndef custom_scalarization(y_mo):\n    \"\"\"Weighted combination.\"\"\"\n    return 0.4 * y_mo[:, 0] + 0.3 * y_mo[:, 1] + 0.3 * y_mo[:, 2]\n\noptimizer = SpotOptim(\n    fun=three_objectives,\n    bounds=[(-5, 5), (-5, 5), (-5, 5)],\n    max_iter=35,\n    n_initial=20,\n    fun_mo2so=custom_scalarization,\n    seed=42\n)\n\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/multiobjective/#example-5-with-noise-handling","title":"Example 5: With Noise Handling","text":"<pre><code>def noisy_bi_objective(X):\n    \"\"\"Noisy multi-objective function.\"\"\"\n    noise1 = np.random.normal(0, 0.05, X.shape[0])\n    noise2 = np.random.normal(0, 0.05, X.shape[0])\n\n    obj1 = np.sum(X**2, axis=1) + noise1\n    obj2 = np.sum((X - 1)**2, axis=1) + noise2\n    return np.column_stack([obj1, obj2])\n\noptimizer = SpotOptim(\n    fun=noisy_bi_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=40,\n    n_initial=20,\n    repeats_initial=3,      # Handle noise\n    repeats_surrogate=2,\n    seed=42\n)\n\nresult = optimizer.optimize()\n# Works seamlessly with noise handling\n</code></pre>"},{"location":"manuals/multiobjective/#common-scalarization-strategies","title":"Common Scalarization Strategies","text":""},{"location":"manuals/multiobjective/#1-weighted-sum","title":"1. Weighted Sum","text":"<pre><code>def weighted_sum(y_mo, weights=[0.5, 0.5]):\n    return sum(w * y_mo[:, i] for i, w in enumerate(weights))\n</code></pre> <p>Use when: Objectives have similar scales and you want linear trade-offs</p>"},{"location":"manuals/multiobjective/#2-weighted-sum-with-normalization","title":"2. Weighted Sum with Normalization","text":"<pre><code>def normalized_weighted_sum(y_mo, weights=[0.5, 0.5]):\n    # Normalize each objective to [0, 1]\n    y_norm = (y_mo - y_mo.min(axis=0)) / (y_mo.max(axis=0) - y_mo.min(axis=0) + 1e-10)\n    return sum(w * y_norm[:, i] for i, w in enumerate(weights))\n</code></pre> <p>Use when: Objectives have very different scales</p>"},{"location":"manuals/multiobjective/#3-min-max-chebyshev","title":"3. Min-Max (Chebyshev)","text":"<pre><code>def min_max(y_mo):\n    return np.max(y_mo, axis=1)\n</code></pre> <p>Use when: You want balanced performance across all objectives</p>"},{"location":"manuals/multiobjective/#4-target-achievement","title":"4. Target Achievement","text":"<pre><code>def target_achievement(y_mo, targets=[0.0, 0.0]):\n    # Minimize deviation from targets\n    return np.sum((y_mo - targets)**2, axis=1)\n</code></pre> <p>Use when: You have specific target values for each objective</p>"},{"location":"manuals/multiobjective/#5-product","title":"5. Product","text":"<pre><code>def product(y_mo):\n    return np.prod(y_mo + 1e-10, axis=1)  # Add small value to avoid zero\n</code></pre> <p>Use when: All objectives should be minimized together</p>"},{"location":"manuals/multiobjective/#integration-with-other-features","title":"Integration with Other Features","text":"<p>Multi-objective support works seamlessly with:</p> <p>\u2705 Noise Handling - Use <code>repeats_initial</code> and <code>repeats_surrogate</code> \u2705 OCBA - Use <code>ocba_delta</code> for intelligent re-evaluation \u2705 TensorBoard Logging - Logs converted single-objective values \u2705 Dimension Reduction - Fixed dimensions work normally \u2705 Custom Variable Names - <code>var_name</code> parameter supported  </p>"},{"location":"manuals/multiobjective/#implementation-details","title":"Implementation Details","text":""},{"location":"manuals/multiobjective/#automatic-detection","title":"Automatic Detection","text":"<p>SpotOptim automatically detects multi-objective functions:</p> <ul> <li>If function returns 2D array (n_samples, n_objectives), it\u2019s multi-objective</li> <li>If function returns 1D array (n_samples,), it\u2019s single-objective</li> </ul>"},{"location":"manuals/multiobjective/#data-flow","title":"Data Flow","text":"<pre><code>User Function \u2192 y_mo (raw) \u2192 _mo2so() \u2192 y_ (single-objective)\n                    \u2193\n               y_mo (stored)\n</code></pre> <ol> <li>Function returns multi-objective values</li> <li><code>_store_mo()</code> saves them in <code>y_mo</code> attribute</li> <li><code>_mo2so()</code> converts to single-objective using <code>fun_mo2so</code> or default</li> <li>Surrogate model optimizes the single-objective values</li> <li>All original multi-objective values remain accessible in <code>y_mo</code></li> </ol>"},{"location":"manuals/multiobjective/#backward-compatibility","title":"Backward Compatibility","text":"<p>\u2705 Fully backward compatible:</p> <ul> <li>Single-objective functions work unchanged</li> <li><code>fun_mo2so</code> defaults to <code>None</code></li> <li><code>y_mo</code> is <code>None</code> for single-objective problems</li> <li>No breaking changes to existing code</li> </ul>"},{"location":"manuals/multiobjective/#limitations-and-notes","title":"Limitations and Notes","text":""},{"location":"manuals/multiobjective/#what-this-is","title":"What This Is","text":"<ul> <li>\u2705 Scalarization approach to multi-objective optimization</li> <li>\u2705 Single solution found per optimization run</li> <li>\u2705 Different scalarizations \u2192 different Pareto solutions</li> <li>\u2705 Suitable for preference-based multi-objective optimization</li> </ul>"},{"location":"manuals/multiobjective/#what-this-is-not","title":"What This Is Not","text":"<ul> <li>\u274c Not a true multi-objective optimizer (doesn\u2019t find Pareto front)</li> <li>\u274c Doesn\u2019t generate multiple solutions in one run</li> <li>\u274c Not suitable for discovering entire Pareto front</li> </ul>"},{"location":"manuals/multiobjective/#for-true-multi-objective-optimization","title":"For True Multi-Objective Optimization","text":"<p>For finding the complete Pareto front, consider specialized tools:</p> <ul> <li>pymoo: Comprehensive multi-objective optimization framework</li> <li>platypus: Multi-objective optimization library</li> <li>NSGA-II, MOEA/D: Dedicated multi-objective algorithms</li> </ul>"},{"location":"manuals/multiobjective/#demo-script","title":"Demo Script","text":"<p>Run the comprehensive demo (the demos files are located in the <code>examples</code> folder):</p> <pre><code>python demo_multiobjective.py\n</code></pre> <p>This demonstrates:</p> <ul> <li>Default behavior (first objective)</li> <li>Weighted sum scalarization</li> <li>Min-max scalarization</li> <li>Noisy multi-objective optimization</li> <li>Three-objective optimization</li> </ul>"},{"location":"manuals/multiobjective/#summary","title":"Summary","text":"<p>SpotOptim provides flexible multi-objective optimization support through:</p> <ul> <li>Automatic detection of multi-objective functions</li> <li>Customizable scalarization strategies via <code>fun_mo2so</code></li> <li>Complete storage of multi-objective values in <code>y_mo</code></li> <li>Full integration with existing features (noise, OCBA, TensorBoard, etc.)</li> <li>100% backward compatible with existing code</li> </ul> <p>This implementation mirrors the approach used in spotPython\u2019s Spot class, providing consistency across the ecosystem.</p>"},{"location":"manuals/plot_surrogate/","title":"Surrogate Model Visualization","text":"<p>This document describes the <code>plot_surrogate()</code> method added to the <code>SpotOptim</code> class, which provides visualization capabilities similar to the <code>plotkd()</code> function in the spotpython package.</p>"},{"location":"manuals/plot_surrogate/#overview","title":"Overview","text":"<p>The <code>plot_surrogate()</code> method creates a comprehensive 4-panel visualization of the fitted surrogate model, showing both predictions and uncertainty estimates across two selected dimensions.</p>"},{"location":"manuals/plot_surrogate/#features","title":"Features","text":"<ul> <li>3D Surface Plots: Visualize the surrogate\u2019s predictions and uncertainty as 3D surfaces</li> <li>Contour Plots: View 2D contours with overlaid evaluation points</li> <li>Multi-dimensional Support: Visualize any two dimensions of higher-dimensional problems</li> <li>Customizable Appearance: Control colors, resolution, transparency, and more</li> </ul>"},{"location":"manuals/plot_surrogate/#usage","title":"Usage","text":""},{"location":"manuals/plot_surrogate/#basic-usage","title":"Basic Usage","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\n# Define objective function\ndef sphere(X):\n    return np.sum(X**2, axis=1)\n\n# Run optimization\noptimizer = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)], max_iter=20)\nresult = optimizer.optimize()\n\n# Visualize the surrogate model\noptimizer.plot_surrogate(i=0, j=1, show=True)\n</code></pre>"},{"location":"manuals/plot_surrogate/#with-custom-parameters","title":"With Custom Parameters","text":"<pre><code>optimizer.plot_surrogate(\n    i=0,                          # First dimension to plot\n    j=1,                          # Second dimension to plot\n    var_name=['x1', 'x2'],        # Variable names for axes\n    add_points=True,              # Show evaluated points\n    cmap='viridis',               # Colormap\n    alpha=0.7,                    # Surface transparency\n    num=100,                      # Grid resolution\n    contour_levels=25,            # Number of contour levels\n    grid_visible=True,            # Show grid on contours\n    figsize=(12, 10),             # Figure size\n    show=True                     # Display immediately\n)\n</code></pre>"},{"location":"manuals/plot_surrogate/#higher-dimensional-problems","title":"Higher-Dimensional Problems","text":"<p>For problems with more than 2 dimensions, <code>plot_surrogate()</code> creates a 2D slice by fixing all other dimensions at their mean values:</p> <pre><code># 4D optimization problem\ndef sphere_4d(X):\n    return np.sum(X**2, axis=1)\n\nbounds = [(-3, 3)] * 4\noptimizer = SpotOptim(fun=sphere_4d, bounds=bounds, max_iter=20)\nresult = optimizer.optimize()\n\n# Visualize dimensions 0 and 2 (dimensions 1 and 3 fixed at mean)\noptimizer.plot_surrogate(\n    i=0, j=2,\n    var_name=['x0', 'x1', 'x2', 'x3']\n)\n\n# Visualize different dimension pair\noptimizer.plot_surrogate(i=1, j=3, var_name=['x0', 'x1', 'x2', 'x3'])\n</code></pre>"},{"location":"manuals/plot_surrogate/#plot-interpretation","title":"Plot Interpretation","text":"<p>The visualization consists of 4 panels:</p>"},{"location":"manuals/plot_surrogate/#top-left-prediction-surface","title":"Top Left: Prediction Surface","text":"<ul> <li>Shows the surrogate model\u2019s predicted function values as a 3D surface</li> <li>Helps understand the model\u2019s belief about the objective function landscape</li> <li>Lower values (blue in default colormap) indicate predicted minima</li> </ul>"},{"location":"manuals/plot_surrogate/#top-right-prediction-uncertainty-surface","title":"Top Right: Prediction Uncertainty Surface","text":"<ul> <li>Shows the standard deviation of predictions as a 3D surface</li> <li>Indicates where the model is uncertain and might benefit from more samples</li> <li>Lower values (blue) indicate high confidence, higher values (red) indicate uncertainty</li> </ul>"},{"location":"manuals/plot_surrogate/#bottom-left-prediction-contour-with-points","title":"Bottom Left: Prediction Contour with Points","text":"<ul> <li>2D contour plot of predictions</li> <li>Red dots show the actual points evaluated during optimization</li> <li>Useful for understanding the exploration-exploitation trade-off</li> </ul>"},{"location":"manuals/plot_surrogate/#bottom-right-uncertainty-contour-with-points","title":"Bottom Right: Uncertainty Contour with Points","text":"<ul> <li>2D contour plot of prediction uncertainty</li> <li>Shows how uncertainty decreases around evaluated points</li> <li>Helps identify unexplored regions</li> </ul>"},{"location":"manuals/plot_surrogate/#parameters","title":"Parameters","text":""},{"location":"manuals/plot_surrogate/#dimension-selection","title":"Dimension Selection","text":"<ul> <li><code>i</code> (int, default=0): Index of first dimension to plot</li> <li><code>j</code> (int, default=1): Index of second dimension to plot</li> </ul>"},{"location":"manuals/plot_surrogate/#appearance","title":"Appearance","text":"<ul> <li><code>var_name</code> (list of str, optional): Names for each dimension</li> <li><code>cmap</code> (str, default=\u2019jet\u2019): Matplotlib colormap name</li> <li><code>alpha</code> (float, default=0.8): Surface transparency (0=transparent, 1=opaque)</li> <li><code>figsize</code> (tuple, default=(12, 10)): Figure size in inches (width, height)</li> </ul>"},{"location":"manuals/plot_surrogate/#grid-and-resolution","title":"Grid and Resolution","text":"<ul> <li><code>num</code> (int, default=100): Number of grid points per dimension</li> <li><code>contour_levels</code> (int, default=30): Number of contour levels</li> <li><code>grid_visible</code> (bool, default=True): Show grid lines on contour plots</li> </ul>"},{"location":"manuals/plot_surrogate/#color-scaling","title":"Color Scaling","text":"<ul> <li><code>vmin</code> (float, optional): Minimum value for color scale</li> <li><code>vmax</code> (float, optional): Maximum value for color scale</li> </ul>"},{"location":"manuals/plot_surrogate/#display","title":"Display","text":"<ul> <li><code>show</code> (bool, default=True): Display plot immediately</li> <li><code>add_points</code> (bool, default=True): Overlay evaluated points on contours</li> </ul>"},{"location":"manuals/plot_surrogate/#examples","title":"Examples","text":""},{"location":"manuals/plot_surrogate/#example-1-2d-rosenbrock-function","title":"Example 1: 2D Rosenbrock Function","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef rosenbrock(X):\n    X = np.atleast_2d(X)\n    x, y = X[:, 0], X[:, 1]\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\noptimizer = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=30,\n    seed=42\n)\nresult = optimizer.optimize()\n\n# Visualize with custom colormap\noptimizer.plot_surrogate(\n    var_name=['x', 'y'],\n    cmap='coolwarm',\n    add_points=True\n)\n</code></pre>"},{"location":"manuals/plot_surrogate/#example-2-using-kriging-surrogate","title":"Example 2: Using Kriging Surrogate","text":"<pre><code>from spotoptim import SpotOptim, Kriging\n\ndef sphere(X):\n    return np.sum(X**2, axis=1)\n\noptimizer = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    surrogate=Kriging(seed=42),  # Use Kriging instead of GP\n    max_iter=20\n)\nresult = optimizer.optimize()\n\n# The plotting works the same with any surrogate\noptimizer.plot_surrogate(var_name=['x1', 'x2'])\n</code></pre>"},{"location":"manuals/plot_surrogate/#example-3-comparing-different-dimension-pairs","title":"Example 3: Comparing Different Dimension Pairs","text":"<pre><code># 3D problem - visualize all dimension pairs\ndef sphere_3d(X):\n    return np.sum(X**2, axis=1)\n\noptimizer = SpotOptim(\n    fun=sphere_3d,\n    bounds=[(-5, 5)] * 3,\n    max_iter=25\n)\nresult = optimizer.optimize()\n\n# Dimensions 0 vs 1\noptimizer.plot_surrogate(i=0, j=1, var_name=['x0', 'x1', 'x2'])\n\n# Dimensions 0 vs 2\noptimizer.plot_surrogate(i=0, j=2, var_name=['x0', 'x1', 'x2'])\n\n# Dimensions 1 vs 2\noptimizer.plot_surrogate(i=1, j=2, var_name=['x0', 'x1', 'x2'])\n</code></pre>"},{"location":"manuals/plot_surrogate/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li> <p>Run Optimization First: Always call <code>optimize()</code> before <code>plot_surrogate()</code></p> </li> <li> <p>Choose Dimensions Wisely: For high-dimensional problems, plot dimensions that you suspect are most important or interactive</p> </li> <li> <p>Adjust Resolution: Use lower <code>num</code> values (e.g., 50) for faster plotting, higher values (e.g., 200) for smoother surfaces</p> </li> <li> <p>Color Scales: Set <code>vmin</code> and <code>vmax</code> explicitly when comparing multiple plots to ensure consistent color scales</p> </li> <li> <p>Uncertainty Analysis: High uncertainty areas (bright colors in uncertainty plots) are good candidates for additional sampling</p> </li> <li> <p>Exploration vs Exploitation: Red dots clustered in low-prediction areas show exploitation; spread-out dots show exploration</p> </li> </ol>"},{"location":"manuals/plot_surrogate/#comparison-with-spotpythons-plotkd","title":"Comparison with spotpython\u2019s plotkd()","text":"<p>The <code>plot_surrogate()</code> method is inspired by spotpython\u2019s <code>plotkd()</code> function but adapted for SpotOptim\u2019s simplified interface:</p>"},{"location":"manuals/plot_surrogate/#similarities","title":"Similarities","text":"<ul> <li>Same 4-panel layout (2 surfaces + 2 contours)</li> <li>Visualizes predictions and uncertainty</li> <li>Supports dimension selection and customization</li> </ul>"},{"location":"manuals/plot_surrogate/#differences","title":"Differences","text":"<ul> <li>Integration: Method of SpotOptim class (no separate function needed)</li> <li>Simpler: Fewer parameters, more sensible defaults</li> <li>Automatic: Uses optimizer\u2019s bounds and data automatically</li> <li>Type Handling: Automatically applies variable type constraints (int/float/factor)</li> </ul>"},{"location":"manuals/plot_surrogate/#error-handling","title":"Error Handling","text":"<p>The method validates inputs and provides clear error messages:</p> <pre><code># Before optimization runs\noptimizer.plot_surrogate()  # ValueError: No optimization data available\n\n# Invalid dimension indices\noptimizer.plot_surrogate(i=5, j=1)  # ValueError: i must be less than n_dim\n\n# Same dimension twice\noptimizer.plot_surrogate(i=0, j=0)  # ValueError: i and j must be different\n</code></pre>"},{"location":"manuals/plot_surrogate/#see-also","title":"See Also","text":"<ul> <li><code>notebooks/demos.ipynb</code>: Example 4 demonstrates <code>plot_surrogate()</code></li> <li><code>examples/plot_surrogate_demo.py</code>: Standalone example script</li> <li><code>tests/test_plot_surrogate.py</code>: Comprehensive test suite</li> </ul>"},{"location":"manuals/point_selection/","title":"Point Selection Implementation","text":""},{"location":"manuals/point_selection/#overview","title":"Overview","text":"<p>This feature automatically selects a subset of evaluated points for surrogate model training when the total number of points exceeds a specified threshold.</p> <p>It is implemented as a point selection mechanism for SpotOptim that mirrors the functionality in spotpython\u2019s <code>Spot</code> class. </p>"},{"location":"manuals/point_selection/#implementation-details","title":"Implementation Details","text":""},{"location":"manuals/point_selection/#parameters","title":"Parameters","text":"<p>Added to <code>SpotOptim.__init__</code>:</p> <ul> <li><code>max_surrogate_points</code> (int, optional): Maximum number of points to use for surrogate fitting</li> <li><code>selection_method</code> (str, default=\u2019distant\u2019): Method for selecting points (\u2018distant\u2019 or \u2018best\u2019)</li> </ul>"},{"location":"manuals/point_selection/#methods","title":"Methods","text":"<ol> <li><code>_select_distant_points(X, y, k)</code></li> </ol> <ul> <li>Uses K-means clustering to find k clusters</li> <li>Selects the point closest to each cluster center</li> <li>Ensures space-filling properties for surrogate training</li> <li>Mimics <code>spotpython.utils.aggregate.select_distant_points</code></li> </ul> <ol> <li><code>_select_best_cluster(X, y, k)</code></li> </ol> <ul> <li>Uses K-means clustering to find k clusters</li> <li>Computes mean objective value for each cluster</li> <li>Selects all points from the cluster with the best (lowest) mean value</li> <li>Mimics <code>spotpython.utils.aggregate.select_best_cluster</code></li> </ul> <ol> <li><code>_selection_dispatcher(X, y)</code></li> </ol> <ul> <li>Dispatcher method that routes to the appropriate selection function</li> <li>Returns all points if <code>max_surrogate_points</code> is None</li> <li>Mimics <code>spotpython.spot.spot.Spot.selection_dispatcher</code></li> </ul> <p>The method <code>_fit_surrogate(X, y)</code> checks if <code>X.shape[0] &gt; self.max_surrogate_points</code>. If true, it calls <code>_selection_dispatcher</code> to get a subset. Then, it fits the surrogate only on the selected points.  This implementation matches the logic in <code>spotpython.spot.spot.Spot.fit_surrogate</code></p>"},{"location":"manuals/point_selection/#key-differences-from-spotpython","title":"Key Differences from spotpython","text":"<p>While the implementation follows spotpython\u2019s design, there is a difference: <code>spotoptim</code> uses a simplified clustering, it uses sklearn\u2019s KMeans directly instead of a custom implementation.</p>"},{"location":"manuals/point_selection/#example-usage","title":"Example Usage","text":"<pre><code>from spotoptim import SpotOptim\n\n# Without point selection (default behavior)\noptimizer1 = SpotOptim(\n    fun=expensive_function,\n    bounds=bounds,\n    max_iter=100,\n    n_initial=20\n)\n\n# With point selection using distant method\noptimizer2 = SpotOptim(\n    fun=expensive_function,\n    bounds=bounds,\n    max_iter=100,\n    n_initial=20,\n    max_surrogate_points=50,\n    selection_method='distant'\n)\n\n# With point selection using best cluster method\noptimizer3 = SpotOptim(\n    fun=expensive_function,\n    bounds=bounds,\n    max_iter=100,\n    n_initial=20,\n    max_surrogate_points=50,\n    selection_method='best'\n)\n</code></pre>"},{"location":"manuals/point_selection/#benefits","title":"Benefits","text":"<ol> <li>Scalability: Enables efficient optimization with many function evaluations</li> <li>Computational efficiency: Reduces surrogate training time for large datasets</li> <li>Maintained accuracy: Careful point selection preserves model quality</li> <li>Flexibility: Two selection methods for different optimization scenarios</li> </ol>"},{"location":"manuals/point_selection/#comparison-with-spotpython","title":"Comparison with spotpython","text":"Feature spotpython SpotOptim Point selection via clustering \u2713 \u2713 \u2018distant\u2019 method \u2713 \u2713 \u2018best\u2019 method \u2713 \u2713 Selection dispatcher \u2713 \u2713 Nystr\u00f6m approximation \u2713 \u2717 Modular design \u2713 (utils.aggregate) \u2713 (class methods)"},{"location":"manuals/point_selection/#references","title":"References","text":"<ul> <li>spotpython implementation: <code>src/spotpython/spot/spot.py</code> lines 1646-1778</li> <li>spotpython utilities: <code>src/spotpython/utils/aggregate.py</code> lines 262-336</li> </ul>"},{"location":"manuals/reproducibility/","title":"Reproducibility in SpotOptim","text":""},{"location":"manuals/reproducibility/#introduction","title":"Introduction","text":"<p>SpotOptim provides full support for reproducible optimization runs through the <code>seed</code> parameter. This is essential for:</p> <ul> <li>Scientific research: Ensuring experiments can be replicated</li> <li>Debugging: Reproducing specific optimization behaviors</li> <li>Benchmarking: Fair comparison between different configurations</li> <li>Production: Consistent results in deployed applications</li> </ul> <p>When you specify a seed, SpotOptim guarantees that running the same optimization multiple times will produce identical results. Without a seed, each run explores the search space differently, which can be useful for robustness testing.</p>"},{"location":"manuals/reproducibility/#basic-usage","title":"Basic Usage","text":""},{"location":"manuals/reproducibility/#making-optimization-reproducible","title":"Making Optimization Reproducible","text":"<p>To ensure reproducible results, simply specify the <code>seed</code> parameter when creating the optimizer:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef sphere(X):\n    \"\"\"Simple sphere function: f(x) = sum(x^2)\"\"\"\n    return np.sum(X**2, axis=1)\n\n# Reproducible optimization\noptimizer = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    n_initial=15,\n    seed=42,  # This ensures reproducibility\n    verbose=True\n)\n\nresult = optimizer.optimize()\nprint(f\"Best solution: {result.x}\")\nprint(f\"Best value: {result.fun}\")\n</code></pre> <p>Key Point: Running this code multiple times (even on different days or machines) will always produce the same result.</p>"},{"location":"manuals/reproducibility/#running-independent-experiments","title":"Running Independent Experiments","text":"<p>If you don\u2019t specify a seed, each optimization run will explore the search space differently:</p> <pre><code># Non-reproducible: different results each time\noptimizer = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    n_initial=15\n    # No seed specified\n)\n\nresult = optimizer.optimize()\n# Results will vary between runs\n</code></pre> <p>This is useful when you want to: - Explore different regions of the search space - Test the robustness of your results - Run multiple independent optimization attempts</p>"},{"location":"manuals/reproducibility/#practical-examples","title":"Practical Examples","text":""},{"location":"manuals/reproducibility/#example-1-comparing-different-configurations","title":"Example 1: Comparing Different Configurations","text":"<p>When comparing different optimizer settings, use the same seed for fair comparison:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef rosenbrock(X):\n    \"\"\"Rosenbrock function\"\"\"\n    x = X[:, 0]\n    y = X[:, 1]\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\n# Configuration 1: More initial points\nopt1 = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=50,\n    n_initial=20,\n    seed=42  # Same seed for fair comparison\n)\nresult1 = opt1.optimize()\n\n# Configuration 2: Fewer initial points, more iterations\nopt2 = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=50,\n    n_initial=10,\n    seed=42  # Same seed\n)\nresult2 = opt2.optimize()\n\nprint(f\"Config 1 (more initial): {result1.fun:.6f}\")\nprint(f\"Config 2 (fewer initial): {result2.fun:.6f}\")\n</code></pre>"},{"location":"manuals/reproducibility/#example-2-reproducible-research-experiment","title":"Example 2: Reproducible Research Experiment","text":"<p>For scientific papers or reports, always use a fixed seed and document it:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef rastrigin(X):\n    \"\"\"Rastrigin function (multimodal)\"\"\"\n    A = 10\n    n = X.shape[1]\n    return A * n + np.sum(X**2 - A * np.cos(2 * np.pi * X), axis=1)\n\n# Documented seed for reproducibility\nRANDOM_SEED = 12345\n\noptimizer = SpotOptim(\n    fun=rastrigin,\n    bounds=[(-5.12, 5.12), (-5.12, 5.12), (-5.12, 5.12)],\n    max_iter=100,\n    n_initial=30,\n    seed=RANDOM_SEED,\n    verbose=True\n)\n\nresult = optimizer.optimize()\n\nprint(f\"\\nExperiment Results (seed={RANDOM_SEED}):\")\nprint(f\"Best solution: {result.x}\")\nprint(f\"Best value: {result.fun}\")\nprint(f\"Iterations: {result.nit}\")\nprint(f\"Function evaluations: {result.nfev}\")\n\n# These results can now be cited in a paper\n</code></pre>"},{"location":"manuals/reproducibility/#example-3-multiple-independent-runs","title":"Example 3: Multiple Independent Runs","text":"<p>To test robustness, run the same optimization with different seeds:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef ackley(X):\n    \"\"\"Ackley function\"\"\"\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    n = X.shape[1]\n\n    sum_sq = np.sum(X**2, axis=1)\n    sum_cos = np.sum(np.cos(c * X), axis=1)\n\n    return -a * np.exp(-b * np.sqrt(sum_sq / n)) - np.exp(sum_cos / n) + a + np.e\n\n# Run 5 independent optimizations\nresults = []\nseeds = [42, 123, 456, 789, 1011]\n\nfor seed in seeds:\n    optimizer = SpotOptim(\n        fun=ackley,\n        bounds=[(-5, 5), (-5, 5)],\n        max_iter=40,\n        n_initial=20,\n        seed=seed,\n        verbose=False\n    )\n    result = optimizer.optimize()\n    results.append(result.fun)\n    print(f\"Run with seed {seed:4d}: f(x) = {result.fun:.6f}\")\n\n# Analyze robustness\nprint(f\"\\nBest result: {min(results):.6f}\")\nprint(f\"Worst result: {max(results):.6f}\")\nprint(f\"Mean: {np.mean(results):.6f}\")\nprint(f\"Std dev: {np.std(results):.6f}\")\n</code></pre>"},{"location":"manuals/reproducibility/#example-4-reproducible-initial-design","title":"Example 4: Reproducible Initial Design","text":"<p>The seed ensures that even the initial design points are reproducible:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef simple_quadratic(X):\n    return np.sum((X - 1)**2, axis=1)\n\n# Create two optimizers with same seed\nopt1 = SpotOptim(\n    fun=simple_quadratic,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=25,\n    n_initial=10,\n    seed=999\n)\n\nopt2 = SpotOptim(\n    fun=simple_quadratic,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=25,\n    n_initial=10,\n    seed=999  # Same seed\n)\n\n# Run both optimizations\nresult1 = opt1.optimize()\nresult2 = opt2.optimize()\n\n# Verify identical results\nprint(\"Initial design points are identical:\", \n      np.allclose(opt1.X_[:10], opt2.X_[:10]))\nprint(\"All evaluated points are identical:\", \n      np.allclose(opt1.X_, opt2.X_))\nprint(\"All function values are identical:\", \n      np.allclose(opt1.y_, opt2.y_))\nprint(\"Best solutions are identical:\", \n      np.allclose(result1.x, result2.x))\n</code></pre>"},{"location":"manuals/reproducibility/#example-5-custom-initial-design-with-seed","title":"Example 5: Custom Initial Design with Seed","text":"<p>Even when providing a custom initial design, the seed ensures reproducible subsequent iterations:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef beale(X):\n    \"\"\"Beale function\"\"\"\n    x = X[:, 0]\n    y = X[:, 1]\n    term1 = (1.5 - x + x * y)**2\n    term2 = (2.25 - x + x * y**2)**2\n    term3 = (2.625 - x + x * y**3)**2\n    return term1 + term2 + term3\n\n# Custom initial design (e.g., from previous knowledge)\nX_start = np.array([\n    [0.0, 0.0],\n    [1.0, 1.0],\n    [2.0, 2.0],\n    [-1.0, -1.0]\n])\n\n# Run twice with same seed and initial design\nopt1 = SpotOptim(\n    fun=beale,\n    bounds=[(-4.5, 4.5), (-4.5, 4.5)],\n    max_iter=30,\n    n_initial=10,\n    seed=777\n)\nresult1 = opt1.optimize(X0=X_start)\n\nopt2 = SpotOptim(\n    fun=beale,\n    bounds=[(-4.5, 4.5), (-4.5, 4.5)],\n    max_iter=30,\n    n_initial=10,\n    seed=777  # Same seed\n)\nresult2 = opt2.optimize(X0=X_start)\n\nprint(\"Results are identical:\", np.allclose(result1.x, result2.x))\nprint(f\"Best value: {result1.fun:.6f}\")\n</code></pre>"},{"location":"manuals/reproducibility/#advanced-topics","title":"Advanced Topics","text":""},{"location":"manuals/reproducibility/#seed-and-noisy-functions","title":"Seed and Noisy Functions","text":"<p>When optimizing noisy functions with repeated evaluations, the seed ensures reproducible noise:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef noisy_sphere(X):\n    \"\"\"Sphere function with Gaussian noise\"\"\"\n    base = np.sum(X**2, axis=1)\n    noise = np.random.normal(0, 0.1, size=base.shape)\n    return base + noise\n\noptimizer = SpotOptim(\n    fun=noisy_sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=40,\n    n_initial=20,\n    repeats_initial=3,  # 3 evaluations per point\n    repeats_surrogate=2,\n    seed=42  # Ensures same noise pattern\n)\n\nresult = optimizer.optimize()\nprint(f\"Best mean value: {optimizer.min_mean_y:.6f}\")\nprint(f\"Variance at best: {optimizer.min_var_y:.6f}\")\n</code></pre> <p>Important: With the same seed, even the noise will be identical across runs!</p>"},{"location":"manuals/reproducibility/#different-seeds-for-different-exploration","title":"Different Seeds for Different Exploration","text":"<p>Use different seeds to explore different regions systematically:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef griewank(X):\n    \"\"\"Griewank function\"\"\"\n    sum_sq = np.sum(X**2 / 4000, axis=1)\n    prod_cos = np.prod(np.cos(X / np.sqrt(np.arange(1, X.shape[1] + 1))), axis=1)\n    return sum_sq - prod_cos + 1\n\n# Systematic exploration with different seeds\nbest_overall = float('inf')\nbest_seed = None\n\nfor seed in range(10, 20):  # Seeds 10-19\n    optimizer = SpotOptim(\n        fun=griewank,\n        bounds=[(-600, 600), (-600, 600)],\n        max_iter=50,\n        n_initial=25,\n        seed=seed\n    )\n    result = optimizer.optimize()\n\n    if result.fun &lt; best_overall:\n        best_overall = result.fun\n        best_seed = seed\n\n    print(f\"Seed {seed}: f(x) = {result.fun:.6f}\")\n\nprint(f\"\\nBest result with seed {best_seed}: {best_overall:.6f}\")\n</code></pre>"},{"location":"manuals/reproducibility/#best-practices","title":"Best Practices","text":""},{"location":"manuals/reproducibility/#1-always-use-seeds-for-production-code","title":"1. Always Use Seeds for Production Code","text":"<pre><code># Good: Reproducible\noptimizer = SpotOptim(fun=objective, bounds=bounds, seed=42)\n\n# Risky: Non-reproducible\noptimizer = SpotOptim(fun=objective, bounds=bounds)\n</code></pre>"},{"location":"manuals/reproducibility/#2-document-your-seeds","title":"2. Document Your Seeds","text":"<pre><code># Configuration for experiment reported in Section 4.2\nEXPERIMENT_SEED = 2024\nMAX_ITERATIONS = 100\n\noptimizer = SpotOptim(\n    fun=my_objective,\n    bounds=my_bounds,\n    max_iter=MAX_ITERATIONS,\n    seed=EXPERIMENT_SEED\n)\n</code></pre>"},{"location":"manuals/reproducibility/#3-use-different-seeds-for-different-experiments","title":"3. Use Different Seeds for Different Experiments","text":"<pre><code># Different experiments should use different seeds\nBASELINE_SEED = 100\nEXPERIMENT_A_SEED = 200\nEXPERIMENT_B_SEED = 300\n</code></pre>"},{"location":"manuals/reproducibility/#4-test-robustness-across-multiple-seeds","title":"4. Test Robustness Across Multiple Seeds","text":"<pre><code># Run same optimization with multiple seeds\nfor seed in [42, 123, 456, 789, 1011]:\n    optimizer = SpotOptim(fun=objective, bounds=bounds, seed=seed)\n    result = optimizer.optimize()\n    # Analyze results\n</code></pre>"},{"location":"manuals/reproducibility/#what-the-seed-controls","title":"What the Seed Controls","text":"<p>The <code>seed</code> parameter ensures reproducibility by controlling:</p> <ol> <li>Initial Design Generation: Latin Hypercube Sampling produces the same initial points</li> <li>Surrogate Model: Gaussian Process random initialization is identical</li> <li>Acquisition Optimization: Differential evolution explores the same candidates</li> <li>Random Sampling: Any random exploration uses the same random numbers</li> </ol> <p>This guarantees that the entire optimization pipeline is deterministic and reproducible.</p>"},{"location":"manuals/reproducibility/#common-questions","title":"Common Questions","text":"<p>Q: Can I use seed=0? A: Yes, any integer (including 0) is a valid seed.</p> <p>Q: Will different Python versions give the same results? A: Generally yes, but minor numerical differences may occur due to underlying library changes. Use the same environment for exact reproducibility.</p> <p>Q: Does the seed affect the objective function? A: No, the seed only affects SpotOptim\u2019s internal random processes. If your objective function has its own randomness, you\u2019ll need to control that separately.</p> <p>Q: How do I choose a good seed value? A: Any integer works. Common choices are 42, 123, or dates (e.g., 20241112). What matters is consistency, not the specific value.</p>"},{"location":"manuals/reproducibility/#summary","title":"Summary","text":"<ul> <li>Use <code>seed</code> parameter for reproducible optimization</li> <li>Same seed \u2192 identical results (every time)</li> <li>No seed \u2192 different results (random exploration)  </li> <li>Essential for research, debugging, and production</li> <li>Document your seeds for transparency</li> <li>Test robustness with multiple different seeds</li> </ul>"},{"location":"manuals/save_load/","title":"Save and Load in SpotOptim","text":"<p>SpotOptim provides comprehensive save and load functionality for serializing optimization configurations and results. This enables distributed workflows where experiments are defined locally, executed remotely, and analyzed back on the local machine.</p>"},{"location":"manuals/save_load/#key-concepts","title":"Key Concepts","text":""},{"location":"manuals/save_load/#experiments-vs-results","title":"Experiments vs Results","text":"<p>SpotOptim distinguishes between two types of saved data:</p> <ul> <li>Experiment (<code>*_exp.pkl</code>): Configuration only, excluding the objective function and results. Used to transfer optimization setup to remote machines.</li> <li>Result (<code>*_res.pkl</code>): Complete optimization state including configuration, all evaluations, and results. Used to save and analyze completed optimizations.</li> </ul>"},{"location":"manuals/save_load/#what-gets-saved","title":"What Gets Saved","text":"Component Experiment Result Configuration (bounds, parameters) \u2713 \u2713 Objective function \u2717 \u2717 Evaluations (X, y) \u2717 \u2713 Best solution \u2717 \u2713 Surrogate model Excluded* \u2713 TensorBoard writer \u2717 \u2717 <p>*Surrogate model is excluded from experiments and automatically recreated when loaded.</p>"},{"location":"manuals/save_load/#quick-start","title":"Quick Start","text":""},{"location":"manuals/save_load/#basic-save-and-load","title":"Basic Save and Load","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef sphere(X):\n    \"\"\"Simple sphere function\"\"\"\n    return np.sum(X**2, axis=1)\n\n# Create and configure optimizer\noptimizer = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=15,\n    seed=42\n)\n\n# Run optimization\nresult = optimizer.optimize()\nprint(f\"Best value: {result.fun:.6f}\")\n\n# Save complete results\noptimizer.save_result(prefix=\"sphere_opt\")\n# Creates: sphere_opt_res.pkl\n\n# Later: load and analyze results\nloaded_opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\nprint(f\"Loaded best value: {loaded_opt.best_y_:.6f}\")\nprint(f\"Total evaluations: {loaded_opt.counter}\")\n</code></pre>"},{"location":"manuals/save_load/#distributed-workflow","title":"Distributed Workflow","text":"<p>The save/load functionality enables a powerful workflow for distributed optimization:</p>"},{"location":"manuals/save_load/#step-1-define-experiment-locally","title":"Step 1: Define Experiment Locally","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\n# Define configuration locally (no need to run optimization yet)\noptimizer = SpotOptim(\n    bounds=[(-10, 10), (-10, 10), (-10, 10)],\n    max_iter=200,\n    n_initial=30,\n    seed=42,\n    verbose=True\n)\n\n# Save experiment configuration\noptimizer.save_experiment(prefix=\"remote_job_001\")\n# Creates: remote_job_001_exp.pkl\n\nprint(\"Experiment saved. Transfer remote_job_001_exp.pkl to remote machine.\")\n</code></pre>"},{"location":"manuals/save_load/#step-2-execute-on-remote-machine","title":"Step 2: Execute on Remote Machine","text":"<pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\n# Define objective function on remote machine\ndef expensive_function(X):\n    \"\"\"Expensive simulation or computation\"\"\"\n    # Your expensive computation here\n    return np.sum(X**2, axis=1) + 0.1 * np.sum(np.sin(10 * X), axis=1)\n\n# Load experiment configuration\noptimizer = SpotOptim.load_experiment(\"remote_job_001_exp.pkl\")\nprint(\"Experiment loaded successfully\")\n\n# Attach objective function (must be done after loading)\noptimizer.fun = expensive_function\n\n# Run optimization\nresult = optimizer.optimize()\nprint(f\"Optimization complete. Best value: {result.fun:.6f}\")\n\n# Save results\noptimizer.save_result(prefix=\"remote_job_001\")\n# Creates: remote_job_001_res.pkl\n\nprint(\"Results saved. Transfer remote_job_001_res.pkl back to local machine.\")\n</code></pre>"},{"location":"manuals/save_load/#step-3-analyze-results-locally","title":"Step 3: Analyze Results Locally","text":"<pre><code>from spotoptim import SpotOptim\nimport matplotlib.pyplot as plt\n\n# Load results from remote execution\noptimizer = SpotOptim.load_result(\"remote_job_001_res.pkl\")\n\n# Access all optimization data\nprint(f\"Best value found: {optimizer.best_y_:.6f}\")\nprint(f\"Best point: {optimizer.best_x_}\")\nprint(f\"Total evaluations: {optimizer.counter}\")\nprint(f\"Number of iterations: {optimizer.n_iter_}\")\n\n# Analyze convergence\nplt.figure(figsize=(10, 6))\nplt.plot(optimizer.y_, 'o-', alpha=0.6, label='Evaluations')\nplt.plot(range(len(optimizer.y_)), \n         [optimizer.y_[:i+1].min() for i in range(len(optimizer.y_))],\n         'r-', linewidth=2, label='Best so far')\nplt.xlabel('Iteration')\nplt.ylabel('Objective Value')\nplt.title('Optimization Progress')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Access all evaluated points\nprint(f\"\\nAll evaluated points shape: {optimizer.X_.shape}\")\nprint(f\"All objective values shape: {optimizer.y_.shape}\")\n</code></pre>"},{"location":"manuals/save_load/#advanced-usage","title":"Advanced Usage","text":""},{"location":"manuals/save_load/#custom-filenames-and-paths","title":"Custom Filenames and Paths","text":"<pre><code>import os\nfrom spotoptim import SpotOptim\nimport numpy as np\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\noptimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    seed=42\n)\n\n# Save with custom filename\noptimizer.save_experiment(\n    filename=\"custom_name.pkl\",\n    verbosity=1\n)\n\n# Save to specific directory\nos.makedirs(\"experiments/batch_001\", exist_ok=True)\noptimizer.save_experiment(\n    prefix=\"exp_001\",\n    path=\"experiments/batch_001\",\n    verbosity=1\n)\n# Creates: experiments/batch_001/exp_001_exp.pkl\n</code></pre>"},{"location":"manuals/save_load/#overwrite-protection","title":"Overwrite Protection","text":"<pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\ndef sphere(X):\n    return np.sum(X**2, axis=1)\n\noptimizer = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)], max_iter=20)\nresult = optimizer.optimize()\n\n# First save\noptimizer.save_result(prefix=\"my_result\")\n\n# Try to save again - raises FileExistsError by default\ntry:\n    optimizer.save_result(prefix=\"my_result\")\nexcept FileExistsError as e:\n    print(f\"File already exists: {e}\")\n\n# Explicitly allow overwriting\noptimizer.save_result(prefix=\"my_result\", overwrite=True)\nprint(\"File overwritten successfully\")\n</code></pre>"},{"location":"manuals/save_load/#loading-and-continuing-optimization","title":"Loading and Continuing Optimization","text":"<pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Initial optimization\nopt1 = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    seed=42\n)\nresult1 = opt1.optimize()\nopt1.save_result(prefix=\"checkpoint\")\n\nprint(f\"Initial optimization: {result1.nfev} evaluations, best={result1.fun:.6f}\")\n\n# Load and continue\nopt2 = SpotOptim.load_result(\"checkpoint_res.pkl\")\nopt2.fun = objective  # Re-attach function\nopt2.max_iter = 50  # Increase budget\n\n# Continue optimization\nresult2 = opt2.optimize()\nprint(f\"After continuation: {result2.nfev} evaluations, best={result2.fun:.6f}\")\n</code></pre>"},{"location":"manuals/save_load/#working-with-noisy-functions","title":"Working with Noisy Functions","text":"<p>Save and load preserves noise statistics for reproducible analysis:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef noisy_objective(X):\n    \"\"\"Objective with measurement noise\"\"\"\n    true_value = np.sum(X**2, axis=1)\n    noise = np.random.normal(0, 0.1, X.shape[0])\n    return true_value + noise\n\n# Optimize noisy function with repeated evaluations\noptimizer = SpotOptim(\n    fun=noisy_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=40,\n    n_initial=15,\n    repeats_initial=3,    # Repeat initial points\n    repeats_surrogate=2,  # Repeat surrogate points\n    seed=42,\n    verbose=True\n)\n\nresult = optimizer.optimize()\n\n# Save results (includes noise statistics)\noptimizer.save_result(prefix=\"noisy_opt\")\n\n# Load and analyze noise statistics\nloaded_opt = SpotOptim.load_result(\"noisy_opt_res.pkl\")\n\nprint(f\"Noise handling enabled: {loaded_opt.noise}\")\nprint(f\"Best mean value: {loaded_opt.best_y_:.6f}\")\n\nif loaded_opt.mean_y is not None:\n    print(f\"Mean values available: {len(loaded_opt.mean_y)}\")\n    print(f\"Variance values available: {len(loaded_opt.var_y)}\")\n</code></pre>"},{"location":"manuals/save_load/#working-with-different-variable-types","title":"Working with Different Variable Types","text":"<p>Save and load preserves variable type information:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef mixed_objective(X):\n    \"\"\"Objective with mixed variable types\"\"\"\n    return np.sum(X**2, axis=1)\n\n# Create optimizer with mixed variable types\noptimizer = SpotOptim(\n    fun=mixed_objective,\n    bounds=[(-5, 5), (-5, 5), (-5, 5), (-5, 5)],\n    var_type=[\"num\", \"int\", \"factor\", \"num\"],\n    var_name=[\"continuous\", \"integer\", \"categorical\", \"another_cont\"],\n    max_iter=50,\n    n_initial=20,\n    seed=42\n)\n\nresult = optimizer.optimize()\n\n# Save results\noptimizer.save_result(prefix=\"mixed_vars\")\n\n# Load results\nloaded_opt = SpotOptim.load_result(\"mixed_vars_res.pkl\")\n\nprint(\"Variable types preserved:\")\nprint(f\"  var_type: {loaded_opt.var_type}\")\nprint(f\"  var_name: {loaded_opt.var_name}\")\n\n# Verify integer variables are still integers\nprint(f\"\\nInteger variable (dim 1) values:\")\nprint(loaded_opt.X_[:5, 1])  # Should be integers\n</code></pre>"},{"location":"manuals/save_load/#best-practices","title":"Best Practices","text":""},{"location":"manuals/save_load/#1-always-re-attach-the-objective-function","title":"1. Always Re-attach the Objective Function","text":"<p>After loading an experiment, you must re-attach the objective function:</p> <pre><code># Load experiment\noptimizer = SpotOptim.load_experiment(\"experiment_exp.pkl\")\n\n# REQUIRED: Re-attach function\noptimizer.fun = your_objective_function\n\n# Now you can optimize\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/save_load/#2-use-meaningful-prefixes","title":"2. Use Meaningful Prefixes","text":"<p>Organize your experiments with descriptive prefixes:</p> <pre><code># Good practice: descriptive prefixes\noptimizer.save_experiment(prefix=\"sphere_d10_seed42\")\noptimizer.save_experiment(prefix=\"rosenbrock_n100_lhs\")\noptimizer.save_result(prefix=\"final_run_2024_11_15\")\n\n# Avoid: generic names\noptimizer.save_experiment(prefix=\"exp1\")  # Not descriptive\noptimizer.save_result(prefix=\"result\")     # Hard to track\n</code></pre>"},{"location":"manuals/save_load/#3-save-experiments-before-remote-execution","title":"3. Save Experiments Before Remote Execution","text":"<pre><code># Define locally\noptimizer = SpotOptim(bounds=bounds, max_iter=500, seed=42)\noptimizer.save_experiment(prefix=\"remote_job\")\n\n# Transfer file to remote machine\n# Execute remotely\n# Transfer results back\n# Analyze locally\n</code></pre>"},{"location":"manuals/save_load/#4-version-your-experiments","title":"4. Version Your Experiments","text":"<pre><code>import datetime\n\n# Add timestamp to prefix\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nprefix = f\"experiment_{timestamp}\"\n\noptimizer.save_experiment(prefix=prefix)\n# Creates: experiment_20241115_143022_exp.pkl\n</code></pre>"},{"location":"manuals/save_load/#5-handle-file-paths-robustly","title":"5. Handle File Paths Robustly","text":"<pre><code>import os\n\n# Create directory structure\nexp_dir = \"experiments/batch_001\"\nos.makedirs(exp_dir, exist_ok=True)\n\n# Save with full path\noptimizer.save_experiment(\n    prefix=\"exp_001\",\n    path=exp_dir\n)\n\n# Load with full path\nexp_file = os.path.join(exp_dir, \"exp_001_exp.pkl\")\nloaded_opt = SpotOptim.load_experiment(exp_file)\n</code></pre>"},{"location":"manuals/save_load/#complete-example-multi-machine-workflow","title":"Complete Example: Multi-Machine Workflow","text":"<p>Here\u2019s a complete example demonstrating the entire workflow:</p>"},{"location":"manuals/save_load/#local-machine-setup","title":"Local Machine (Setup)","text":"<pre><code># setup_experiment.py\nimport numpy as np\nfrom spotoptim import SpotOptim\nimport os\n\n# Create experiments directory\nos.makedirs(\"experiments\", exist_ok=True)\n\n# Define multiple experiments\nexperiments = [\n    {\"seed\": 42, \"max_iter\": 100, \"prefix\": \"exp_seed42\"},\n    {\"seed\": 123, \"max_iter\": 100, \"prefix\": \"exp_seed123\"},\n    {\"seed\": 999, \"max_iter\": 100, \"prefix\": \"exp_seed999\"},\n]\n\nfor exp_config in experiments:\n    optimizer = SpotOptim(\n        bounds=[(-10, 10), (-10, 10), (-10, 10)],\n        max_iter=exp_config[\"max_iter\"],\n        n_initial=30,\n        seed=exp_config[\"seed\"],\n        verbose=False\n    )\n\n    optimizer.save_experiment(\n        prefix=exp_config[\"prefix\"],\n        path=\"experiments\"\n    )\n\n    print(f\"Created: experiments/{exp_config['prefix']}_exp.pkl\")\n\nprint(\"\\nAll experiments created. Transfer 'experiments' folder to remote machine.\")\n</code></pre>"},{"location":"manuals/save_load/#remote-machine-execution","title":"Remote Machine (Execution)","text":"<pre><code># run_experiments.py\nimport numpy as np\nfrom spotoptim import SpotOptim\nimport os\nimport glob\n\ndef complex_objective(X):\n    \"\"\"Complex multimodal objective function\"\"\"\n    term1 = np.sum(X**2, axis=1)\n    term2 = 10 * np.sum(np.cos(2 * np.pi * X), axis=1)\n    term3 = 0.1 * np.sum(np.sin(5 * np.pi * X), axis=1)\n    return term1 - term2 + term3\n\n# Find all experiment files\nexp_files = glob.glob(\"experiments/*_exp.pkl\")\nprint(f\"Found {len(exp_files)} experiments to run\")\n\n# Run each experiment\nfor exp_file in exp_files:\n    print(f\"\\nProcessing: {exp_file}\")\n\n    # Load experiment\n    optimizer = SpotOptim.load_experiment(exp_file)\n\n    # Attach objective\n    optimizer.fun = complex_objective\n\n    # Run optimization\n    result = optimizer.optimize()\n    print(f\"  Best value: {result.fun:.6f}\")\n\n    # Save result (same prefix, different suffix)\n    prefix = os.path.basename(exp_file).replace(\"_exp.pkl\", \"\")\n    optimizer.save_result(\n        prefix=prefix,\n        path=\"experiments\"\n    )\n    print(f\"  Saved: experiments/{prefix}_res.pkl\")\n\nprint(\"\\nAll experiments completed. Transfer results back to local machine.\")\n</code></pre>"},{"location":"manuals/save_load/#local-machine-analysis","title":"Local Machine (Analysis)","text":"<pre><code># analyze_results.py\nimport numpy as np\nfrom spotoptim import SpotOptim\nimport glob\nimport matplotlib.pyplot as plt\n\n# Find all result files\nresult_files = glob.glob(\"experiments/*_res.pkl\")\nprint(f\"Found {len(result_files)} results to analyze\")\n\n# Load and compare results\nresults = []\nfor res_file in result_files:\n    opt = SpotOptim.load_result(res_file)\n    results.append({\n        \"file\": res_file,\n        \"best_value\": opt.best_y_,\n        \"best_point\": opt.best_x_,\n        \"n_evals\": opt.counter,\n        \"seed\": opt.seed\n    })\n    print(f\"{res_file}: best={opt.best_y_:.6f}, evals={opt.counter}\")\n\n# Find best overall result\nbest = min(results, key=lambda x: x[\"best_value\"])\nprint(f\"\\nBest result:\")\nprint(f\"  File: {best['file']}\")\nprint(f\"  Value: {best['best_value']:.6f}\")\nprint(f\"  Point: {best['best_point']}\")\nprint(f\"  Seed: {best['seed']}\")\n\n# Plot convergence comparison\nplt.figure(figsize=(12, 6))\n\nfor res_file in result_files:\n    opt = SpotOptim.load_result(res_file)\n    seed = opt.seed\n    cummin = [opt.y_[:i+1].min() for i in range(len(opt.y_))]\n    plt.plot(cummin, label=f\"Seed {seed}\", linewidth=2, alpha=0.7)\n\nplt.xlabel(\"Iteration\", fontsize=12)\nplt.ylabel(\"Best Value Found\", fontsize=12)\nplt.title(\"Optimization Progress Comparison\", fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"experiments/convergence_comparison.png\", dpi=150)\nprint(\"\\nConvergence plot saved to: experiments/convergence_comparison.png\")\n</code></pre>"},{"location":"manuals/save_load/#technical-details","title":"Technical Details","text":""},{"location":"manuals/save_load/#serialization-method","title":"Serialization Method","text":"<p>SpotOptim uses Python\u2019s built-in <code>pickle</code> module for serialization. This provides:</p> <ul> <li>Standard library: No additional dependencies required</li> <li>Compatibility: Works with numpy arrays, sklearn models, scipy functions</li> <li>Performance: Efficient serialization of large datasets</li> </ul>"},{"location":"manuals/save_load/#component-reinitialization","title":"Component Reinitialization","text":"<p>When loading experiments, certain components are automatically recreated:</p> <ul> <li>Surrogate model: Gaussian Process with default kernel</li> <li>LHS sampler: Latin Hypercube Sampler with original seed</li> </ul> <p>This ensures loaded experiments can continue optimization without manual configuration.</p>"},{"location":"manuals/save_load/#excluded-components","title":"Excluded Components","text":"<p>Some components cannot be pickled and are automatically excluded:</p> <ul> <li>Objective function (<code>fun</code>): Lambda functions and local functions cannot be reliably pickled</li> <li>TensorBoard writer (<code>tb_writer</code>): File handles cannot be serialized</li> <li>Surrogate model (experiments only): Recreated on load for experiments</li> </ul>"},{"location":"manuals/save_load/#file-format","title":"File Format","text":"<p>Files are saved using pickle\u2019s highest protocol:</p> <pre><code>with open(filename, \"wb\") as handle:\n    pickle.dump(optimizer_state, handle, protocol=pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"manuals/save_load/#troubleshooting","title":"Troubleshooting","text":""},{"location":"manuals/save_load/#issue-attributeerror-spotoptim-object-has-no-attribute-fun","title":"Issue: \u201cAttributeError: \u2018SpotOptim\u2019 object has no attribute \u2018fun\u2019\u201d","text":"<p>Cause: Objective function not re-attached after loading experiment.</p> <p>Solution: Always re-attach the function after loading:</p> <pre><code>opt = SpotOptim.load_experiment(\"exp.pkl\")\nopt.fun = your_objective_function  # Add this line\nresult = opt.optimize()\n</code></pre>"},{"location":"manuals/save_load/#issue-filenotfounderror-experiment-file-not-found","title":"Issue: \u201cFileNotFoundError: Experiment file not found\u201d","text":"<p>Cause: Incorrect file path or file doesn\u2019t exist.</p> <p>Solution: Check file path and ensure file exists:</p> <pre><code>import os\n\nfilename = \"experiment_exp.pkl\"\nif os.path.exists(filename):\n    opt = SpotOptim.load_experiment(filename)\nelse:\n    print(f\"File not found: {filename}\")\n</code></pre>"},{"location":"manuals/save_load/#issue-fileexistserror-file-already-exists","title":"Issue: \u201cFileExistsError: File already exists\u201d","text":"<p>Cause: Attempting to save over an existing file without <code>overwrite=True</code>.</p> <p>Solution: Either use a different prefix or enable overwriting:</p> <pre><code># Option 1: Use different prefix\noptimizer.save_result(prefix=\"my_result_v2\")\n\n# Option 2: Enable overwriting\noptimizer.save_result(prefix=\"my_result\", overwrite=True)\n</code></pre>"},{"location":"manuals/save_load/#issue-results-differ-after-loading","title":"Issue: Results differ after loading","text":"<p>Cause: Random state not preserved or function behavior changed.</p> <p>Solution: Ensure you\u2019re using the same seed and function definition:</p> <pre><code># When saving\noptimizer = SpotOptim(..., seed=42)  # Use fixed seed\n\n# When loading and continuing\nloaded_opt = SpotOptim.load_result(\"result_res.pkl\")\nloaded_opt.fun = same_objective_function  # Same function definition\n</code></pre>"},{"location":"manuals/save_load/#see-also","title":"See Also","text":"<ul> <li>Reproducibility Manual: Learn about using seeds for reproducible results</li> <li>TensorBoard Manual: Monitor optimization progress in real-time</li> </ul>"},{"location":"manuals/success_rate/","title":"Success Rate Tracking in SpotOptim","text":"<p>SpotOptim tracks the success rate of the optimization process, which measures how often the optimizer finds improvements over recent evaluations. This metric helps you understand whether the optimization is making progress or has stalled.</p>"},{"location":"manuals/success_rate/#what-is-success-rate","title":"What is Success Rate?","text":"<p>The success rate is a rolling metric that tracks the percentage of recent evaluations that improved upon the best value found so far. It\u2019s calculated over a sliding window of the last 100 evaluations.</p> <p>Key Points: - A \u201csuccess\u201d occurs when a new evaluation finds a value better (smaller) than the best found so far - The rate is computed over the last 100 evaluations (window size) - Values range from 0.0 (no recent improvements) to 1.0 (all recent evaluations improved) - Helps identify when optimization is stalling and may need adjustment</p>"},{"location":"manuals/success_rate/#first-example","title":"First Example","text":"<ul> <li>Start <code>TensorBoard</code> to visualize success rate in real-time:</li> </ul> <pre><code>tensorboard --logdir=runs\n</code></pre> <p>The execute the following code:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef rosenbrock(X):\n    \"\"\"Rosenbrock function - challenging optimization problem\"\"\"\n    x = X[:, 0]\n    y = X[:, 1]\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\n# Run optimization with periodic success rate checks\noptimizer = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=200,\n    n_initial=20,\n    tensorboard_log=True,\n    tensorboard_clean=True,\n    seed=42\n)\n\nresult = optimizer.optimize()\n\n# Analyze final success rate\nprint(f\"\\nOptimization Results:\")\nprint(f\"Best value: {result.fun:.6f}\")\nprint(f\"Total evaluations: {optimizer.counter}\")\nprint(f\"Final success rate: {optimizer.success_rate:.2%}\")\n\n# Interpret the result\nif optimizer.success_rate &gt; 0.5:\n    print(\"\u2192 High success rate: Optimization is still making good progress\")\nelif optimizer.success_rate &gt; 0.2:\n    print(\"\u2192 Medium success rate: Approaching convergence\")\nelse:\n    print(\"\u2192 Low success rate: Optimization has likely converged\")\n</code></pre>"},{"location":"manuals/success_rate/#second-example","title":"Second Example","text":"<pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\ndef sphere(X):\n    \"\"\"Simple sphere function: f(x) = sum(x^2)\"\"\"\n    return np.sum(X**2, axis=1)\n\n# Create optimizer\noptimizer = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5), (-5, 5)],\n    max_iter=100,\n    n_initial=20,\n    verbose=True\n)\n\n# Run optimization\nresult = optimizer.optimize()\n\n# Check success rate\nprint(f\"Final success rate: {optimizer.success_rate:.2%}\")\nprint(f\"Total evaluations: {optimizer.counter}\")\n</code></pre>"},{"location":"manuals/success_rate/#accessing-success-rate","title":"Accessing Success Rate","text":"<p>The success rate is stored in the <code>success_rate</code> attribute:</p> <pre><code>optimizer = SpotOptim(fun=objective, bounds=bounds, max_iter=50)\nresult = optimizer.optimize()\n\n# Access success rate\ncurrent_rate = optimizer.success_rate\nprint(f\"Success rate: {current_rate:.2%}\")\n\n# Also available via getter method\nrate = optimizer._get_success_rate()\n</code></pre>"},{"location":"manuals/success_rate/#interpreting-success-rate","title":"Interpreting Success Rate","text":""},{"location":"manuals/success_rate/#high-success-rate-05","title":"High Success Rate (&gt; 0.5)","text":"<pre><code>Success Rate: 75%\n</code></pre> <p>Interpretation: The optimizer is finding improvements frequently. This typically indicates: - The optimization is in an exploratory phase - The surrogate model is effectively guiding the search - There\u2019s still room for improvement in the search space</p> <p>Action: Continue optimization - progress is good!</p>"},{"location":"manuals/success_rate/#medium-success-rate-02-05","title":"Medium Success Rate (0.2 - 0.5)","text":"<pre><code>Success Rate: 35%\n</code></pre> <p>Interpretation: The optimizer occasionally finds improvements. This suggests: - The search is becoming more refined - The optimizer is balancing exploration and exploitation - Approaching a local or global optimum</p> <p>Action: Monitor progress and consider stopping criteria.</p>"},{"location":"manuals/success_rate/#low-success-rate-02","title":"Low Success Rate (&lt; 0.2)","text":"<pre><code>Success Rate: 8%\n</code></pre> <p>Interpretation: Few recent evaluations improve the best value. This may indicate: - The optimization has converged to a (local) optimum - The search is stuck in a plateau region - The budget may be exhausted in terms of meaningful progress</p> <p>Action: Consider stopping optimization or adjusting parameters.</p>"},{"location":"manuals/success_rate/#tensorboard-visualization","title":"TensorBoard Visualization","text":"<p>When TensorBoard logging is enabled, success rate is automatically logged and can be visualized in real-time:</p> <pre><code>optimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=100,\n    n_initial=20,\n    tensorboard_log=True,  # Enable logging\n    verbose=True\n)\n\nresult = optimizer.optimize()\n</code></pre> <p>View in TensorBoard:</p> <pre><code>tensorboard --logdir=runs\n</code></pre> <p>In the TensorBoard interface, look for: - SCALARS tab \u2192 <code>success_rate</code>: Rolling success rate over iterations - Compare multiple runs side-by-side - Identify when optimization stalls</p>"},{"location":"manuals/success_rate/#example-monitoring-optimization-progress","title":"Example: Monitoring Optimization Progress","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef rosenbrock(X):\n    \"\"\"Rosenbrock function - challenging optimization problem\"\"\"\n    x = X[:, 0]\n    y = X[:, 1]\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\n# Run optimization with periodic success rate checks\noptimizer = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=100,\n    n_initial=20,\n    tensorboard_log=True,\n    seed=42\n)\n\nresult = optimizer.optimize()\n\n# Analyze final success rate\nprint(f\"\\nOptimization Results:\")\nprint(f\"Best value: {result.fun:.6f}\")\nprint(f\"Total evaluations: {optimizer.counter}\")\nprint(f\"Final success rate: {optimizer.success_rate:.2%}\")\n\n# Interpret the result\nif optimizer.success_rate &gt; 0.5:\n    print(\"\u2192 High success rate: Optimization is still making good progress\")\nelif optimizer.success_rate &gt; 0.2:\n    print(\"\u2192 Medium success rate: Approaching convergence\")\nelse:\n    print(\"\u2192 Low success rate: Optimization has likely converged\")\n</code></pre>"},{"location":"manuals/success_rate/#example-comparing-multiple-runs","title":"Example: Comparing Multiple Runs","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef ackley(X):\n    \"\"\"Ackley function - multimodal test function\"\"\"\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    n = X.shape[1]\n\n    sum_sq = np.sum(X**2, axis=1)\n    sum_cos = np.sum(np.cos(c * X), axis=1)\n\n    return -a * np.exp(-b * np.sqrt(sum_sq / n)) - np.exp(sum_cos / n) + a + np.e\n\n# Run with different configurations\nconfigs = [\n    {\"n_initial\": 10, \"max_iter\": 50, \"name\": \"Small initial\"},\n    {\"n_initial\": 30, \"max_iter\": 50, \"name\": \"Large initial\"},\n]\n\nresults = []\nfor config in configs:\n    optimizer = SpotOptim(\n        fun=ackley,\n        bounds=[(-5, 5), (-5, 5)],\n        n_initial=config[\"n_initial\"],\n        max_iter=config[\"max_iter\"],\n        seed=42,\n        verbose=False\n    )\n    result = optimizer.optimize()\n\n    results.append({\n        \"name\": config[\"name\"],\n        \"best_value\": result.fun,\n        \"success_rate\": optimizer.success_rate,\n        \"n_evals\": optimizer.counter\n    })\n\n    print(f\"\\n{config['name']}:\")\n    print(f\"  Best value: {result.fun:.6f}\")\n    print(f\"  Success rate: {optimizer.success_rate:.2%}\")\n    print(f\"  Evaluations: {optimizer.counter}\")\n\n# Find best configuration\nbest = min(results, key=lambda x: x[\"best_value\"])\nprint(f\"\\nBest configuration: {best['name']}\")\nprint(f\"  Achieved: f(x) = {best['best_value']:.6f}\")\nprint(f\"  Final success rate: {best['success_rate']:.2%}\")\n</code></pre>"},{"location":"manuals/success_rate/#success-rate-with-noisy-functions","title":"Success Rate with Noisy Functions","text":"<p>For noisy functions (when <code>repeats_initial &gt; 1</code> or <code>repeats_surrogate &gt; 1</code>), the success rate tracks improvements in the raw y values, not the aggregated means:</p> <pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\ndef noisy_sphere(X):\n    \"\"\"Sphere function with Gaussian noise\"\"\"\n    base = np.sum(X**2, axis=1)\n    noise = np.random.normal(0, 0.5, size=base.shape)\n    return base + noise\n\noptimizer = SpotOptim(\n    fun=noisy_sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=15,\n    repeats_initial=3,    # 3 evaluations per initial point\n    repeats_surrogate=2,  # 2 evaluations per new point\n    seed=42,\n    verbose=True\n)\n\nresult = optimizer.optimize()\n\nprint(f\"\\nNoisy Optimization Results:\")\nprint(f\"Best raw value: {optimizer.min_y:.6f}\")\nprint(f\"Best mean value: {optimizer.min_mean_y:.6f}\")\nprint(f\"Success rate: {optimizer.success_rate:.2%}\")\nprint(f\"Total evaluations: {optimizer.counter}\")\nprint(f\"Unique design points: {optimizer.mean_X.shape[0]}\")\n</code></pre> <p>Note: With noisy functions, the success rate may be lower because: - Noise can mask true improvements - Multiple evaluations of the same point contribute to the window - Focus on the mean values (<code>min_mean_y</code>) for better assessment</p>"},{"location":"manuals/success_rate/#advanced-custom-window-size","title":"Advanced: Custom Window Size","text":"<p>The success rate is calculated over a window of 100 evaluations by default. This is controlled by the <code>window_size</code> attribute:</p> <pre><code>optimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=10\n)\n\n# Check default window size\nprint(f\"Window size: {optimizer.window_size}\")  # 100\n\n# The window size is set during initialization\n# To use a different window, you would need to modify it\n# before running optimization (not typically recommended)\n</code></pre>"},{"location":"manuals/success_rate/#best-practices","title":"Best Practices","text":""},{"location":"manuals/success_rate/#1-monitor-during-long-runs","title":"1. Monitor During Long Runs","text":"<p>For expensive optimization runs, periodically check success rate:</p> <pre><code># Could be implemented with callbacks in future versions\n# For now, success rate is updated automatically and logged to TensorBoard\n</code></pre>"},{"location":"manuals/success_rate/#2-combine-with-tensorboard","title":"2. Combine with TensorBoard","text":"<p>Always enable TensorBoard logging for visual monitoring:</p> <pre><code>optimizer = SpotOptim(\n    fun=expensive_function,\n    bounds=bounds,\n    max_iter=1000,\n    tensorboard_log=True,  # Track success_rate visually\n    tensorboard_path=\"runs/long_optimization\"\n)\n</code></pre>"},{"location":"manuals/success_rate/#3-use-as-stopping-criterion","title":"3. Use as Stopping Criterion","text":"<p>Consider stopping when success rate drops very low:</p> <pre><code># Manual stopping check (conceptual)\nif optimizer.success_rate &lt; 0.05 and optimizer.counter &gt; 50:\n    print(\"Success rate very low - optimization has likely converged\")\n</code></pre>"},{"location":"manuals/success_rate/#4-compare-different-strategies","title":"4. Compare Different Strategies","text":"<p>Use success rate to compare optimization strategies:</p> <pre><code>strategies = [\"ei\", \"pi\", \"y\"]  # Different acquisition functions\nfor acq in strategies:\n    opt = SpotOptim(fun=obj, bounds=bnds, acquisition=acq, max_iter=50)\n    result = opt.optimize()\n    print(f\"{acq}: success_rate={opt.success_rate:.2%}, best={result.fun:.6f}\")\n</code></pre>"},{"location":"manuals/success_rate/#technical-details","title":"Technical Details","text":""},{"location":"manuals/success_rate/#how-success-is-counted","title":"How Success is Counted","text":"<p>A new evaluation <code>y_new</code> is considered a success if:</p> <pre><code>y_new &lt; best_y_so_far\n</code></pre> <p>where <code>best_y_so_far</code> is the minimum value found in all previous evaluations.</p>"},{"location":"manuals/success_rate/#rolling-window-calculation","title":"Rolling Window Calculation","text":"<p>The success rate is computed as:</p> <pre><code>success_rate = (number of successes in last 100 evals) / (window size)\n</code></pre> <ul> <li>Window size defaults to 100</li> <li>If fewer than 100 evaluations have been performed, the window size is the number of evaluations</li> <li>The window slides forward with each new evaluation</li> </ul>"},{"location":"manuals/success_rate/#update-frequency","title":"Update Frequency","text":"<p>The success rate is updated after: 1. Initial design evaluation 2. Each iteration\u2019s new point evaluation(s) 3. OCBA re-evaluations (if applicable)</p>"},{"location":"manuals/success_rate/#summary","title":"Summary","text":"<ul> <li>Success rate measures the percentage of recent evaluations that improve the best value</li> <li>Calculated over a rolling window of the last 100 evaluations</li> <li>Values range from 0.0 to 1.0</li> <li>High rates (&gt;0.5) indicate active progress</li> <li>Low rates (&lt;0.2) suggest convergence</li> <li>Automatically logged to TensorBoard when logging is enabled</li> <li>Available via <code>optimizer.success_rate</code> attribute after optimization</li> </ul> <p>Use success rate to: - \u2713 Monitor optimization progress in real-time - \u2713 Identify when to stop optimization - \u2713 Compare different optimization strategies - \u2713 Assess optimization difficulty for different problems</p>"},{"location":"manuals/tensorboard/","title":"TensorBoard Logging in SpotOptim","text":"<p>SpotOptim supports TensorBoard logging for monitoring optimization progress in real-time.</p>"},{"location":"manuals/tensorboard/#quick-start","title":"Quick Start","text":""},{"location":"manuals/tensorboard/#1-enable-tensorboard-logging","title":"1. Enable TensorBoard Logging","text":"<pre><code>from spotoptim import SpotOptim\nimport numpy as np\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\noptimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=15,\n    tensorboard_log=True,  # Enable logging\n    verbose=True\n)\n\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/tensorboard/#2-view-logs-in-tensorboard","title":"2. View Logs in TensorBoard","text":"<p>In a separate terminal, run:</p> <pre><code>tensorboard --logdir=runs\n</code></pre> <p>Then open your browser to http://localhost:6006</p>"},{"location":"manuals/tensorboard/#cleaning-old-logs","title":"Cleaning Old Logs","text":"<p>You can automatically remove old TensorBoard logs before starting a new optimization:</p> <pre><code>optimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    tensorboard_log=True,\n    tensorboard_clean=True,  # Remove old logs from 'runs' directory\n    verbose=True\n)\n</code></pre> <p>Warning: This permanently deletes all subdirectories in the <code>runs</code> folder. Make sure to save important logs elsewhere before enabling this feature.</p>"},{"location":"manuals/tensorboard/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Clean Start - Remove old logs and create new one:    <code>python    tensorboard_log=True, tensorboard_clean=True</code></p> </li> <li> <p>Preserve History - Keep old logs and add new one (default):    <code>python    tensorboard_log=True, tensorboard_clean=False</code></p> </li> <li> <p>Just Clean - Remove old logs without new logging:    <code>python    tensorboard_log=False, tensorboard_clean=True</code></p> </li> </ol>"},{"location":"manuals/tensorboard/#custom-log-directory","title":"Custom Log Directory","text":"<p>Specify a custom path for TensorBoard logs:</p> <pre><code>optimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    tensorboard_log=True,\n    tensorboard_path=\"my_experiments/run_001\",\n    ...\n)\n</code></pre>"},{"location":"manuals/tensorboard/#what-gets-logged","title":"What Gets Logged","text":""},{"location":"manuals/tensorboard/#scalar-metrics","title":"Scalar Metrics","text":"<p>For Deterministic Functions:</p> <ul> <li><code>y_values/min</code>: Best (minimum) y value found so far</li> <li><code>y_values/last</code>: Most recently evaluated y value</li> <li><code>X_best/x0, X_best/x1, ...</code>: Coordinates of the best point</li> </ul> <p>For Noisy Functions (repeats &gt; 1):</p> <ul> <li><code>y_values/min</code>: Best single evaluation</li> <li><code>y_values/mean_best</code>: Best mean y value</li> <li><code>y_values/last</code>: Most recent evaluation</li> <li><code>y_variance_at_best</code>: Variance at the best mean point</li> <li><code>X_mean_best/x0, X_mean_best/x1, ...</code>: Coordinates of best mean point</li> </ul>"},{"location":"manuals/tensorboard/#hyperparameters","title":"Hyperparameters","text":"<p>Each function evaluation is logged with:</p> <ul> <li>Input coordinates (x0, x1, x2, \u2026)</li> <li>Function value (hp_metric)</li> </ul> <p>This allows you to explore the relationship between hyperparameters and objective values in the HPARAMS tab.</p>"},{"location":"manuals/tensorboard/#examples","title":"Examples","text":""},{"location":"manuals/tensorboard/#basic-usage","title":"Basic Usage","text":"<pre><code>optimizer = SpotOptim(\n    fun=lambda X: np.sum(X**2, axis=1),\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    tensorboard_log=True,\n    verbose=True\n)\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/tensorboard/#noisy-optimization","title":"Noisy Optimization","text":"<pre><code>def noisy_objective(X):\n    base = np.sum(X**2, axis=1)\n    noise = np.random.normal(0, 0.1, size=base.shape)\n    return base + noise\n\noptimizer = SpotOptim(\n    fun=noisy_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    repeats_initial=3,\n    repeats_surrogate=2,\n    tensorboard_log=True,\n    tensorboard_path=\"runs/noisy_exp\",\n    seed=42\n)\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/tensorboard/#with-ocba","title":"With OCBA","text":"<pre><code>optimizer = SpotOptim(\n    fun=noisy_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    repeats_initial=2,\n    ocba_delta=3,  # Re-evaluate 3 promising points per iteration\n    tensorboard_log=True,\n    seed=42\n)\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/tensorboard/#comparing-multiple-runs","title":"Comparing Multiple Runs","text":"<p>Run multiple optimizations with different settings:</p> <pre><code># Run 1: Standard\nopt1 = SpotOptim(..., tensorboard_path=\"runs/standard\")\nopt1.optimize()\n\n# Run 2: With OCBA\nopt2 = SpotOptim(..., ocba_delta=3, tensorboard_path=\"runs/with_ocba\")\nopt2.optimize()\n\n# Run 3: More initial points\nopt3 = SpotOptim(..., n_initial=20, tensorboard_path=\"runs/more_initial\")\nopt3.optimize()\n</code></pre> <p>Then view all runs together:</p> <pre><code>tensorboard --logdir=runs\n</code></pre>"},{"location":"manuals/tensorboard/#tensorboard-features","title":"TensorBoard Features","text":""},{"location":"manuals/tensorboard/#scalars-tab","title":"SCALARS Tab","text":"<ul> <li>View convergence curves</li> <li>Compare optimization progress across runs</li> <li>Track how metrics change over iterations</li> </ul>"},{"location":"manuals/tensorboard/#hparams-tab","title":"HPARAMS Tab","text":"<ul> <li>Explore hyperparameter space</li> <li>See which parameter combinations work best</li> <li>Identify patterns in successful configurations</li> </ul>"},{"location":"manuals/tensorboard/#text-tab","title":"Text Tab","text":"<ul> <li>View configuration details</li> <li>Check run metadata</li> </ul>"},{"location":"manuals/tensorboard/#tips","title":"Tips","text":"<ol> <li> <p>Organize Experiments: Use descriptive tensorboard_path names:    <code>python    tensorboard_path=f\"runs/exp_{date}_{config_name}\"</code></p> </li> <li> <p>Compare Algorithms: Run multiple optimization strategies and compare:    <code>python    # Different acquisition functions    for acq in ['ei', 'pi', 'y']:        opt = SpotOptim(..., acquisition=acq, tensorboard_path=f\"runs/acq_{acq}\")        opt.optimize()</code></p> </li> <li> <p>Clean Up Old Runs: Use <code>tensorboard_clean=True</code> for automatic cleanup, or manually:    <code>bash    rm -rf runs/old_experiment</code></p> </li> <li> <p>Port Conflicts: If port 6006 is busy, use a different port:    <code>bash    tensorboard --logdir=runs --port=6007</code></p> </li> </ol>"},{"location":"manuals/tensorboard/#demo-scripts","title":"Demo Scripts","text":"<p>Run the comprehensive TensorBoard demo:</p> <pre><code>python demo_tensorboard.py\n</code></pre> <p>This demonstrates:</p> <ul> <li>Deterministic optimization (Rosenbrock function)</li> <li>Noisy optimization with repeated evaluations</li> <li>OCBA for intelligent re-evaluation</li> </ul> <p>Run the log cleaning demo:</p> <pre><code>python demo_tensorboard_clean.py\n</code></pre> <p>This demonstrates:</p> <ul> <li>Creating multiple log directories</li> <li>Preserving old logs (default behavior)</li> <li>Cleaning old logs automatically</li> <li>Cleaning without creating new logs</li> </ul> <p>This demonstrates:</p> <ul> <li>Deterministic optimization (Rosenbrock function)</li> <li>Noisy optimization with repeated evaluations</li> <li>OCBA for intelligent re-evaluation</li> </ul>"},{"location":"manuals/tensorboard/#troubleshooting","title":"Troubleshooting","text":"<p>Q: TensorBoard shows \u201cNo dashboards are active\u201d A: Make sure you\u2019ve run an optimization with <code>tensorboard_log=True</code> first.</p> <p>Q: Can\u2019t see my latest run A: Refresh TensorBoard (click the reload button in the upper right).</p> <p>Q: How do I stop TensorBoard? A: Press Ctrl+C in the terminal where TensorBoard is running.</p> <p>Q: Logs taking up too much space? A: Use <code>tensorboard_clean=True</code> to automatically remove old logs, or manually delete old run directories.</p> <p>Q: How do I remove all old logs at once? A: Set <code>tensorboard_clean=True</code> when creating your optimizer. This will remove all subdirectories in the <code>runs</code> folder.</p>"},{"location":"manuals/tensorboard/#related-parameters","title":"Related Parameters","text":"<ul> <li><code>tensorboard_log</code> (bool): Enable/disable logging (default: False)</li> <li><code>tensorboard_path</code> (str): Custom log directory (default: auto-generated with timestamp)</li> <li><code>tensorboard_clean</code> (bool): Remove old logs from \u2018runs\u2019 directory before starting (default: False)</li> <li><code>verbose</code> (bool): Print progress to console (default: False)</li> <li><code>var_name</code> (list): Custom names for variables (used in TensorBoard labels)</li> </ul>"},{"location":"manuals/tensorboard/#performance-notes","title":"Performance Notes","text":"<p>TensorBoard logging has minimal overhead:</p> <ul> <li>&lt; 1% slowdown for typical optimizations</li> <li>Event files are efficiently buffered and written</li> <li>Writer is properly closed after optimization completes</li> </ul>"},{"location":"manuals/tensorboard_clean/","title":"TensorBoard Log Cleaning Feature","text":""},{"location":"manuals/tensorboard_clean/#summary","title":"Summary","text":"<p>Automatic cleaning of old TensorBoard log directories with the <code>tensorboard_clean</code> parameter.</p>"},{"location":"manuals/tensorboard_clean/#usage","title":"Usage","text":""},{"location":"manuals/tensorboard_clean/#basic-usage","title":"Basic Usage","text":"<pre><code>from spotoptim import SpotOptim\n\n# Remove old logs and create new log directory\noptimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    tensorboard_log=True,\n    tensorboard_clean=True,  # Removes all subdirectories in 'runs'\n    verbose=True\n)\n\nresult = optimizer.optimize()\n</code></pre>"},{"location":"manuals/tensorboard_clean/#use-cases","title":"Use Cases","text":"<code>tensorboard_log</code> <code>tensorboard_clean</code> Behavior <code>True</code> <code>True</code> Clean old logs, create new log directory <code>True</code> <code>False</code> Preserve old logs, create new log directory <code>False</code> <code>True</code> Clean old logs, no new logging <code>False</code> <code>False</code> No logging, no cleaning (default)"},{"location":"manuals/tensorboard_clean/#implementation-details","title":"Implementation Details","text":""},{"location":"manuals/tensorboard_clean/#cleaning-method","title":"Cleaning Method","text":"<pre><code>def _clean_tensorboard_logs(self) -&gt; None:\n    \"\"\"Clean old TensorBoard log directories from the runs folder.\"\"\"\n    if self.tensorboard_clean:\n        runs_dir = \"runs\"\n        if os.path.exists(runs_dir) and os.path.isdir(runs_dir):\n            # Get all subdirectories in runs\n            subdirs = [\n                os.path.join(runs_dir, d)\n                for d in os.listdir(runs_dir)\n                if os.path.isdir(os.path.join(runs_dir, d))\n            ]\n\n            # Remove each subdirectory\n            for subdir in subdirs:\n                try:\n                    shutil.rmtree(subdir)\n                    if self.verbose:\n                        print(f\"Removed old TensorBoard logs: {subdir}\")\n                except Exception as e:\n                    if self.verbose:\n                        print(f\"Warning: Could not remove {subdir}: {e}\")\n</code></pre>"},{"location":"manuals/tensorboard_clean/#execution-flow","title":"Execution Flow","text":"<ol> <li>User creates <code>SpotOptim</code> instance with <code>tensorboard_clean=True</code></li> <li>During initialization, <code>_clean_tensorboard_logs()</code> is called</li> <li>Method checks if \u2018runs\u2019 directory exists</li> <li>Removes all subdirectories (but preserves files)</li> <li>If <code>tensorboard_log=True</code>, a new log directory is created</li> <li>Optimization proceeds normally</li> </ol>"},{"location":"manuals/tensorboard_clean/#safety-features","title":"Safety Features","text":"<ul> <li>Only removes directories, not files in \u2018runs\u2019 folder</li> <li>Handles missing \u2018runs\u2019 directory gracefully</li> <li>Error handling for permission issues</li> <li>Verbose output shows what\u2019s being removed</li> <li>Default is <code>False</code> to prevent accidental deletion</li> </ul>"},{"location":"manuals/tensorboard_clean/#warning","title":"Warning","text":"<p>\u26a0\ufe0f IMPORTANT: Setting <code>tensorboard_clean=True</code> permanently deletes all subdirectories in the \u2018runs\u2019 folder. Make sure to save important logs elsewhere before enabling this feature.</p>"},{"location":"manuals/var_type/","title":"Variable Type (var_type) Implementation","text":""},{"location":"manuals/var_type/#overview","title":"Overview","text":"<p>This document describes the <code>var_type</code> implementation in SpotOptim, which allows users to specify different data types for optimization variables.</p>"},{"location":"manuals/var_type/#supported-variable-types","title":"Supported Variable Types","text":"<p>SpotOptim supports three main data types:</p>"},{"location":"manuals/var_type/#1-float","title":"1. \u2018float\u2019","text":"<ul> <li>Purpose: Continuous optimization with Python floats</li> <li>Behavior: No rounding applied, values remain continuous</li> <li>Use case: Standard continuous optimization variables</li> <li>Example: Temperature (23.5\u00b0C), Distance (1.234m)</li> </ul>"},{"location":"manuals/var_type/#2-int","title":"2. \u2018int\u2019","text":"<ul> <li>Purpose: Discrete integer optimization</li> <li>Behavior: Float values are automatically rounded to integers</li> <li>Use case: Count variables, discrete parameters</li> <li>Example: Number of layers (5), Population size (100)</li> </ul>"},{"location":"manuals/var_type/#3-factor","title":"3. \u2018factor\u2019","text":"<ul> <li>Purpose: Unordered categorical data</li> <li>Behavior: Internally mapped to integer values (0, 1, 2, \u2026)</li> <li>Use case: Categorical choices like colors, algorithms, modes</li> <li>Example: Color (\u201cred\u201d\u21920, \u201cgreen\u201d\u21921, \u201cblue\u201d\u21922)</li> <li>Note: The actual string-to-int mapping is external to SpotOptim; the optimizer works with the integer representation</li> </ul>"},{"location":"manuals/var_type/#implementation-details","title":"Implementation Details","text":""},{"location":"manuals/var_type/#where-var_type-is-used","title":"Where <code>var_type</code> is Used","text":"<p>The <code>var_type</code> parameter is properly propagated throughout the optimization process:</p> <ol> <li>Initialization (<code>__init__</code>):</li> </ol> <ul> <li>Stored as <code>self.var_type</code></li> <li>Default: <code>[\"float\"] * n_dim</code> if not specified</li> </ul> <ol> <li>Initial Design Generation (<code>_generate_initial_design</code>):</li> </ol> <ul> <li>Applies type constraints via <code>_repair_non_numeric()</code></li> <li>Ensures initial points respect variable types</li> </ul> <ol> <li>New Point Suggestion (<code>_suggest_next_point</code>):</li> </ol> <ul> <li>Applies type constraints to acquisition function optimization results</li> <li>Ensures suggested points respect variable types</li> </ul> <ol> <li>User-Provided Initial Design (<code>optimize</code>):</li> </ol> <ul> <li>Applies type constraints to X0 if provided</li> <li>Ensures consistency regardless of input source</li> </ul> <ol> <li>Mesh Grid Generation (<code>_generate_mesh_grid</code>):</li> </ol> <ul> <li>Used for plotting, respects variable types</li> <li>Ensures visualization shows correct discrete/continuous behavior</li> </ul>"},{"location":"manuals/var_type/#core-method-_repair_non_numeric","title":"Core Method: <code>_repair_non_numeric()</code>","text":"<p>This method enforces variable type constraints:</p> <pre><code>def _repair_non_numeric(self, X: np.ndarray, var_type: List[str]) -&gt; np.ndarray:\n    \"\"\"Round non-continuous values to integers.\"\"\"\n    mask = np.isin(var_type, [\"float\"], invert=True)\n    X[:, mask] = np.around(X[:, mask])\n    return X\n</code></pre> <p>Logic:</p> <ul> <li>Variables with type <code>'float'</code>: No change (continuous)</li> <li>Variables with type <code>'int'</code> or <code>'factor'</code>: Rounded to integers</li> </ul>"},{"location":"manuals/var_type/#usage-examples","title":"Usage Examples","text":""},{"location":"manuals/var_type/#5-example-usage","title":"5. Example Usage","text":"<pre><code>import numpy as np\nfrom spotoptim import SpotOptim\n\n# Example 1: All float variables (default)\nopt1 = SpotOptim(\n    fun=lambda x: np.sum(x**2),\n    lower=np.array([0, 0, 0]),\n    upper=np.array([10, 10, 10])\n    # var_type defaults to [\"float\", \"float\", \"float\"]\n)\n</code></pre>"},{"location":"manuals/var_type/#example-2-pure-integer-optimization","title":"Example 2: Pure Integer Optimization","text":"<pre><code>def discrete_func(X):\n    return np.sum(np.round(X)**2, axis=1)\n\nbounds = [(-5, 5), (-5, 5)]\nvar_type = [\"int\", \"int\"]\n\nopt = SpotOptim(\n    fun=discrete_func,\n    bounds=bounds,\n    var_type=var_type,\n    max_iter=20,\n    seed=42\n)\n\nresult = opt.optimize()\n# result.x will have integer values like [1.0, -2.0]\n</code></pre>"},{"location":"manuals/var_type/#example-3-categorical-factor-variables","title":"Example 3: Categorical (Factor) Variables","text":"<pre><code>def categorical_func(X):\n    # Assume X[:, 0] represents 3 categories: 0, 1, 2\n    # Category 0 is best\n    return (X[:, 0]**2) + (X[:, 1]**2)\n\nbounds = [(0, 2), (0, 3)]  # 3 and 4 categories respectively\nvar_type = [\"factor\", \"factor\"]\n\nopt = SpotOptim(\n    fun=categorical_func,\n    bounds=bounds,\n    var_type=var_type,\n    max_iter=20,\n    seed=42\n)\n\nresult = opt.optimize()\n# result.x will be integers like [0.0, 1.0] representing categories\n</code></pre>"},{"location":"manuals/var_type/#example-4-mixed-variable-types","title":"Example 4: Mixed Variable Types","text":"<pre><code>def mixed_func(X):\n    # X[:, 0]: continuous temperature\n    # X[:, 1]: discrete number of iterations\n    # X[:, 2]: categorical algorithm choice (0, 1, 2)\n    return X[:, 0]**2 + X[:, 1]**2 + X[:, 2]**2\n\nbounds = [(-5, 5), (1, 100), (0, 2)]\nvar_type = [\"float\", \"int\", \"factor\"]\n\nopt = SpotOptim(\n    fun=mixed_func,\n    bounds=bounds,\n    var_type=var_type,\n    max_iter=20,\n    seed=42\n)\n\nresult = opt.optimize()\n# result.x[0]: continuous float like 0.123\n# result.x[1]: integer like 5.0\n# result.x[2]: integer category like 0.0\n</code></pre>"},{"location":"manuals/var_type/#key-findings","title":"Key Findings","text":"<ol> <li> <p>Type Persistence: Variable types are correctly maintained throughout the entire optimization process, from initial design through all iterations.</p> </li> <li> <p>Automatic Enforcement: The <code>_repair_non_numeric()</code> method is called at all critical points, ensuring type constraints are never violated.</p> </li> <li> <p>Three Explicit Types: Only <code>'float'</code>, <code>'int'</code>, and <code>'factor'</code> are supported. The legacy <code>'num'</code> type has been removed for clarity.</p> </li> <li> <p>User-Provided Data: Type constraints are applied even to user-provided initial designs, ensuring consistency.</p> </li> <li> <p>Plotting Compatibility: The plotting functionality respects variable types, ensuring correct visualization of discrete vs. continuous variables.</p> </li> </ol>"},{"location":"manuals/var_type/#recommendations","title":"Recommendations","text":"<ol> <li>Always specify var_type explicitly for clarity, especially in mixed-type problems</li> <li>Use appropriate bounds for factor variables (e.g., <code>(0, n_categories-1)</code>)</li> <li>External mapping for string categories: Maintain your own mapping dictionary outside SpotOptim (e.g., <code>{\"red\": 0, \"green\": 1, \"blue\": 2}</code>)</li> <li>Validation: The current implementation doesn\u2019t validate var_type length matches bounds length - users should ensure this manually</li> </ol>"},{"location":"manuals/var_type/#future-enhancements-optional","title":"Future Enhancements (Optional)","text":"<p>Potential improvements that could be added:</p> <ol> <li>Validation: Add validation in <code>__init__</code> to check <code>len(var_type) == len(bounds)</code></li> <li>String Categories: Add built-in support for automatic string-to-int mapping</li> <li>Ordered Categories: Support ordered categorical variables (ordinal data)</li> <li>Type Checking: Validate that var_type values are one of the allowed strings</li> <li>Bounds Checking: Warn if factor bounds are not integer ranges</li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>spotoptim<ul> <li>SpotOptim</li> <li>data<ul> <li>diabetes</li> </ul> </li> <li>nn<ul> <li>linear_regressor</li> </ul> </li> <li>surrogate<ul> <li>kriging</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/spotoptim/SpotOptim/","title":"SpotOptim","text":""},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim","title":"<code>SpotOptim</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>SPOT optimizer compatible with scipy.optimize interface.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>Objective function to minimize. Should accept array of shape (n_samples, n_features).</p> required <code>bounds</code> <code>list of tuple</code> <p>Bounds for each dimension as [(low, high), \u2026].</p> required <code>max_iter</code> <code>int</code> <p>Maximum number of total function evaluations (including initial design). For example, max_iter=30 with n_initial=10 will perform 10 initial evaluations plus 20 sequential optimization iterations. Defaults to 20.</p> <code>20</code> <code>n_initial</code> <code>int</code> <p>Number of initial design points. Defaults to 10.</p> <code>10</code> <code>surrogate</code> <code>object</code> <p>Surrogate model. Defaults to Gaussian Process with Matern kernel.</p> <code>None</code> <code>acquisition</code> <code>str</code> <p>Acquisition function (\u2018ei\u2019, \u2018y\u2019, \u2018pi\u2019). Defaults to \u2018ei\u2019.</p> <code>'ei'</code> <code>var_type</code> <code>list of str</code> <p>Variable types for each dimension. Supported types: - \u2018float\u2019: Python floats, continuous optimization (no rounding) - \u2018int\u2019: Python int, float values will be rounded to integers - \u2018factor\u2019: Unordered categorical data, internally mapped to int values   (e.g., \u201cred\u201d-&gt;0, \u201cgreen\u201d-&gt;1, etc.) Defaults to None (which sets all dimensions to \u2018float\u2019).</p> <code>None</code> <code>var_name</code> <code>list of str</code> <p>Variable names for each dimension. If None, uses default names [\u2018x0\u2019, \u2018x1\u2019, \u2018x2\u2019, \u2026]. Defaults to None.</p> <code>None</code> <code>tolerance_x</code> <code>float</code> <p>Minimum distance between points. Defaults to np.sqrt(np.spacing(1))</p> <code>None</code> <code>max_time</code> <code>float</code> <p>Maximum runtime in minutes. If np.inf (default), no time limit. The optimization terminates when either max_iter evaluations are reached OR max_time minutes have elapsed, whichever comes first. Defaults to np.inf.</p> <code>inf</code> <code>repeats_initial</code> <code>int</code> <p>Number of times to evaluate each initial design point. Useful for noisy objective functions. If &gt; 1, noise handling is activated and statistics (mean, variance) are tracked. Defaults to 1.</p> <code>1</code> <code>repeats_surrogate</code> <code>int</code> <p>Number of times to evaluate each surrogate-suggested point. Useful for noisy objective functions. If &gt; 1, noise handling is activated and statistics (mean, variance) are tracked. Defaults to 1.</p> <code>1</code> <code>ocba_delta</code> <code>int</code> <p>Number of additional evaluations to allocate using Optimal Computing Budget Allocation (OCBA) when noise handling is active. OCBA determines which existing design points should be re-evaluated to best distinguish between alternatives. Only used when noise=True (repeats &gt; 1) and ocba_delta &gt; 0. Requires at least 3 design points with variance information. Defaults to 0 (no OCBA).</p> <code>0</code> <code>tensorboard_log</code> <code>bool</code> <p>Enable TensorBoard logging. If True, optimization metrics and hyperparameters are logged to TensorBoard. View logs by running: <code>tensorboard --logdir=&lt;tensorboard_path&gt;</code> in a separate terminal. Defaults to False.</p> <code>False</code> <code>tensorboard_path</code> <code>str</code> <p>Path for TensorBoard log files. If None and tensorboard_log is True, creates a default path: runs/spotoptim_YYYYMMDD_HHMMSS. Defaults to None.</p> <code>None</code> <code>tensorboard_clean</code> <code>bool</code> <p>If True, removes all old TensorBoard log directories from the \u2018runs\u2019 folder before starting optimization. Use with caution as this permanently deletes all subdirectories in \u2018runs\u2019. Defaults to False.</p> <code>False</code> <code>fun_mo2so</code> <code>callable</code> <p>Function to convert multi-objective values to single-objective. Takes an array of shape (n_samples, n_objectives) and returns array of shape (n_samples,). If None and objective function returns multi-objective values, uses first objective. Defaults to None.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information. Defaults to False.</p> <code>False</code> <code>warnings_filter</code> <code>str</code> <p>Filter for warnings. One of \u201cerror\u201d, \u201cignore\u201d, \u201calways\u201d, \u201call\u201d, \u201cdefault\u201d, \u201cmodule\u201d, or \u201conce\u201d. Defaults to \u201cignore\u201d.</p> <code>'ignore'</code> <code>max_surrogate_points</code> <code>int</code> <p>Maximum number of points to use for surrogate model fitting. If None, all points are used. If the number of evaluated points exceeds this limit, a subset is selected using the selection method. Defaults to None.</p> <code>None</code> <code>selection_method</code> <code>str</code> <p>Method for selecting points when max_surrogate_points is exceeded. Options: \u2018distant\u2019 (Select points that are distant from each other via K-means clustering) or \u2018best\u2019 (Select all points from the cluster with the best mean objective value). Defaults to \u2018distant\u2019.</p> <code>'distant'</code> <code>acquisition_failure_strategy</code> <code>str</code> <p>Strategy for handling acquisition function failures. Options: \u2018random\u2019 (space-filling design via Latin Hypercube Sampling) or \u2018mm\u2019 (Morris-Mitchell phi minimizing point for maximal distance from existing points). Defaults to \u2018random\u2019.</p> <code>'random'</code> <p>Attributes:</p> Name Type Description <code>X_</code> <code>ndarray</code> <p>All evaluated points, shape (n_samples, n_features).</p> <code>y_</code> <code>ndarray</code> <p>Function values at X_, shape (n_samples,). For multi-objective problems, these are the converted single-objective values.</p> <code>y_mo</code> <code>ndarray or None</code> <p>Multi-objective function values, shape (n_samples, n_objectives). None for single-objective problems.</p> <code>best_x_</code> <code>ndarray</code> <p>Best point found, shape (n_features,).</p> <code>best_y_</code> <code>float</code> <p>Best function value found.</p> <code>n_iter_</code> <code>int</code> <p>Number of iterations performed.</p> <code>counter</code> <code>int</code> <p>Total number of function evaluations.</p> <code>success_rate</code> <code>float</code> <p>Rolling success rate over the last window_size evaluations. A success is counted when a new evaluation improves upon the best value found so far.</p> <code>warnings_filter</code> <code>str</code> <p>Filter for warnings during optimization.</p> <code>max_surrogate_points</code> <code>int or None</code> <p>Maximum number of points for surrogate fitting.</p> <code>selection_method</code> <code>str</code> <p>Point selection method.</p> <code>acquisition_failure_strategy</code> <code>str</code> <p>Strategy for handling acquisition failures (\u2018random\u2019 or \u2018mm\u2019).</p> <code>noise</code> <code>bool</code> <p>True if noise handling is active (repeats &gt; 1).</p> <code>mean_X</code> <code>ndarray or None</code> <p>Aggregated unique design points (if noise=True).</p> <code>mean_y</code> <code>ndarray or None</code> <p>Mean y values per design point (if noise=True).</p> <code>var_y</code> <code>ndarray or None</code> <p>Variance of y values per design point (if noise=True).</p> <code>min_mean_X</code> <code>ndarray or None</code> <p>X value of best mean y (if noise=True).</p> <code>min_mean_y</code> <code>float or None</code> <p>Best mean y value (if noise=True).</p> <code>min_var_y</code> <code>float or None</code> <p>Variance of best mean y (if noise=True).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; def objective(X):\n...     return np.sum(X**2, axis=1)\n...\n&gt;&gt;&gt; # Example 1: Basic usage (deterministic function)\n&gt;&gt;&gt; bounds = [(-5, 5), (-5, 5)]\n&gt;&gt;&gt; optimizer = SpotOptim(fun=objective, bounds=bounds, max_iter=10, n_initial=5, verbose=True)\n&gt;&gt;&gt; result = optimizer.optimize()\n&gt;&gt;&gt; print(\"Best x:\", result.x)\n&gt;&gt;&gt; print(\"Best f(x):\", result.fun)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: With custom variable names\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     var_name=[\"param1\", \"param2\"],\n...     max_iter=10,\n...     n_initial=5\n... )\n&gt;&gt;&gt; result = optimizer.optimize()\n&gt;&gt;&gt; optimizer.plot_surrogate()  # Uses custom names in plot labels\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Noisy function with repeated evaluations\n&gt;&gt;&gt; def noisy_objective(X):\n...     import numpy as np\n...     base = np.sum(X**2, axis=1)\n...     noise = np.random.normal(0, 0.1, size=base.shape)\n...     return base + noise\n...\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=noisy_objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     repeats_initial=3,      # Evaluate each initial point 3 times\n...     repeats_surrogate=2,    # Evaluate each new point 2 times\n...     seed=42,                # For reproducibility\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer.optimize()\n&gt;&gt;&gt; # Access noise statistics\n&gt;&gt;&gt; print(\"Unique design points:\", optimizer.mean_X.shape[0])\n&gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n&gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: Noisy function with OCBA (Optimal Computing Budget Allocation)\n&gt;&gt;&gt; optimizer_ocba = SpotOptim(\n...     fun=noisy_objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=50,\n...     n_initial=10,\n...     repeats_initial=2,      # Initial repeats\n...     repeats_surrogate=1,    # Surrogate repeats\n...     ocba_delta=3,           # Allocate 3 additional evaluations per iteration\n...     seed=42,\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer_ocba.optimize()\n&gt;&gt;&gt; # OCBA intelligently re-evaluates promising points to reduce uncertainty\n&gt;&gt;&gt; print(\"Total evaluations:\", result.nfev)\n&gt;&gt;&gt; print(\"Unique design points:\", optimizer_ocba.mean_X.shape[0])\n&gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n&gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 5: With TensorBoard logging\n&gt;&gt;&gt; optimizer_tb = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     tensorboard_log=True,   # Enable TensorBoard\n...     tensorboard_path=\"runs/my_optimization\",  # Optional custom path\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer_tb.optimize()\n&gt;&gt;&gt; # View logs in browser: tensorboard --logdir=runs/my_optimization\n&gt;&gt;&gt; print(\"Logs saved to:\", optimizer_tb.tensorboard_path)\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>class SpotOptim(BaseEstimator):\n    \"\"\"SPOT optimizer compatible with scipy.optimize interface.\n\n    Args:\n        fun (callable): Objective function to minimize. Should accept array of shape (n_samples, n_features).\n        bounds (list of tuple): Bounds for each dimension as [(low, high), ...].\n        max_iter (int, optional): Maximum number of total function evaluations (including initial design).\n            For example, max_iter=30 with n_initial=10 will perform 10 initial evaluations plus\n            20 sequential optimization iterations. Defaults to 20.\n        n_initial (int, optional): Number of initial design points. Defaults to 10.\n        surrogate (object, optional): Surrogate model. Defaults to Gaussian Process with Matern kernel.\n        acquisition (str, optional): Acquisition function ('ei', 'y', 'pi'). Defaults to 'ei'.\n        var_type (list of str, optional): Variable types for each dimension. Supported types:\n            - 'float': Python floats, continuous optimization (no rounding)\n            - 'int': Python int, float values will be rounded to integers\n            - 'factor': Unordered categorical data, internally mapped to int values\n              (e.g., \"red\"-&gt;0, \"green\"-&gt;1, etc.)\n            Defaults to None (which sets all dimensions to 'float').\n        var_name (list of str, optional): Variable names for each dimension.\n            If None, uses default names ['x0', 'x1', 'x2', ...]. Defaults to None.\n        tolerance_x (float, optional): Minimum distance between points. Defaults to np.sqrt(np.spacing(1))\n        max_time (float, optional): Maximum runtime in minutes. If np.inf (default), no time limit.\n            The optimization terminates when either max_iter evaluations are reached OR max_time\n            minutes have elapsed, whichever comes first. Defaults to np.inf.\n        repeats_initial (int, optional): Number of times to evaluate each initial design point.\n            Useful for noisy objective functions. If &gt; 1, noise handling is activated and\n            statistics (mean, variance) are tracked. Defaults to 1.\n        repeats_surrogate (int, optional): Number of times to evaluate each surrogate-suggested point.\n            Useful for noisy objective functions. If &gt; 1, noise handling is activated and\n            statistics (mean, variance) are tracked. Defaults to 1.\n        ocba_delta (int, optional): Number of additional evaluations to allocate using Optimal Computing\n            Budget Allocation (OCBA) when noise handling is active. OCBA determines which existing\n            design points should be re-evaluated to best distinguish between alternatives. Only used\n            when noise=True (repeats &gt; 1) and ocba_delta &gt; 0. Requires at least 3 design points with\n            variance information. Defaults to 0 (no OCBA).\n        tensorboard_log (bool, optional): Enable TensorBoard logging. If True, optimization metrics\n            and hyperparameters are logged to TensorBoard. View logs by running:\n            `tensorboard --logdir=&lt;tensorboard_path&gt;` in a separate terminal. Defaults to False.\n        tensorboard_path (str, optional): Path for TensorBoard log files. If None and tensorboard_log\n            is True, creates a default path: runs/spotoptim_YYYYMMDD_HHMMSS. Defaults to None.\n        tensorboard_clean (bool, optional): If True, removes all old TensorBoard log directories from\n            the 'runs' folder before starting optimization. Use with caution as this permanently\n            deletes all subdirectories in 'runs'. Defaults to False.\n        fun_mo2so (callable, optional): Function to convert multi-objective values to single-objective.\n            Takes an array of shape (n_samples, n_objectives) and returns array of shape (n_samples,).\n            If None and objective function returns multi-objective values, uses first objective.\n            Defaults to None.\n        seed (int, optional): Random seed for reproducibility. Defaults to None.\n        verbose (bool, optional): Print progress information. Defaults to False.\n        warnings_filter (str, optional): Filter for warnings. One of \"error\", \"ignore\", \"always\", \"all\",\n            \"default\", \"module\", or \"once\". Defaults to \"ignore\".\n        max_surrogate_points (int, optional): Maximum number of points to use for surrogate model fitting.\n            If None, all points are used. If the number of evaluated points exceeds this limit,\n            a subset is selected using the selection method. Defaults to None.\n        selection_method (str, optional): Method for selecting points when max_surrogate_points is exceeded.\n            Options: 'distant' (Select points that are distant from each other via K-means clustering) or\n            'best' (Select all points from the cluster with the best mean objective value).\n            Defaults to 'distant'.\n        acquisition_failure_strategy (str, optional): Strategy for handling acquisition function failures.\n            Options: 'random' (space-filling design via Latin Hypercube Sampling) or\n            'mm' (Morris-Mitchell phi minimizing point for maximal distance from existing points).\n            Defaults to 'random'.\n\n    Attributes:\n        X_ (ndarray): All evaluated points, shape (n_samples, n_features).\n        y_ (ndarray): Function values at X_, shape (n_samples,). For multi-objective problems,\n            these are the converted single-objective values.\n        y_mo (ndarray or None): Multi-objective function values, shape (n_samples, n_objectives).\n            None for single-objective problems.\n        best_x_ (ndarray): Best point found, shape (n_features,).\n        best_y_ (float): Best function value found.\n        n_iter_ (int): Number of iterations performed.\n        counter (int): Total number of function evaluations.\n        success_rate (float): Rolling success rate over the last window_size evaluations.\n            A success is counted when a new evaluation improves upon the best value found so far.\n        warnings_filter (str): Filter for warnings during optimization.\n        max_surrogate_points (int or None): Maximum number of points for surrogate fitting.\n        selection_method (str): Point selection method.\n        acquisition_failure_strategy (str): Strategy for handling acquisition failures ('random' or 'mm').\n        noise (bool): True if noise handling is active (repeats &gt; 1).\n        mean_X (ndarray or None): Aggregated unique design points (if noise=True).\n        mean_y (ndarray or None): Mean y values per design point (if noise=True).\n        var_y (ndarray or None): Variance of y values per design point (if noise=True).\n        min_mean_X (ndarray or None): X value of best mean y (if noise=True).\n        min_mean_y (float or None): Best mean y value (if noise=True).\n        min_var_y (float or None): Variance of best mean y (if noise=True).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; def objective(X):\n        ...     return np.sum(X**2, axis=1)\n        ...\n        &gt;&gt;&gt; # Example 1: Basic usage (deterministic function)\n        &gt;&gt;&gt; bounds = [(-5, 5), (-5, 5)]\n        &gt;&gt;&gt; optimizer = SpotOptim(fun=objective, bounds=bounds, max_iter=10, n_initial=5, verbose=True)\n        &gt;&gt;&gt; result = optimizer.optimize()\n        &gt;&gt;&gt; print(\"Best x:\", result.x)\n        &gt;&gt;&gt; print(\"Best f(x):\", result.fun)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: With custom variable names\n        &gt;&gt;&gt; optimizer = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     var_name=[\"param1\", \"param2\"],\n        ...     max_iter=10,\n        ...     n_initial=5\n        ... )\n        &gt;&gt;&gt; result = optimizer.optimize()\n        &gt;&gt;&gt; optimizer.plot_surrogate()  # Uses custom names in plot labels\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 3: Noisy function with repeated evaluations\n        &gt;&gt;&gt; def noisy_objective(X):\n        ...     import numpy as np\n        ...     base = np.sum(X**2, axis=1)\n        ...     noise = np.random.normal(0, 0.1, size=base.shape)\n        ...     return base + noise\n        ...\n        &gt;&gt;&gt; optimizer = SpotOptim(\n        ...     fun=noisy_objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     repeats_initial=3,      # Evaluate each initial point 3 times\n        ...     repeats_surrogate=2,    # Evaluate each new point 2 times\n        ...     seed=42,                # For reproducibility\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer.optimize()\n        &gt;&gt;&gt; # Access noise statistics\n        &gt;&gt;&gt; print(\"Unique design points:\", optimizer.mean_X.shape[0])\n        &gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n        &gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 4: Noisy function with OCBA (Optimal Computing Budget Allocation)\n        &gt;&gt;&gt; optimizer_ocba = SpotOptim(\n        ...     fun=noisy_objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=50,\n        ...     n_initial=10,\n        ...     repeats_initial=2,      # Initial repeats\n        ...     repeats_surrogate=1,    # Surrogate repeats\n        ...     ocba_delta=3,           # Allocate 3 additional evaluations per iteration\n        ...     seed=42,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer_ocba.optimize()\n        &gt;&gt;&gt; # OCBA intelligently re-evaluates promising points to reduce uncertainty\n        &gt;&gt;&gt; print(\"Total evaluations:\", result.nfev)\n        &gt;&gt;&gt; print(\"Unique design points:\", optimizer_ocba.mean_X.shape[0])\n        &gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n        &gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 5: With TensorBoard logging\n        &gt;&gt;&gt; optimizer_tb = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     tensorboard_log=True,   # Enable TensorBoard\n        ...     tensorboard_path=\"runs/my_optimization\",  # Optional custom path\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer_tb.optimize()\n        &gt;&gt;&gt; # View logs in browser: tensorboard --logdir=runs/my_optimization\n        &gt;&gt;&gt; print(\"Logs saved to:\", optimizer_tb.tensorboard_path)\n    \"\"\"\n\n    def __init__(\n        self,\n        fun: Callable,\n        bounds: list,\n        max_iter: int = 20,\n        n_initial: int = 10,\n        surrogate: Optional[object] = None,\n        acquisition: str = \"ei\",\n        var_type: Optional[list] = None,\n        var_name: Optional[list] = None,\n        tolerance_x: Optional[float] = None,\n        max_time: float = np.inf,\n        repeats_initial: int = 1,\n        repeats_surrogate: int = 1,\n        ocba_delta: int = 0,\n        tensorboard_log: bool = False,\n        tensorboard_path: Optional[str] = None,\n        tensorboard_clean: bool = False,\n        fun_mo2so: Optional[Callable] = None,\n        seed: Optional[int] = None,\n        verbose: bool = False,\n        warnings_filter: str = \"ignore\",\n        max_surrogate_points: Optional[int] = None,\n        selection_method: str = \"distant\",\n        acquisition_failure_strategy: str = \"random\",\n    ):\n\n        warnings.filterwarnings(warnings_filter)\n\n        # small value, converted to float\n        self.eps = np.sqrt(np.spacing(1))\n\n        if tolerance_x is None:\n            self.tolerance_x = self.eps\n        else:\n            self.tolerance_x = tolerance_x\n\n        # Validate parameters\n        if max_iter &lt; n_initial:\n            raise ValueError(\n                f\"max_iter ({max_iter}) must be &gt;= n_initial ({n_initial}). \"\n                f\"max_iter represents the total function evaluation budget including initial design.\"\n            )\n\n        self.fun = fun\n        self.bounds = bounds\n        self.max_iter = max_iter\n        self.n_initial = n_initial\n        self.surrogate = surrogate\n        self.acquisition = acquisition\n        self.var_type = var_type\n        self.var_name = var_name\n        self.max_time = max_time\n        self.repeats_initial = repeats_initial\n        self.repeats_surrogate = repeats_surrogate\n        self.ocba_delta = ocba_delta\n        self.tensorboard_log = tensorboard_log\n        self.tensorboard_path = tensorboard_path\n        self.tensorboard_clean = tensorboard_clean\n        self.fun_mo2so = fun_mo2so\n        self.seed = seed\n        self.verbose = verbose\n        self.max_surrogate_points = max_surrogate_points\n        self.selection_method = selection_method\n        self.acquisition_failure_strategy = acquisition_failure_strategy\n\n        # Determine if noise handling is active\n        self.noise = (repeats_initial &gt; 1) or (repeats_surrogate &gt; 1)\n\n        # Derived attributes\n        self.n_dim = len(bounds)\n        self.lower = np.array([b[0] for b in bounds])\n        self.upper = np.array([b[1] for b in bounds])\n\n        # Default variable types\n        if self.var_type is None:\n            self.var_type = [\"float\"] * self.n_dim\n\n        # Default variable names\n        if self.var_name is None:\n            self.var_name = [f\"x{i}\" for i in range(self.n_dim)]\n\n        # Dimension reduction: backup original bounds and identify fixed dimensions\n        self._setup_dimension_reduction()\n\n        # Initialize surrogate if not provided\n        if self.surrogate is None:\n            kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(\n                length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=2.5\n            )\n            self.surrogate = GaussianProcessRegressor(\n                kernel=kernel,\n                n_restarts_optimizer=10,\n                normalize_y=True,\n                random_state=self.seed,\n            )\n\n        # Design generator\n        self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n        # Storage for results\n        self.X_ = None\n        self.y_ = None\n        self.y_mo = None  # Multi-objective values (if applicable)\n        self.best_x_ = None\n        self.best_y_ = None\n        self.n_iter_ = 0\n\n        # Noise handling attributes (initialized in update_stats if noise=True)\n        self.mean_X = None\n        self.mean_y = None\n        self.var_y = None\n        self.min_mean_X = None\n        self.min_mean_y = None\n        self.min_var_y = None\n        self.min_X = None\n        self.min_y = None\n        self.counter = 0\n\n        # Success rate tracking (similar to Spot class)\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.window_size = 100\n        self._success_history = []\n\n        # Clean old TensorBoard logs if requested\n        self._clean_tensorboard_logs()\n\n        # Initialize TensorBoard writer\n        self._init_tensorboard_writer()\n\n    def _setup_dimension_reduction(self) -&gt; None:\n        \"\"\"Set up dimension reduction by identifying fixed dimensions.\n\n        This method identifies dimensions where lower and upper bounds are equal,\n        indicating fixed (constant) variables. It stores:\n        - Original bounds and metadata in `all_*` attributes\n        - Boolean mask of fixed dimensions in `ident`\n        - Reduced bounds, types, and names for optimization\n        - `red_dim` flag indicating if reduction occurred\n        \"\"\"\n        # Backup original values\n        self.all_lower = self.lower.copy()\n        self.all_upper = self.upper.copy()\n        self.all_var_type = self.var_type.copy()\n        self.all_var_name = self.var_name.copy()\n\n        # Identify fixed dimensions (lower == upper)\n        self.ident = (self.upper - self.lower) == 0\n\n        # Check if any dimension is fixed\n        self.red_dim = self.ident.any()\n\n        if self.red_dim:\n            # Reduce bounds to only varying dimensions\n            self.lower = self.lower[~self.ident]\n            self.upper = self.upper[~self.ident]\n\n            # Update dimension count\n            self.n_dim = self.lower.size\n\n            # Reduce variable types and names\n            self.var_type = [\n                vtype\n                for vtype, fixed in zip(self.all_var_type, self.ident)\n                if not fixed\n            ]\n            self.var_name = [\n                vname\n                for vname, fixed in zip(self.all_var_name, self.ident)\n                if not fixed\n            ]\n\n            # Update bounds list for reduced dimensions\n            self.bounds = [(self.lower[i], self.upper[i]) for i in range(self.n_dim)]\n\n            # Recreate LHS sampler with reduced dimensions\n            self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n    def to_all_dim(self, X_red: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Expand reduced-dimensional points to full-dimensional representation.\n\n        This method restores points from the reduced optimization space to the\n        full-dimensional space by inserting fixed values for constant dimensions.\n\n        Args:\n            X_red (ndarray): Points in reduced space, shape (n_samples, n_reduced_dims).\n\n        Returns:\n            ndarray: Points in full space, shape (n_samples, n_original_dims).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Create problem with one fixed dimension\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n            ...     max_iter=1,\n            ...     n_initial=3\n            ... )\n            &gt;&gt;&gt; X_red = np.array([[1.0, 3.0], [2.0, 4.0]])  # Only x0 and x2\n            &gt;&gt;&gt; X_full = opt.to_all_dim(X_red)\n            &gt;&gt;&gt; X_full.shape\n            (2, 3)\n            &gt;&gt;&gt; X_full[:, 1]  # Middle dimension should be 2.0\n            array([2., 2.])\n        \"\"\"\n        if not self.red_dim:\n            # No reduction occurred, return as-is\n            return X_red\n\n        # Number of samples and full dimensions\n        n_samples = X_red.shape[0]\n        n_full_dims = len(self.ident)\n\n        # Initialize full-dimensional array\n        X_full = np.zeros((n_samples, n_full_dims))\n\n        # Track index in reduced array\n        red_idx = 0\n\n        # Fill in values dimension by dimension\n        for i in range(n_full_dims):\n            if self.ident[i]:\n                # Fixed dimension: use stored value\n                X_full[:, i] = self.all_lower[i]\n            else:\n                # Varying dimension: use value from reduced array\n                X_full[:, i] = X_red[:, red_idx]\n                red_idx += 1\n\n        return X_full\n\n    def to_red_dim(self, X_full: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reduce full-dimensional points to optimization space.\n\n        This method removes fixed dimensions from full-dimensional points,\n        extracting only the varying dimensions used in optimization.\n\n        Args:\n            X_full (ndarray): Points in full space, shape (n_samples, n_original_dims).\n\n        Returns:\n            ndarray: Points in reduced space, shape (n_samples, n_reduced_dims).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Create problem with one fixed dimension\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n            ...     max_iter=1,\n            ...     n_initial=3\n            ... )\n            &gt;&gt;&gt; X_full = np.array([[1.0, 2.0, 3.0], [4.0, 2.0, 5.0]])\n            &gt;&gt;&gt; X_red = opt.to_red_dim(X_full)\n            &gt;&gt;&gt; X_red.shape\n            (2, 2)\n            &gt;&gt;&gt; np.array_equal(X_red, np.array([[1.0, 3.0], [4.0, 5.0]]))\n            True\n        \"\"\"\n        if not self.red_dim:\n            # No reduction occurred, return as-is\n            return X_full\n\n        # Select only non-fixed dimensions\n        return X_full[:, ~self.ident]\n\n    def _aggregate_mean_var(\n        self, X: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Aggregate X and y values to compute mean and variance per group.\n\n        For repeated evaluations at the same design point, this method computes\n        the mean function value and variance (using population variance, ddof=0).\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values, shape (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - X_agg (ndarray): Unique design points, shape (n_groups, n_features)\n                - y_mean (ndarray): Mean y values per group, shape (n_groups,)\n                - y_var (ndarray): Variance of y values per group, shape (n_groups,)\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 repeats_initial=2)\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2]])\n            &gt;&gt;&gt; y = np.array([1, 2, 3])\n            &gt;&gt;&gt; X_agg, y_mean, y_var = opt._aggregate_mean_var(X, y)\n            &gt;&gt;&gt; X_agg.shape\n            (2, 2)\n            &gt;&gt;&gt; y_mean\n            array([2., 2.])\n            &gt;&gt;&gt; y_var\n            array([1., 0.])\n        \"\"\"\n        # Input validation\n        X = np.asarray(X)\n        y = np.asarray(y)\n\n        if X.ndim != 2 or y.ndim != 1 or X.shape[0] != y.shape[0]:\n            raise ValueError(\"Invalid input shapes for _aggregate_mean_var\")\n\n        if X.shape[0] == 0:\n            return np.empty((0, X.shape[1])), np.array([]), np.array([])\n\n        # Find unique rows and group indices\n        _, unique_idx, inverse_idx = np.unique(\n            X, axis=0, return_index=True, return_inverse=True\n        )\n\n        X_agg = X[unique_idx]\n\n        # Calculate mean and variance for each group\n        n_groups = len(unique_idx)\n        y_mean = np.zeros(n_groups)\n        y_var = np.zeros(n_groups)\n\n        for i in range(n_groups):\n            group_mask = inverse_idx == i\n            group_y = y[group_mask]\n            y_mean[i] = np.mean(group_y)\n            # Use population variance (ddof=0) for consistency with Spot\n            y_var[i] = np.var(group_y, ddof=0)\n\n        return X_agg, y_mean, y_var\n\n    def update_stats(self) -&gt; None:\n        \"\"\"Update optimization statistics.\n\n        Updates:\n        1. `min_y`: Minimum y value found so far\n        2. `min_X`: X value corresponding to minimum y\n        3. `counter`: Total number of function evaluations\n\n        Note: `success_rate` is updated separately via `_update_success_rate()` method,\n        which is called after each batch of function evaluations.\n\n        If `noise` is True (repeats &gt; 1), additionally computes:\n        1. `mean_X`: Unique design points (aggregated from repeated evaluations)\n        2. `mean_y`: Mean y values per design point\n        3. `var_y`: Variance of y values per design point\n        4. `min_mean_X`: X value of the best mean y value\n        5. `min_mean_y`: Best mean y value\n        6. `min_var_y`: Variance of the best mean y value\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Without noise\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_iter=10, n_initial=5)\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0, 1.0])\n            &gt;&gt;&gt; opt.update_stats()\n            &gt;&gt;&gt; opt.min_y\n            1.0\n            &gt;&gt;&gt; opt.min_X\n            array([0, 1])\n            &gt;&gt;&gt; opt.counter\n            3\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With noise\n            &gt;&gt;&gt; opt_noise = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                       bounds=[(-5, 5), (-5, 5)],\n            ...                       repeats_initial=2)\n            &gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [1, 2], [3, 4]])\n            &gt;&gt;&gt; opt_noise.y_ = np.array([4.0, 6.0, 25.0])\n            &gt;&gt;&gt; opt_noise.update_stats()\n            &gt;&gt;&gt; opt_noise.min_y\n            4.0\n            &gt;&gt;&gt; opt_noise.mean_y\n            array([ 5., 25.])\n            &gt;&gt;&gt; opt_noise.var_y\n            array([1., 0.])\n        \"\"\"\n        if self.y_ is None or len(self.y_) == 0:\n            return\n\n        # Basic stats\n        self.min_y = np.min(self.y_)\n        self.min_X = self.X_[np.argmin(self.y_)]\n        self.counter = len(self.y_)\n\n        # Aggregated stats for noisy functions\n        if self.noise:\n            self.mean_X, self.mean_y, self.var_y = self._aggregate_mean_var(\n                self.X_, self.y_\n            )\n            # X value of the best mean y value so far\n            best_mean_idx = np.argmin(self.mean_y)\n            self.min_mean_X = self.mean_X[best_mean_idx]\n            # Best mean y value so far\n            self.min_mean_y = self.mean_y[best_mean_idx]\n            # Variance of the best mean y value so far\n            self.min_var_y = self.var_y[best_mean_idx]\n\n    def _update_success_rate(self, y_new: np.ndarray) -&gt; None:\n        \"\"\"Update the rolling success rate of the optimization process.\n\n        A success is counted only if the new value is better (smaller) than the best\n        found y value so far. The success rate is calculated based on the last\n        `window_size` successes.\n\n        Important: This method should be called BEFORE updating self.y_ to correctly\n        track improvements against the previous best value.\n\n        Args:\n            y_new (ndarray): The new function values to consider for the success rate update.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_iter=10, n_initial=5)\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 3.0, 2.0])\n            &gt;&gt;&gt; opt._update_success_rate(np.array([1.5, 2.5]))\n            &gt;&gt;&gt; opt.success_rate &gt; 0\n            True\n        \"\"\"\n        # Initialize or update the rolling history of successes (1 for success, 0 for failure)\n        if not hasattr(self, \"_success_history\") or self._success_history is None:\n            self._success_history = []\n\n        # Get the best y value so far (before adding new evaluations)\n        # Since this is called BEFORE updating self.y_, we can safely use min(self.y_)\n        if self.y_ is not None and len(self.y_) &gt; 0:\n            best_y_before = min(self.y_)\n        else:\n            # This is the initial design, no previous best\n            best_y_before = float(\"inf\")\n\n        successes = []\n        current_best = best_y_before\n\n        for val in y_new:\n            if val &lt; current_best:\n                successes.append(1)\n                current_best = val  # Update for next comparison within this batch\n            else:\n                successes.append(0)\n\n        # Add new successes to the history\n        self._success_history.extend(successes)\n        # Keep only the last window_size successes\n        self._success_history = self._success_history[-self.window_size :]\n\n        # Calculate the rolling success rate\n        window_size = len(self._success_history)\n        num_successes = sum(self._success_history)\n        self.success_rate = num_successes / window_size if window_size &gt; 0 else 0.0\n\n    def _get_success_rate(self) -&gt; float:\n        \"\"\"Get the current success rate of the optimization process.\n\n        Returns:\n            float: The current success rate.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: x,\n            ...                 bounds=[(-5, 5), (-5, 5)])\n            &gt;&gt;&gt; print(opt._get_success_rate())\n            0.0\n        \"\"\"\n        return float(getattr(self, \"success_rate\", 0.0) or 0.0)\n\n    def _clean_tensorboard_logs(self) -&gt; None:\n        \"\"\"Clean old TensorBoard log directories from the runs folder.\n\n        Removes all subdirectories in the 'runs' directory if tensorboard_clean is True.\n        This is useful for removing old logs before starting a new optimization run.\n\n        Warning:\n            This will permanently delete all subdirectories in the 'runs' folder.\n            Use with caution.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     tensorboard_log=True,\n            ...     tensorboard_clean=True\n            ... )\n            &gt;&gt;&gt; # Old logs in 'runs' will be removed before optimization starts\n        \"\"\"\n        if self.tensorboard_clean:\n            runs_dir = \"runs\"\n            if os.path.exists(runs_dir) and os.path.isdir(runs_dir):\n                # Get all subdirectories in runs\n                subdirs = [\n                    os.path.join(runs_dir, d)\n                    for d in os.listdir(runs_dir)\n                    if os.path.isdir(os.path.join(runs_dir, d))\n                ]\n\n                if subdirs:\n                    removed_count = 0\n                    for subdir in subdirs:\n                        try:\n                            shutil.rmtree(subdir)\n                            removed_count += 1\n                            if self.verbose:\n                                print(f\"Removed old TensorBoard logs: {subdir}\")\n                        except Exception as e:\n                            if self.verbose:\n                                print(f\"Warning: Could not remove {subdir}: {e}\")\n\n                    if self.verbose and removed_count &gt; 0:\n                        print(\n                            f\"Cleaned {removed_count} old TensorBoard log director{'y' if removed_count == 1 else 'ies'}\"\n                        )\n                elif self.verbose:\n                    print(\"No old TensorBoard logs to clean in 'runs' directory\")\n            elif self.verbose:\n                print(\"'runs' directory does not exist, nothing to clean\")\n\n    def _init_tensorboard_writer(self) -&gt; None:\n        \"\"\"Initialize TensorBoard SummaryWriter if logging is enabled.\n\n        Creates a unique log directory based on timestamp if tensorboard_log is True.\n        The log directory will be in the format: runs/spotoptim_YYYYMMDD_HHMMSS\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     tensorboard_log=True\n            ... )\n            &gt;&gt;&gt; hasattr(opt, 'tb_writer')\n            True\n        \"\"\"\n        if self.tensorboard_log:\n            if self.tensorboard_path is None:\n                # Create default path with timestamp\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                self.tensorboard_path = f\"runs/spotoptim_{timestamp}\"\n\n            # Create directory if it doesn't exist\n            os.makedirs(self.tensorboard_path, exist_ok=True)\n\n            self.tb_writer = SummaryWriter(log_dir=self.tensorboard_path)\n            if self.verbose:\n                print(f\"TensorBoard logging enabled: {self.tensorboard_path}\")\n        else:\n            self.tb_writer = None\n            if self.verbose:\n                print(\"TensorBoard logging disabled\")\n\n    def _write_tensorboard_scalars(self) -&gt; None:\n        \"\"\"Write scalar metrics to TensorBoard.\n\n        Logs the following metrics:\n        - Best y value found so far (min_y)\n        - Last y value evaluated\n        - Best X coordinates (for each dimension)\n        - If noise=True: also logs mean values and variance\n        \"\"\"\n        if self.tb_writer is None or self.y_ is None or len(self.y_) == 0:\n            return\n\n        step = self.counter\n        y_last = self.y_[-1]\n\n        if not self.noise:\n            # Non-noisy optimization\n            self.tb_writer.add_scalars(\n                \"y_values\", {\"min\": self.min_y, \"last\": y_last}, step\n            )\n            # Log success rate\n            self.tb_writer.add_scalar(\"success_rate\", self.success_rate, step)\n            # Log best X coordinates\n            for i in range(self.n_dim):\n                self.tb_writer.add_scalar(f\"X_best/x{i}\", self.min_X[i], step)\n        else:\n            # Noisy optimization\n            self.tb_writer.add_scalars(\n                \"y_values\",\n                {\"min\": self.min_y, \"mean_best\": self.min_mean_y, \"last\": y_last},\n                step,\n            )\n            # Log variance of best mean\n            self.tb_writer.add_scalar(\"y_variance_at_best\", self.min_var_y, step)\n            # Log success rate\n            self.tb_writer.add_scalar(\"success_rate\", self.success_rate, step)\n\n            # Log best X coordinates (by mean)\n            for i in range(self.n_dim):\n                self.tb_writer.add_scalar(f\"X_mean_best/x{i}\", self.min_mean_X[i], step)\n\n        self.tb_writer.flush()\n\n    def _write_tensorboard_hparams(self, X: np.ndarray, y: float) -&gt; None:\n        \"\"\"Write hyperparameters and metric to TensorBoard.\n\n        Args:\n            X (ndarray): Design point coordinates, shape (n_features,)\n            y (float): Function value at X\n        \"\"\"\n        if self.tb_writer is None:\n            return\n\n        # Create hyperparameter dict with variable names\n        hparam_dict = {self.var_name[i]: float(X[i]) for i in range(self.n_dim)}\n        metric_dict = {\"hp_metric\": float(y)}\n\n        self.tb_writer.add_hparams(hparam_dict, metric_dict)\n        self.tb_writer.flush()\n\n    def _close_tensorboard_writer(self) -&gt; None:\n        \"\"\"Close TensorBoard writer and cleanup.\"\"\"\n        if hasattr(self, \"tb_writer\") and self.tb_writer is not None:\n            self.tb_writer.flush()\n            self.tb_writer.close()\n            if self.verbose:\n                print(\n                    f\"TensorBoard writer closed. View logs with: tensorboard --logdir={self.tensorboard_path}\"\n                )\n            del self.tb_writer\n\n    def _get_shape(self, y: np.ndarray) -&gt; Tuple[int, Optional[int]]:\n        \"\"\"Get the shape of the objective function output.\n\n        Args:\n            y (ndarray): Objective function output, shape (n_samples,) or (n_samples, n_objectives).\n\n        Returns:\n            tuple: (n_samples, n_objectives) where n_objectives is None for single-objective.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_single = np.array([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; n, m = opt._get_shape(y_single)\n            &gt;&gt;&gt; print(f\"n={n}, m={m}\")\n            n=3, m=None\n            &gt;&gt;&gt; y_multi = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            &gt;&gt;&gt; n, m = opt._get_shape(y_multi)\n            &gt;&gt;&gt; print(f\"n={n}, m={m}\")\n            n=3, m=2\n        \"\"\"\n        if y.ndim == 1:\n            return y.shape[0], None\n        elif y.ndim == 2:\n            return y.shape[0], y.shape[1]\n        else:\n            # For higher dimensions, flatten to 1D\n            return y.size, None\n\n    def _store_mo(self, y_mo: np.ndarray) -&gt; None:\n        \"\"\"Store multi-objective values in self.y_mo.\n\n        If multi-objective values are present (ndim==2), they are stored in self.y_mo.\n        New values are appended to existing ones. For single-objective problems,\n        self.y_mo remains None.\n\n        Args:\n            y_mo (ndarray): If multi-objective, shape (n_samples, n_objectives).\n                           If single-objective, shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.column_stack([\n            ...         np.sum(X**2, axis=1),\n            ...         np.sum((X-1)**2, axis=1)\n            ...     ]),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_mo_1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n            &gt;&gt;&gt; opt._store_mo(y_mo_1)\n            &gt;&gt;&gt; print(f\"y_mo after first call: {opt.y_mo}\")\n            y_mo after first call: [[1. 2.]\n             [3. 4.]]\n            &gt;&gt;&gt; y_mo_2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n            &gt;&gt;&gt; opt._store_mo(y_mo_2)\n            &gt;&gt;&gt; print(f\"y_mo after second call: {opt.y_mo}\")\n            y_mo after second call: [[1. 2.]\n             [3. 4.]\n             [5. 6.]\n             [7. 8.]]\n        \"\"\"\n        # Store y_mo in self.y_mo (append new values) if multi-objective\n        if self.y_mo is None and y_mo.ndim == 2:\n            self.y_mo = y_mo\n        elif y_mo.ndim == 2:\n            self.y_mo = np.vstack([self.y_mo, y_mo])\n\n    def _mo2so(self, y_mo: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert multi-objective values to single-objective.\n\n        Converts multi-objective values to a single-objective value by applying a user-defined\n        function from `fun_mo2so`. If no user-defined function is given, the\n        values in the first objective column are used.\n\n        This method is called after the objective function evaluation. It returns a 1D array\n        with the single-objective values.\n\n        Args:\n            y_mo (ndarray): If multi-objective, shape (n_samples, n_objectives).\n                           If single-objective, shape (n_samples,).\n\n        Returns:\n            ndarray: Single-objective values, shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Multi-objective function\n            &gt;&gt;&gt; def mo_fun(X):\n            ...     return np.column_stack([\n            ...         np.sum(X**2, axis=1),\n            ...         np.sum((X-1)**2, axis=1)\n            ...     ])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 1: Default behavior (use first objective)\n            &gt;&gt;&gt; opt1 = SpotOptim(\n            ...     fun=mo_fun,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_mo = np.array([[1.0, 2.0], [3.0, 4.0]])\n            &gt;&gt;&gt; y_so = opt1._mo2so(y_mo)\n            &gt;&gt;&gt; print(f\"Single-objective (default): {y_so}\")\n            Single-objective (default): [1. 3.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: Custom conversion function (sum of objectives)\n            &gt;&gt;&gt; def custom_mo2so(y_mo):\n            ...     return y_mo[:, 0] + y_mo[:, 1]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; opt2 = SpotOptim(\n            ...     fun=mo_fun,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5,\n            ...     fun_mo2so=custom_mo2so\n            ... )\n            &gt;&gt;&gt; y_so_custom = opt2._mo2so(y_mo)\n            &gt;&gt;&gt; print(f\"Single-objective (custom): {y_so_custom}\")\n            Single-objective (custom): [3. 7.]\n        \"\"\"\n        n, m = self._get_shape(y_mo)\n        self._store_mo(y_mo)\n\n        # Use ndim to check if multi-objective\n        if y_mo.ndim == 2:\n            if self.fun_mo2so is not None:\n                # Apply user-defined conversion function\n                y0 = self.fun_mo2so(y_mo)\n            else:\n                # Default: use first column\n                if y_mo.size &gt; 0:\n                    y0 = y_mo[:, 0]\n                else:\n                    y0 = y_mo\n        else:\n            # Single-objective, return as-is\n            y0 = y_mo\n\n        return y0\n\n    def _get_ranks(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Returns ranks of numbers within input array x.\n\n        Args:\n            x (ndarray): Input array.\n\n        Returns:\n            ndarray: Ranks array where ranks[i] is the rank of x[i].\n\n        Examples:\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; opt._get_ranks(np.array([2, 1]))\n            array([1, 0])\n            &gt;&gt;&gt; opt._get_ranks(np.array([20, 10, 100]))\n            array([1, 0, 2])\n        \"\"\"\n        ts = x.argsort()\n        ranks = np.empty_like(ts)\n        ranks[ts] = np.arange(len(x))\n        return ranks\n\n    def _get_ocba(\n        self, means: np.ndarray, vars: np.ndarray, delta: int, verbose: bool = False\n    ) -&gt; np.ndarray:\n        \"\"\"Optimal Computing Budget Allocation (OCBA).\n\n        Calculates budget recommendations for given means, variances, and incremental\n        budget using the OCBA algorithm.\n\n        References:\n            [1] Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization:\n                An Optimal Computer Budget Allocation, pp. 49 and pp. 215\n\n        Args:\n            means (ndarray): Array of means.\n            vars (ndarray): Array of variances.\n            delta (int): Incremental budget.\n            verbose (bool): If True, print debug information. Defaults to False.\n\n        Returns:\n            ndarray: Array of budget recommendations, or None if conditions not met.\n\n        Examples:\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; means = np.array([1, 2, 3, 4, 5])\n            &gt;&gt;&gt; vars = np.array([1, 1, 9, 9, 4])\n            &gt;&gt;&gt; allocations = opt._get_ocba(means, vars, 50)\n            &gt;&gt;&gt; allocations\n            array([11,  9, 19,  9,  2])\n        \"\"\"\n        if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n            n_designs = means.shape[0]\n            allocations = np.zeros(n_designs, np.int32)\n            ratios = np.zeros(n_designs, np.float64)\n            budget = delta\n            ranks = self._get_ranks(means)\n            best, second_best = np.argpartition(ranks, 2)[:2]\n            ratios[second_best] = 1.0\n            select = [i for i in range(n_designs) if i not in [best, second_best]]\n            temp = (means[best] - means[second_best]) / (means[best] - means[select])\n            ratios[select] = np.square(temp) * (vars[select] / vars[second_best])\n            select = [i for i in range(n_designs) if i not in [best]]\n            temp = (np.square(ratios[select]) / vars[select]).sum()\n            ratios[best] = np.sqrt(vars[best] * temp)\n            more_runs = np.full(n_designs, True, dtype=bool)\n            add_budget = np.zeros(n_designs, dtype=float)\n            more_alloc = True\n\n            if verbose:\n                print(\"\\nIn _get_ocba():\")\n                print(f\"means: {means}\")\n                print(f\"vars: {vars}\")\n                print(f\"delta: {delta}\")\n                print(f\"n_designs: {n_designs}\")\n                print(f\"Ratios: {ratios}\")\n                print(f\"Best: {best}, Second best: {second_best}\")\n\n            while more_alloc:\n                more_alloc = False\n                ratio_s = (more_runs * ratios).sum()\n                add_budget[more_runs] = (budget / ratio_s) * ratios[more_runs]\n                add_budget = np.around(add_budget).astype(int)\n                mask = add_budget &lt; allocations\n                add_budget[mask] = allocations[mask]\n                more_runs[mask] = 0\n\n                if mask.sum() &gt; 0:\n                    more_alloc = True\n                if more_alloc:\n                    budget = allocations.sum() + delta\n                    budget -= (add_budget * ~more_runs).sum()\n\n            t_budget = add_budget.sum()\n            add_budget[best] += allocations.sum() + delta - t_budget\n            return add_budget - allocations\n        else:\n            return None\n\n    def _get_ocba_X(\n        self,\n        X: np.ndarray,\n        means: np.ndarray,\n        vars: np.ndarray,\n        delta: int,\n        verbose: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate OCBA allocation and repeat input array X.\n\n        Args:\n            X (ndarray): Input array to be repeated, shape (n_designs, n_features).\n            means (ndarray): Array of means for each design.\n            vars (ndarray): Array of variances for each design.\n            delta (int): Incremental budget.\n            verbose (bool): If True, print debug information. Defaults to False.\n\n        Returns:\n            ndarray: Repeated array of X based on OCBA allocation, or None if\n                     conditions not met.\n\n        Examples:\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; X = np.array([[1, 2], [4, 5], [7, 8]])\n            &gt;&gt;&gt; means = np.array([1.5, 35, 550])\n            &gt;&gt;&gt; vars = np.array([0.5, 50, 5000])\n            &gt;&gt;&gt; X_new = opt._get_ocba_X(X, means, vars, delta=5, verbose=False)\n            &gt;&gt;&gt; X_new.shape[0] == 5  # Should have 5 additional evaluations\n            True\n        \"\"\"\n        if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n            o = self._get_ocba(means=means, vars=vars, delta=delta, verbose=verbose)\n            return np.repeat(X, o, axis=0)\n        else:\n            return None\n\n    def _evaluate_function(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Evaluate objective function at points X.\n\n        If dimension reduction is active, expands X to full dimensions before evaluation.\n        Supports both single-objective and multi-objective functions. For multi-objective\n        functions, converts to single-objective using _mo2so method.\n\n        Args:\n            X (ndarray): Points to evaluate in reduced space, shape (n_samples, n_reduced_features).\n\n        Returns:\n            ndarray: Function values, shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Single-objective function\n            &gt;&gt;&gt; opt_so = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; X = np.array([[1.0, 2.0], [3.0, 4.0]])\n            &gt;&gt;&gt; y = opt_so._evaluate_function(X)\n            &gt;&gt;&gt; print(f\"Single-objective output: {y}\")\n            Single-objective output: [ 5. 25.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Multi-objective function (default: use first objective)\n            &gt;&gt;&gt; opt_mo = SpotOptim(\n            ...     fun=lambda X: np.column_stack([\n            ...         np.sum(X**2, axis=1),\n            ...         np.sum((X-1)**2, axis=1)\n            ...     ]),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_mo = opt_mo._evaluate_function(X)\n            &gt;&gt;&gt; print(f\"Multi-objective output (first obj): {y_mo}\")\n            Multi-objective output (first obj): [ 5. 25.]\n        \"\"\"\n        # Ensure X is 2D\n        X = np.atleast_2d(X)\n\n        # Expand to full dimensions if needed\n        if self.red_dim:\n            X = self.to_all_dim(X)\n\n        # Evaluate function\n        y_raw = self.fun(X)\n\n        # Convert to numpy array if needed\n        if not isinstance(y_raw, np.ndarray):\n            y_raw = np.array([y_raw])\n\n        # Handle multi-objective case\n        y = self._mo2so(y_raw)\n\n        # Ensure y is 1D\n        if y.ndim &gt; 1:\n            y = y.ravel()\n\n        return y\n\n    def _generate_initial_design(self) -&gt; np.ndarray:\n        \"\"\"Generate initial space-filling design using Latin Hypercube Sampling.\n\n        Returns:\n            ndarray: Initial design points, shape (n_initial, n_features).\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 n_initial=10)\n            &gt;&gt;&gt; X0 = opt._generate_initial_design()\n            &gt;&gt;&gt; X0.shape\n            (10, 2)\n        \"\"\"\n        # Generate samples in [0, 1]^d\n        X0_unit = self.lhs_sampler.random(n=self.n_initial)\n\n        # Scale to [lower, upper]\n        X0 = self.lower + X0_unit * (self.upper - self.lower)\n\n        return self._repair_non_numeric(X0, self.var_type)\n\n    def _select_distant_points(\n        self, X: np.ndarray, y: np.ndarray, k: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Selects k points that are distant from each other using K-means clustering.\n\n        This method performs K-means clustering to find k clusters, then selects\n        the point closest to each cluster center. This ensures a space-filling\n        subset of points for surrogate model training.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n            k (int): Number of points to select.\n\n        Returns:\n            tuple: A tuple containing:\n                - selected_X (ndarray): Selected design points, shape (k, n_features).\n                - selected_y (ndarray): Function values at selected points, shape (k,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=5)\n            &gt;&gt;&gt; X = np.random.rand(100, 2)\n            &gt;&gt;&gt; y = np.random.rand(100)\n            &gt;&gt;&gt; X_sel, y_sel = opt._select_distant_points(X, y, 5)\n            &gt;&gt;&gt; X_sel.shape\n            (5, 2)\n        \"\"\"\n        from sklearn.cluster import KMeans\n\n        # Perform k-means clustering\n        kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n\n        # Find the closest point to each cluster center\n        selected_indices = []\n        for center in kmeans.cluster_centers_:\n            distances = np.linalg.norm(X - center, axis=1)\n            closest_idx = np.argmin(distances)\n            selected_indices.append(closest_idx)\n\n        selected_indices = np.array(selected_indices)\n        return X[selected_indices], y[selected_indices]\n\n    def _select_best_cluster(\n        self, X: np.ndarray, y: np.ndarray, k: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Selects all points from the cluster with the smallest mean y value.\n\n        This method performs K-means clustering and selects all points from the\n        cluster whose center corresponds to the best (smallest) mean objective\n        function value.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n            k (int): Number of clusters.\n\n        Returns:\n            tuple: A tuple containing:\n                - selected_X (ndarray): Selected design points from best cluster, shape (m, n_features).\n                - selected_y (ndarray): Function values at selected points, shape (m,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=5,\n            ...                 selection_method='best')\n            &gt;&gt;&gt; X = np.random.rand(100, 2)\n            &gt;&gt;&gt; y = np.random.rand(100)\n            &gt;&gt;&gt; X_sel, y_sel = opt._select_best_cluster(X, y, 5)\n        \"\"\"\n        from sklearn.cluster import KMeans\n\n        # Perform k-means clustering\n        kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n        labels = kmeans.labels_\n\n        # Compute mean y for each cluster\n        cluster_means = []\n        for cluster_idx in range(k):\n            cluster_y = y[labels == cluster_idx]\n            if len(cluster_y) == 0:\n                cluster_means.append(np.inf)\n            else:\n                cluster_means.append(np.mean(cluster_y))\n\n        # Find cluster with smallest mean y\n        best_cluster = np.argmin(cluster_means)\n\n        # Select all points from the best cluster\n        mask = labels == best_cluster\n        return X[mask], y[mask]\n\n    def _selection_dispatcher(\n        self, X: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Dispatcher for selection methods.\n\n        Depending on the value of `self.selection_method`, this method calls\n        the appropriate selection function to choose a subset of points for\n        surrogate model training when the total number of points exceeds\n        `self.max_surrogate_points`.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - selected_X (ndarray): Selected design points.\n                - selected_y (ndarray): Function values at selected points.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=5)\n            &gt;&gt;&gt; X = np.random.rand(100, 2)\n            &gt;&gt;&gt; y = np.random.rand(100)\n            &gt;&gt;&gt; X_sel, y_sel = opt._selection_dispatcher(X, y)\n            &gt;&gt;&gt; X_sel.shape[0] &lt;= 5\n            True\n        \"\"\"\n        if self.max_surrogate_points is None:\n            return X, y\n\n        if self.selection_method == \"distant\":\n            return self._select_distant_points(X=X, y=y, k=self.max_surrogate_points)\n        elif self.selection_method == \"best\":\n            return self._select_best_cluster(X=X, y=y, k=self.max_surrogate_points)\n        else:\n            # If no valid selection method, return all points\n            return X, y\n\n    def _fit_surrogate(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"Fit surrogate model to data.\n\n        If the number of points exceeds `self.max_surrogate_points`,\n        a subset of points is selected using the selection dispatcher.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n        \"\"\"\n        X_fit = X\n        y_fit = y\n\n        # Select subset if needed\n        if (\n            self.max_surrogate_points is not None\n            and X.shape[0] &gt; self.max_surrogate_points\n        ):\n            if self.verbose:\n                print(\n                    f\"Selecting subset of {self.max_surrogate_points} points \"\n                    f\"from {X.shape[0]} total points for surrogate fitting.\"\n                )\n            X_fit, y_fit = self._selection_dispatcher(X, y)\n\n        self.surrogate.fit(X_fit, y_fit)\n\n    def _select_new(\n        self, A: np.ndarray, X: np.ndarray, tolerance: float = 0\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Select rows from A that are not in X.\n\n        Args:\n            A (ndarray): Array with new values.\n            X (ndarray): Array with known values.\n            tolerance (float, optional): Tolerance value for comparison. Defaults to 0.\n\n        Returns:\n            tuple: A tuple containing:\n                - ndarray: Array with unknown (new) values.\n                - ndarray: Array with True if value is new, otherwise False.\n        \"\"\"\n        B = np.abs(A[:, None] - X)\n        ind = np.any(np.all(B &lt;= tolerance, axis=2), axis=1)\n        return A[~ind], ~ind\n\n    def _repair_non_numeric(self, X: np.ndarray, var_type: List[str]) -&gt; np.ndarray:\n        \"\"\"Round non-numeric values to integers based on variable type.\n\n        This method applies rounding to variables that are not continuous:\n        - 'float': No rounding (continuous values)\n        - 'int': Rounded to integers\n        - 'factor': Rounded to integers (representing categorical values)\n\n        Args:\n            X (ndarray): X array with values to potentially round.\n            var_type (list of str): List with type information for each dimension.\n\n        Returns:\n            ndarray: X array with non-continuous values rounded to integers.\n        \"\"\"\n        mask = np.isin(var_type, [\"float\"], invert=True)\n        X[:, mask] = np.around(X[:, mask])\n        return X\n\n    def _handle_acquisition_failure(self) -&gt; np.ndarray:\n        \"\"\"Handle acquisition failure by proposing new design points.\n\n        This method is called when no new design points can be suggested\n        by the surrogate model (e.g., when the proposed point is too close\n        to existing points). Depending on the specified strategy, it either\n        proposes a Morris-Mitchell minimizing point or generates a new\n        space-filling design as a fallback.\n\n        Returns:\n            ndarray: New design point as a fallback, shape (n_features,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     acquisition_failure_strategy='random'\n            ... )\n            &gt;&gt;&gt; opt.X_ = np.array([[0, 0], [1, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([0, 2])\n            &gt;&gt;&gt; x_fallback = opt._handle_acquisition_failure()\n            &gt;&gt;&gt; x_fallback.shape\n            (2,)\n        \"\"\"\n        if self.acquisition_failure_strategy == \"mm\":\n            # Morris-Mitchell phi minimizing point\n            # This strategy finds a point that maximizes the minimum distance\n            # to all existing points, providing good space-filling properties\n            if self.verbose:\n                print(\n                    \"Acquisition failure: Using Morris-Mitchell minimizing point as fallback.\"\n                )\n\n            # Calculate distances from all possible candidates to existing points\n            # We'll use a simple approach: generate many random candidates and pick the one\n            # with maximum minimum distance to existing points\n            n_candidates = 100\n            candidates_unit = self.lhs_sampler.random(n=n_candidates)\n            candidates = self.lower + candidates_unit * (self.upper - self.lower)\n\n            # Calculate minimum distance from each candidate to existing points\n            min_distances = np.zeros(n_candidates)\n            for i, candidate in enumerate(candidates):\n                distances = np.linalg.norm(self.X_ - candidate, axis=1)\n                min_distances[i] = np.min(distances)\n\n            # Select candidate with maximum minimum distance\n            best_idx = np.argmax(min_distances)\n            x_new = candidates[best_idx]\n\n        else:\n            # Default: random space-filling design (Latin Hypercube Sampling)\n            if self.verbose:\n                print(\n                    \"Acquisition failure: Using random space-filling design as fallback.\"\n                )\n\n            x_new_unit = self.lhs_sampler.random(n=1)[0]\n            x_new = self.lower + x_new_unit * (self.upper - self.lower)\n\n        return self._repair_non_numeric(x_new.reshape(1, -1), self.var_type)[0]\n\n    def _acquisition_function(self, x: np.ndarray) -&gt; float:\n        \"\"\"Compute acquisition function value.\n\n        Args:\n            x (ndarray): Point to evaluate, shape (n_features,).\n\n        Returns:\n            float: Acquisition function value (to be minimized).\n        \"\"\"\n        x = x.reshape(1, -1)\n\n        if self.acquisition == \"y\":\n            # Predicted mean\n            return self.surrogate.predict(x)[0]\n\n        elif self.acquisition == \"ei\":\n            # Expected Improvement\n            mu, sigma = self.surrogate.predict(x, return_std=True)\n            mu = mu[0]\n            sigma = sigma[0]\n\n            if sigma &lt; 1e-10:\n                return 0.0\n\n            y_best = np.min(self.y_)\n            improvement = y_best - mu\n            Z = improvement / sigma\n\n            from scipy.stats import norm\n\n            ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n            return -ei  # Minimize negative EI\n\n        elif self.acquisition == \"pi\":\n            # Probability of Improvement\n            mu, sigma = self.surrogate.predict(x, return_std=True)\n            mu = mu[0]\n            sigma = sigma[0]\n\n            if sigma &lt; 1e-10:\n                return 0.0\n\n            y_best = np.min(self.y_)\n            Z = (y_best - mu) / sigma\n\n            from scipy.stats import norm\n\n            pi = norm.cdf(Z)\n            return -pi  # Minimize negative PI\n\n        else:\n            raise ValueError(f\"Unknown acquisition function: {self.acquisition}\")\n\n    def _suggest_next_point(self) -&gt; np.ndarray:\n        \"\"\"Suggest next point to evaluate using acquisition function optimization.\n\n        If the acquisition function optimization fails to find a sufficiently distant\n        point, falls back to the strategy specified by acquisition_failure_strategy.\n\n        Returns:\n            ndarray: Next point to evaluate, shape (n_features,).\n        \"\"\"\n        result = differential_evolution(\n            func=self._acquisition_function,\n            bounds=self.bounds,\n            seed=self.seed,\n            maxiter=1000,\n        )\n\n        x_next = result.x\n\n        # Ensure minimum distance to existing points\n        x_next_2d = x_next.reshape(1, -1)\n        x_new, _ = self._select_new(A=x_next_2d, X=self.X_, tolerance=self.tolerance_x)\n\n        if x_new.shape[0] == 0:\n            # No new point found on surrogate - use fallback strategy\n            return self._handle_acquisition_failure()\n\n        return self._repair_non_numeric(x_next.reshape(1, -1), self.var_type)[0]\n\n    def optimize(self, X0: Optional[np.ndarray] = None) -&gt; OptimizeResult:\n        \"\"\"Run the optimization process.\n\n        The optimization terminates when either:\n        - Total function evaluations reach max_iter (including initial design), OR\n        - Runtime exceeds max_time minutes\n\n        Args:\n            X0 (ndarray, optional): Initial design points, shape (n_initial, n_features).\n                If None, generates space-filling design. Defaults to None.\n\n        Returns:\n            OptimizeResult: Optimization result with fields:\n                - x: best point found\n                - fun: best function value\n                - nfev: number of function evaluations (including initial design)\n                - nit: number of sequential optimization iterations (after initial design)\n                - success: whether optimization succeeded\n                - message: termination message indicating reason for stopping\n                - X: all evaluated points\n                - y: all function values\n\n        Examples:\n            &gt;&gt;&gt; # Example 1: Budget-based termination\n            &gt;&gt;&gt; opt = SpotOptim(fun=objective, bounds=bounds, max_iter=30, n_initial=10)\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; # Will perform 10 initial + 20 sequential iterations = 30 total evaluations\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: Time-based termination\n            &gt;&gt;&gt; opt = SpotOptim(fun=expensive_objective, bounds=bounds,\n            ...                 max_iter=1000, max_time=5.0)  # 5 minutes max\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; # Will stop after 5 minutes OR 1000 evaluations, whichever comes first\n        \"\"\"\n        # Generate or use provided initial design\n        if X0 is None:\n            X0 = self._generate_initial_design()\n        else:\n            X0 = np.atleast_2d(X0)\n            # If X0 is in full dimensions and we have dimension reduction, reduce it\n            if self.red_dim and X0.shape[1] == len(self.ident):\n                X0 = self.to_red_dim(X0)\n            X0 = self._repair_non_numeric(X0, self.var_type)\n\n        # Repeat initial design points if repeats_initial &gt; 1\n        if self.repeats_initial &gt; 1:\n            X0 = np.repeat(X0, self.repeats_initial, axis=0)\n\n        # Evaluate initial design\n        y0 = self._evaluate_function(X0)\n\n        # Update success rate BEFORE updating storage (initial design - all should be successes since starting from scratch)\n        self._update_success_rate(y0)\n\n        # Initialize storage\n        self.X_ = X0.copy()\n        self.y_ = y0.copy()\n        self.n_iter_ = 0\n\n        # Update stats after initial design\n        self.update_stats()\n\n        # Log initial design to TensorBoard\n        if self.tb_writer is not None:\n            for i in range(len(self.y_)):\n                self._write_tensorboard_hparams(self.X_[i], self.y_[i])\n            self._write_tensorboard_scalars()\n\n        # Initial best\n        best_idx = np.argmin(self.y_)\n        self.best_x_ = self.X_[best_idx].copy()\n        self.best_y_ = self.y_[best_idx]\n\n        if self.verbose:\n            if self.noise:\n                print(\n                    f\"Initial best: f(x) = {self.best_y_:.6f}, mean best: f(x) = {self.min_mean_y:.6f}\"\n                )\n            else:\n                print(f\"Initial best: f(x) = {self.best_y_:.6f}\")\n\n        # Start timer for max_time check\n        timeout_start = time.time()\n\n        # Main optimization loop\n        # Termination: continue while (total_evals &lt; max_iter) AND (elapsed_time &lt; max_time)\n        while (len(self.y_) &lt; self.max_iter) and (\n            time.time() &lt; timeout_start + self.max_time * 60\n        ):\n            self.n_iter_ += 1\n\n            # Fit surrogate (use mean_y if noise, otherwise y_)\n            if self.noise:\n                self._fit_surrogate(self.mean_X, self.mean_y)\n            else:\n                self._fit_surrogate(self.X_, self.y_)\n\n            # OCBA: Compute optimal budget allocation for noisy functions\n            # This determines which existing design points should be re-evaluated\n            X_ocba = None\n            if self.noise and self.ocba_delta &gt; 0:\n                # Check conditions for OCBA (need variance &gt; 0 and at least 3 points)\n                if not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n                    if self.verbose:\n                        print(\n                            f\"Warning: OCBA skipped (need &gt;2 points with variance &gt; 0)\"\n                        )\n                elif np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n                    # Get OCBA allocation\n                    X_ocba = self._get_ocba_X(\n                        self.mean_X, self.mean_y, self.var_y, self.ocba_delta\n                    )\n                    if self.verbose and X_ocba is not None:\n                        print(f\"  OCBA: Adding {X_ocba.shape[0]} re-evaluation(s)\")\n\n            # Suggest next point\n            x_next = self._suggest_next_point()\n\n            # Repeat next point if repeats_surrogate &gt; 1\n            if self.repeats_surrogate &gt; 1:\n                x_next_repeated = np.repeat(\n                    x_next.reshape(1, -1), self.repeats_surrogate, axis=0\n                )\n            else:\n                x_next_repeated = x_next.reshape(1, -1)\n\n            # Append OCBA points to new design points (if applicable)\n            if X_ocba is not None:\n                x_next_repeated = append(X_ocba, x_next_repeated, axis=0)\n\n            # Evaluate next point(s) including OCBA points\n            y_next = self._evaluate_function(x_next_repeated)\n\n            # Update success rate BEFORE updating storage (so it compares against previous best)\n            self._update_success_rate(y_next)\n\n            # Update storage\n            self.X_ = np.vstack([self.X_, x_next_repeated])\n            self.y_ = np.append(self.y_, y_next)\n\n            # Update stats\n            self.update_stats()\n\n            # Log to TensorBoard\n            if self.tb_writer is not None:\n                # Log each new evaluation\n                for i in range(len(y_next)):\n                    self._write_tensorboard_hparams(x_next_repeated[i], y_next[i])\n                self._write_tensorboard_scalars()\n\n            # Update best\n            current_best = np.min(y_next)\n            if current_best &lt; self.best_y_:\n                best_idx_in_new = np.argmin(y_next)\n                self.best_x_ = x_next_repeated[best_idx_in_new].copy()\n                self.best_y_ = current_best\n\n                if self.verbose:\n                    if self.noise:\n                        print(\n                            f\"Iteration {self.n_iter_}: New best f(x) = {self.best_y_:.6f}, mean best: f(x) = {self.min_mean_y:.6f}\"\n                        )\n                    else:\n                        print(\n                            f\"Iteration {self.n_iter_}: New best f(x) = {self.best_y_:.6f}\"\n                        )\n            elif self.verbose:\n                if self.noise:\n                    mean_y_new = np.mean(y_next)\n                    print(f\"Iteration {self.n_iter_}: mean f(x) = {mean_y_new:.6f}\")\n                else:\n                    print(f\"Iteration {self.n_iter_}: f(x) = {y_next[0]:.6f}\")\n\n        # Expand results to full dimensions if needed\n        best_x_full = (\n            self.to_all_dim(self.best_x_.reshape(1, -1))[0]\n            if self.red_dim\n            else self.best_x_\n        )\n        X_full = self.to_all_dim(self.X_) if self.red_dim else self.X_\n\n        # Determine termination reason\n        elapsed_time = time.time() - timeout_start\n        if len(self.y_) &gt;= self.max_iter:\n            message = f\"Optimization terminated: maximum evaluations ({self.max_iter}) reached\"\n        elif elapsed_time &gt;= self.max_time * 60:\n            message = (\n                f\"Optimization terminated: time limit ({self.max_time:.2f} min) reached\"\n            )\n        else:\n            message = \"Optimization finished successfully\"\n\n        # Close TensorBoard writer\n        self._close_tensorboard_writer()\n\n        # Return scipy-style result\n        return OptimizeResult(\n            x=best_x_full,\n            fun=self.best_y_,\n            nfev=len(self.y_),\n            nit=self.n_iter_,\n            success=True,\n            message=message,\n            X=X_full,\n            y=self.y_,\n        )\n\n    def plot_surrogate(\n        self,\n        i: int = 0,\n        j: int = 1,\n        show: bool = True,\n        alpha: float = 0.8,\n        var_name: Optional[List[str]] = None,\n        cmap: str = \"jet\",\n        num: int = 100,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        add_points: bool = True,\n        grid_visible: bool = True,\n        contour_levels: int = 30,\n        figsize: Tuple[int, int] = (12, 10),\n    ) -&gt; None:\n        \"\"\"Plot the surrogate model for two dimensions.\n\n        Creates a 2x2 plot showing:\n        - Top left: 3D surface of predictions\n        - Top right: 3D surface of prediction uncertainty\n        - Bottom left: Contour plot of predictions with evaluated points\n        - Bottom right: Contour plot of prediction uncertainty\n\n        Args:\n            i (int, optional): Index of the first dimension to plot. Defaults to 0.\n            j (int, optional): Index of the second dimension to plot. Defaults to 1.\n            show (bool, optional): If True, displays the plot immediately. Defaults to True.\n            alpha (float, optional): Transparency of the 3D surface plots (0=transparent, 1=opaque).\n                Defaults to 0.8.\n            var_name (list of str, optional): Names for each dimension. If None, uses instance var_name.\n                Defaults to None.\n            cmap (str, optional): Matplotlib colormap name. Defaults to 'jet'.\n            num (int, optional): Number of grid points per dimension for mesh grid. Defaults to 100.\n            vmin (float, optional): Minimum value for color scale. If None, determined from data.\n                Defaults to None.\n            vmax (float, optional): Maximum value for color scale. If None, determined from data.\n                Defaults to None.\n            add_points (bool, optional): If True, overlay evaluated points on contour plots.\n                Defaults to True.\n            grid_visible (bool, optional): If True, show grid lines on contour plots. Defaults to True.\n            contour_levels (int, optional): Number of contour levels. Defaults to 30.\n            figsize (tuple of int, optional): Figure size in inches (width, height). Defaults to (12, 10).\n\n        Raises:\n            ValueError: If optimization hasn't been run yet, or if i, j are invalid.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; def sphere(X):\n            ...     return np.sum(X**2, axis=1)\n            &gt;&gt;&gt; # Example 1: Using var_name in constructor\n            &gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n            ...                 max_iter=10, n_initial=5, var_name=['x1', 'x2'])\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; opt.plot_surrogate(i=0, j=1)  # Uses instance var_name\n            &gt;&gt;&gt; # Example 2: Override var_name for this plot\n            &gt;&gt;&gt; opt.plot_surrogate(i=0, j=1, var_name=['custom1', 'custom2'])\n        \"\"\"\n        # Validation\n        if self.X_ is None or self.y_ is None:\n            raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n        k = self.n_dim\n        if i &gt;= k or j &gt;= k:\n            raise ValueError(f\"Dimensions i={i} and j={j} must be less than n_dim={k}.\")\n        if i == j:\n            raise ValueError(\"Dimensions i and j must be different.\")\n\n        # Use instance var_name if not provided\n        if var_name is None:\n            var_name = self.var_name\n\n        # Generate mesh grid\n        X_i, X_j, grid_points = self._generate_mesh_grid(i, j, num)\n\n        # Predict on grid\n        y_pred, y_std = self.surrogate.predict(grid_points, return_std=True)\n        Z_pred = y_pred.reshape(X_i.shape)\n        Z_std = y_std.reshape(X_i.shape)\n\n        # Create figure\n        fig = plt.figure(figsize=figsize)\n\n        # Plot 1: 3D surface of predictions\n        ax1 = fig.add_subplot(221, projection=\"3d\")\n        ax1.plot_surface(X_i, X_j, Z_pred, cmap=cmap, alpha=alpha, vmin=vmin, vmax=vmax)\n        ax1.set_title(\"Prediction Surface\")\n        ax1.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n        ax1.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n        ax1.set_zlabel(\"Prediction\")\n\n        # Plot 2: 3D surface of prediction uncertainty\n        ax2 = fig.add_subplot(222, projection=\"3d\")\n        ax2.plot_surface(X_i, X_j, Z_std, cmap=cmap, alpha=alpha)\n        ax2.set_title(\"Prediction Uncertainty Surface\")\n        ax2.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n        ax2.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n        ax2.set_zlabel(\"Std. Dev.\")\n\n        # Plot 3: Contour of predictions\n        ax3 = fig.add_subplot(223)\n        contour3 = ax3.contourf(\n            X_i, X_j, Z_pred, levels=contour_levels, cmap=cmap, vmin=vmin, vmax=vmax\n        )\n        plt.colorbar(contour3, ax=ax3)\n        if add_points:\n            ax3.scatter(\n                self.X_[:, i],\n                self.X_[:, j],\n                c=\"red\",\n                s=30,\n                edgecolors=\"black\",\n                zorder=5,\n                label=\"Evaluated points\",\n            )\n            ax3.legend()\n        ax3.set_title(\"Prediction Contour\")\n        ax3.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n        ax3.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n        ax3.grid(visible=grid_visible)\n\n        # Plot 4: Contour of prediction uncertainty\n        ax4 = fig.add_subplot(224)\n        contour4 = ax4.contourf(X_i, X_j, Z_std, levels=contour_levels, cmap=cmap)\n        plt.colorbar(contour4, ax=ax4)\n        if add_points:\n            ax4.scatter(\n                self.X_[:, i],\n                self.X_[:, j],\n                c=\"red\",\n                s=30,\n                edgecolors=\"black\",\n                zorder=5,\n                label=\"Evaluated points\",\n            )\n            ax4.legend()\n        ax4.set_title(\"Uncertainty Contour\")\n        ax4.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n        ax4.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n        ax4.grid(visible=grid_visible)\n\n        plt.tight_layout()\n\n        if show:\n            plt.show()\n\n    def _generate_mesh_grid(\n        self, i: int, j: int, num: int = 100\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Generate a mesh grid for two dimensions, filling others with mean values.\n\n        Args:\n            i (int): Index of the first dimension to vary.\n            j (int): Index of the second dimension to vary.\n            num (int, optional): Number of grid points per dimension. Defaults to 100.\n\n        Returns:\n            tuple: A tuple containing:\n                - X_i (ndarray): Meshgrid for dimension i.\n                - X_j (ndarray): Meshgrid for dimension j.\n                - grid_points (ndarray): Grid points for prediction, shape (num*num, n_dim).\n        \"\"\"\n        k = self.n_dim\n        mean_values = self.X_.mean(axis=0)\n\n        # Create grid for dimensions i and j\n        x_i = linspace(self.lower[i], self.upper[i], num=num)\n        x_j = linspace(self.lower[j], self.upper[j], num=num)\n        X_i, X_j = meshgrid(x_i, x_j)\n\n        # Initialize grid points with mean values\n        grid_points = np.tile(mean_values, (X_i.size, 1))\n        grid_points[:, i] = X_i.ravel()\n        grid_points[:, j] = X_j.ravel()\n\n        # Apply type constraints\n        grid_points = self._repair_non_numeric(grid_points, self.var_type)\n\n        return X_i, X_j, grid_points\n\n    def _get_experiment_filename(self, prefix: str) -&gt; str:\n        \"\"\"Generate experiment filename from prefix.\n\n        Args:\n            prefix (str): Prefix for the filename.\n\n        Returns:\n            str: Filename with '_exp.pkl' suffix.\n        \"\"\"\n        if prefix is None:\n            return \"experiment_exp.pkl\"\n        return f\"{prefix}_exp.pkl\"\n\n    def _get_result_filename(self, prefix: str) -&gt; str:\n        \"\"\"Generate result filename from prefix.\n\n        Args:\n            prefix (str): Prefix for the filename.\n\n        Returns:\n            str: Filename with '_res.pkl' suffix.\n        \"\"\"\n        if prefix is None:\n            return \"result_res.pkl\"\n        return f\"{prefix}_res.pkl\"\n\n    def _close_and_del_tensorboard_writer(self) -&gt; None:\n        \"\"\"Close and delete TensorBoard writer to prepare for pickling.\"\"\"\n        if hasattr(self, \"tb_writer\") and self.tb_writer is not None:\n            try:\n                self.tb_writer.flush()\n                self.tb_writer.close()\n            except Exception:\n                pass\n            self.tb_writer = None\n\n    def _get_pickle_safe_optimizer(\n        self, unpickleables: str = \"file_io\", verbosity: int = 0\n    ) -&gt; \"SpotOptim\":\n        \"\"\"Create a pickle-safe copy of the optimizer.\n\n        This method creates a copy of the optimizer instance with unpickleable components removed\n        or set to None to enable safe serialization.\n\n        Args:\n            unpickleables (str): Type of unpickleable components to exclude.\n                - \"file_io\": Excludes only file I/O components (tb_writer) and fun\n                - \"all\": Excludes file I/O, fun, surrogate, and lhs_sampler\n                Defaults to \"file_io\".\n            verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n        Returns:\n            SpotOptim: A copy of the optimizer with unpickleable components removed.\n        \"\"\"\n        # Always exclude fun and tb_writer (can't reliably pickle lambda/local functions)\n        # Determine which additional attributes to exclude\n        if unpickleables == \"file_io\":\n            unpickleable_attrs = [\"tb_writer\", \"fun\"]\n        else:\n            unpickleable_attrs = [\"tb_writer\", \"fun\", \"surrogate\", \"lhs_sampler\"]\n\n        # Prepare picklable state dictionary\n        picklable_state = {}\n\n        for key, value in self.__dict__.items():\n            if key not in unpickleable_attrs:\n                try:\n                    # Test if attribute can be pickled\n                    pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n                    picklable_state[key] = value\n                    if verbosity &gt; 1:\n                        print(f\"Attribute '{key}' is picklable and will be included.\")\n                except Exception as e:\n                    if verbosity &gt; 0:\n                        print(\n                            f\"Attribute '{key}' is not picklable and will be excluded: {e}\"\n                        )\n                    continue\n            else:\n                if verbosity &gt; 1:\n                    print(f\"Attribute '{key}' explicitly excluded from pickling.\")\n\n        # Create new instance with picklable state\n        picklable_instance = self.__class__.__new__(self.__class__)\n        picklable_instance.__dict__.update(picklable_state)\n\n        # Set excluded attributes to None\n        for attr in unpickleable_attrs:\n            if not hasattr(picklable_instance, attr):\n                setattr(picklable_instance, attr, None)\n\n        return picklable_instance\n\n    def save_experiment(\n        self,\n        filename: Optional[str] = None,\n        prefix: str = \"experiment\",\n        path: Optional[str] = None,\n        overwrite: bool = True,\n        unpickleables: str = \"all\",\n        verbosity: int = 0,\n    ) -&gt; None:\n        \"\"\"Save the experiment configuration to a pickle file.\n\n        An experiment contains the optimizer configuration needed to run optimization,\n        but excludes the results. This is useful for defining experiments locally and\n        executing them on remote machines.\n\n        The experiment includes:\n        - Bounds, variable types, variable names\n        - Optimization parameters (max_iter, n_initial, etc.)\n        - Surrogate and acquisition settings\n        - Random seed\n\n        The experiment excludes:\n        - Function evaluations (X_, y_)\n        - Optimization results\n        - Objective function (must be re-attached after loading)\n\n        Args:\n            filename (str, optional): Filename for the experiment file. If None, generates\n                from prefix. Defaults to None.\n            prefix (str): Prefix for auto-generated filename. Defaults to \"experiment\".\n            path (str, optional): Directory path to save the file. If None, saves in current\n                directory. Creates directory if it doesn't exist. Defaults to None.\n            overwrite (bool): If True, overwrites existing file. If False, raises error if\n                file exists. Defaults to True.\n            unpickleables (str): Components to exclude for pickling:\n                - \"all\": Excludes fun, surrogate, lhs_sampler, tb_writer (experiment only)\n                - \"file_io\": Excludes only tb_writer (lighter exclusion)\n                Defaults to \"all\".\n            verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Define experiment locally\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Save experiment (without results)\n            &gt;&gt;&gt; opt.save_experiment(prefix=\"sphere_opt\")\n            Experiment saved to sphere_opt_exp.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # On remote machine: load and run\n            &gt;&gt;&gt; # opt_remote = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n            &gt;&gt;&gt; # opt_remote.fun = objective_function  # Re-attach function\n            &gt;&gt;&gt; # result = opt_remote.optimize()\n            &gt;&gt;&gt; # opt_remote.save_result(prefix=\"sphere_opt\")  # Save results\n        \"\"\"\n        # Close TensorBoard writer before pickling\n        self._close_and_del_tensorboard_writer()\n\n        # Create pickle-safe copy\n        optimizer_copy = self._get_pickle_safe_optimizer(\n            unpickleables=unpickleables, verbosity=verbosity\n        )\n\n        # Determine filename\n        if filename is None:\n            filename = self._get_experiment_filename(prefix)\n\n        # Add path if provided\n        if path is not None:\n            if not os.path.exists(path):\n                os.makedirs(path)\n            filename = os.path.join(path, filename)\n\n        # Check for existing file\n        if os.path.exists(filename) and not overwrite:\n            raise FileExistsError(\n                f\"File {filename} already exists. Use overwrite=True to overwrite.\"\n            )\n\n        # Save to pickle file\n        try:\n            with open(filename, \"wb\") as handle:\n                pickle.dump(optimizer_copy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n            print(f\"Experiment saved to {filename}\")\n        except Exception as e:\n            print(f\"Error during pickling: {e}\")\n            raise\n\n    def save_result(\n        self,\n        filename: Optional[str] = None,\n        prefix: str = \"result\",\n        path: Optional[str] = None,\n        overwrite: bool = True,\n        verbosity: int = 0,\n    ) -&gt; None:\n        \"\"\"Save the complete optimization results to a pickle file.\n\n        A result contains all information from a completed optimization run, including\n        the experiment configuration and all evaluation results. This is useful for\n        saving completed runs for later analysis.\n\n        The result includes everything in an experiment plus:\n        - All evaluated points (X_)\n        - All function values (y_)\n        - Best point and best value\n        - Iteration count\n        - Success rate statistics\n        - Noise statistics (if applicable)\n\n        Args:\n            filename (str, optional): Filename for the result file. If None, generates\n                from prefix. Defaults to None.\n            prefix (str): Prefix for auto-generated filename. Defaults to \"result\".\n            path (str, optional): Directory path to save the file. If None, saves in current\n                directory. Creates directory if it doesn't exist. Defaults to None.\n            overwrite (bool): If True, overwrites existing file. If False, raises error if\n                file exists. Defaults to True.\n            verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Run optimization\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Save complete results\n            &gt;&gt;&gt; opt.save_result(prefix=\"sphere_opt\")\n            Result saved to sphere_opt_res.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Later: load and analyze\n            &gt;&gt;&gt; # opt_loaded = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n            &gt;&gt;&gt; # print(\"Best value:\", opt_loaded.best_y_)\n            &gt;&gt;&gt; # opt_loaded.plot_surrogate()\n        \"\"\"\n        # Use save_experiment with file_io unpickleables to preserve results\n        if filename is None:\n            filename = self._get_result_filename(prefix)\n\n        self.save_experiment(\n            filename=filename,\n            path=path,\n            overwrite=overwrite,\n            unpickleables=\"file_io\",\n            verbosity=verbosity,\n        )\n\n        # Update message\n        if path is not None:\n            full_path = os.path.join(path, filename)\n        else:\n            full_path = filename\n        print(f\"Result saved to {full_path}\")\n\n    def _reinitialize_components(self) -&gt; None:\n        \"\"\"Reinitialize components that were excluded during pickling.\n\n        This method recreates the surrogate model and LHS sampler that were\n        excluded when saving an experiment or result.\n        \"\"\"\n        # Reinitialize LHS sampler if needed\n        if not hasattr(self, \"lhs_sampler\") or self.lhs_sampler is None:\n            self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n        # Reinitialize surrogate if needed\n        if not hasattr(self, \"surrogate\") or self.surrogate is None:\n            kernel = ConstantKernel(1.0) * Matern(\n                length_scale=np.ones(self.n_dim),\n                length_scale_bounds=(1e-2, 1e2),\n                nu=2.5,\n            )\n            self.surrogate = GaussianProcessRegressor(\n                kernel=kernel,\n                n_restarts_optimizer=10,\n                random_state=self.seed,\n                normalize_y=True,\n            )\n\n    @staticmethod\n    def load_experiment(filename: str) -&gt; \"SpotOptim\":\n        \"\"\"Load an experiment configuration from a pickle file.\n\n        Loads an experiment that was saved with save_experiment(). The loaded optimizer\n        will have the configuration but not the objective function (which must be\n        re-attached) or results.\n\n        Args:\n            filename (str): Path to the experiment pickle file.\n\n        Returns:\n            SpotOptim: Loaded optimizer instance (without fun attached).\n\n        Raises:\n            FileNotFoundError: If the specified file doesn't exist.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load experiment\n            &gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n            Loaded experiment from sphere_opt_exp.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Re-attach objective function\n            &gt;&gt;&gt; opt.fun = lambda X: np.sum(X**2, axis=1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Run optimization\n            &gt;&gt;&gt; result = opt.optimize()\n        \"\"\"\n        if not os.path.exists(filename):\n            raise FileNotFoundError(f\"Experiment file not found: {filename}\")\n\n        try:\n            with open(filename, \"rb\") as handle:\n                optimizer = pickle.load(handle)\n            print(f\"Loaded experiment from {filename}\")\n\n            # Reinitialize components that were excluded\n            optimizer._reinitialize_components()\n\n            return optimizer\n        except Exception as e:\n            print(f\"Error loading experiment: {e}\")\n            raise\n\n    @staticmethod\n    def load_result(filename: str) -&gt; \"SpotOptim\":\n        \"\"\"Load complete optimization results from a pickle file.\n\n        Loads results that were saved with save_result(). The loaded optimizer\n        will have both configuration and all optimization results.\n\n        Args:\n            filename (str): Path to the result pickle file.\n\n        Returns:\n            SpotOptim: Loaded optimizer instance with complete results.\n\n        Raises:\n            FileNotFoundError: If the specified file doesn't exist.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load results\n            &gt;&gt;&gt; opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n            Loaded result from sphere_opt_res.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Analyze results\n            &gt;&gt;&gt; print(\"Best point:\", opt.best_x_)\n            &gt;&gt;&gt; print(\"Best value:\", opt.best_y_)\n            &gt;&gt;&gt; print(\"Total evaluations:\", opt.counter)\n            &gt;&gt;&gt; print(\"Success rate:\", opt.success_rate)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Continue optimization if needed\n            &gt;&gt;&gt; # opt.fun = lambda X: np.sum(X**2, axis=1)  # Re-attach if continuing\n            &gt;&gt;&gt; # opt.max_iter = 50  # Increase budget\n            &gt;&gt;&gt; # result = opt.optimize()\n        \"\"\"\n        if not os.path.exists(filename):\n            raise FileNotFoundError(f\"Result file not found: {filename}\")\n\n        try:\n            with open(filename, \"rb\") as handle:\n                optimizer = pickle.load(handle)\n            print(f\"Loaded result from {filename}\")\n\n            # Reinitialize components that were excluded\n            optimizer._reinitialize_components()\n\n            return optimizer\n        except Exception as e:\n            print(f\"Error loading result: {e}\")\n            raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.load_experiment","title":"<code>load_experiment(filename)</code>  <code>staticmethod</code>","text":"<p>Load an experiment configuration from a pickle file.</p> <p>Loads an experiment that was saved with save_experiment(). The loaded optimizer will have the configuration but not the objective function (which must be re-attached) or results.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the experiment pickle file.</p> required <p>Returns:</p> Name Type Description <code>SpotOptim</code> <code>SpotOptim</code> <p>Loaded optimizer instance (without fun attached).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file doesn\u2019t exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load experiment\n&gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\nLoaded experiment from sphere_opt_exp.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Re-attach objective function\n&gt;&gt;&gt; opt.fun = lambda X: np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run optimization\n&gt;&gt;&gt; result = opt.optimize()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>@staticmethod\ndef load_experiment(filename: str) -&gt; \"SpotOptim\":\n    \"\"\"Load an experiment configuration from a pickle file.\n\n    Loads an experiment that was saved with save_experiment(). The loaded optimizer\n    will have the configuration but not the objective function (which must be\n    re-attached) or results.\n\n    Args:\n        filename (str): Path to the experiment pickle file.\n\n    Returns:\n        SpotOptim: Loaded optimizer instance (without fun attached).\n\n    Raises:\n        FileNotFoundError: If the specified file doesn't exist.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load experiment\n        &gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n        Loaded experiment from sphere_opt_exp.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Re-attach objective function\n        &gt;&gt;&gt; opt.fun = lambda X: np.sum(X**2, axis=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run optimization\n        &gt;&gt;&gt; result = opt.optimize()\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"Experiment file not found: {filename}\")\n\n    try:\n        with open(filename, \"rb\") as handle:\n            optimizer = pickle.load(handle)\n        print(f\"Loaded experiment from {filename}\")\n\n        # Reinitialize components that were excluded\n        optimizer._reinitialize_components()\n\n        return optimizer\n    except Exception as e:\n        print(f\"Error loading experiment: {e}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.load_result","title":"<code>load_result(filename)</code>  <code>staticmethod</code>","text":"<p>Load complete optimization results from a pickle file.</p> <p>Loads results that were saved with save_result(). The loaded optimizer will have both configuration and all optimization results.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the result pickle file.</p> required <p>Returns:</p> Name Type Description <code>SpotOptim</code> <code>SpotOptim</code> <p>Loaded optimizer instance with complete results.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file doesn\u2019t exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load results\n&gt;&gt;&gt; opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\nLoaded result from sphere_opt_res.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Analyze results\n&gt;&gt;&gt; print(\"Best point:\", opt.best_x_)\n&gt;&gt;&gt; print(\"Best value:\", opt.best_y_)\n&gt;&gt;&gt; print(\"Total evaluations:\", opt.counter)\n&gt;&gt;&gt; print(\"Success rate:\", opt.success_rate)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Continue optimization if needed\n&gt;&gt;&gt; # opt.fun = lambda X: np.sum(X**2, axis=1)  # Re-attach if continuing\n&gt;&gt;&gt; # opt.max_iter = 50  # Increase budget\n&gt;&gt;&gt; # result = opt.optimize()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>@staticmethod\ndef load_result(filename: str) -&gt; \"SpotOptim\":\n    \"\"\"Load complete optimization results from a pickle file.\n\n    Loads results that were saved with save_result(). The loaded optimizer\n    will have both configuration and all optimization results.\n\n    Args:\n        filename (str): Path to the result pickle file.\n\n    Returns:\n        SpotOptim: Loaded optimizer instance with complete results.\n\n    Raises:\n        FileNotFoundError: If the specified file doesn't exist.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load results\n        &gt;&gt;&gt; opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n        Loaded result from sphere_opt_res.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Analyze results\n        &gt;&gt;&gt; print(\"Best point:\", opt.best_x_)\n        &gt;&gt;&gt; print(\"Best value:\", opt.best_y_)\n        &gt;&gt;&gt; print(\"Total evaluations:\", opt.counter)\n        &gt;&gt;&gt; print(\"Success rate:\", opt.success_rate)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Continue optimization if needed\n        &gt;&gt;&gt; # opt.fun = lambda X: np.sum(X**2, axis=1)  # Re-attach if continuing\n        &gt;&gt;&gt; # opt.max_iter = 50  # Increase budget\n        &gt;&gt;&gt; # result = opt.optimize()\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"Result file not found: {filename}\")\n\n    try:\n        with open(filename, \"rb\") as handle:\n            optimizer = pickle.load(handle)\n        print(f\"Loaded result from {filename}\")\n\n        # Reinitialize components that were excluded\n        optimizer._reinitialize_components()\n\n        return optimizer\n    except Exception as e:\n        print(f\"Error loading result: {e}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.optimize","title":"<code>optimize(X0=None)</code>","text":"<p>Run the optimization process.</p> <p>The optimization terminates when either: - Total function evaluations reach max_iter (including initial design), OR - Runtime exceeds max_time minutes</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>ndarray</code> <p>Initial design points, shape (n_initial, n_features). If None, generates space-filling design. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>OptimizeResult</code> <code>OptimizeResult</code> <p>Optimization result with fields: - x: best point found - fun: best function value - nfev: number of function evaluations (including initial design) - nit: number of sequential optimization iterations (after initial design) - success: whether optimization succeeded - message: termination message indicating reason for stopping - X: all evaluated points - y: all function values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example 1: Budget-based termination\n&gt;&gt;&gt; opt = SpotOptim(fun=objective, bounds=bounds, max_iter=30, n_initial=10)\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; # Will perform 10 initial + 20 sequential iterations = 30 total evaluations\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: Time-based termination\n&gt;&gt;&gt; opt = SpotOptim(fun=expensive_objective, bounds=bounds,\n...                 max_iter=1000, max_time=5.0)  # 5 minutes max\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; # Will stop after 5 minutes OR 1000 evaluations, whichever comes first\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def optimize(self, X0: Optional[np.ndarray] = None) -&gt; OptimizeResult:\n    \"\"\"Run the optimization process.\n\n    The optimization terminates when either:\n    - Total function evaluations reach max_iter (including initial design), OR\n    - Runtime exceeds max_time minutes\n\n    Args:\n        X0 (ndarray, optional): Initial design points, shape (n_initial, n_features).\n            If None, generates space-filling design. Defaults to None.\n\n    Returns:\n        OptimizeResult: Optimization result with fields:\n            - x: best point found\n            - fun: best function value\n            - nfev: number of function evaluations (including initial design)\n            - nit: number of sequential optimization iterations (after initial design)\n            - success: whether optimization succeeded\n            - message: termination message indicating reason for stopping\n            - X: all evaluated points\n            - y: all function values\n\n    Examples:\n        &gt;&gt;&gt; # Example 1: Budget-based termination\n        &gt;&gt;&gt; opt = SpotOptim(fun=objective, bounds=bounds, max_iter=30, n_initial=10)\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; # Will perform 10 initial + 20 sequential iterations = 30 total evaluations\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: Time-based termination\n        &gt;&gt;&gt; opt = SpotOptim(fun=expensive_objective, bounds=bounds,\n        ...                 max_iter=1000, max_time=5.0)  # 5 minutes max\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; # Will stop after 5 minutes OR 1000 evaluations, whichever comes first\n    \"\"\"\n    # Generate or use provided initial design\n    if X0 is None:\n        X0 = self._generate_initial_design()\n    else:\n        X0 = np.atleast_2d(X0)\n        # If X0 is in full dimensions and we have dimension reduction, reduce it\n        if self.red_dim and X0.shape[1] == len(self.ident):\n            X0 = self.to_red_dim(X0)\n        X0 = self._repair_non_numeric(X0, self.var_type)\n\n    # Repeat initial design points if repeats_initial &gt; 1\n    if self.repeats_initial &gt; 1:\n        X0 = np.repeat(X0, self.repeats_initial, axis=0)\n\n    # Evaluate initial design\n    y0 = self._evaluate_function(X0)\n\n    # Update success rate BEFORE updating storage (initial design - all should be successes since starting from scratch)\n    self._update_success_rate(y0)\n\n    # Initialize storage\n    self.X_ = X0.copy()\n    self.y_ = y0.copy()\n    self.n_iter_ = 0\n\n    # Update stats after initial design\n    self.update_stats()\n\n    # Log initial design to TensorBoard\n    if self.tb_writer is not None:\n        for i in range(len(self.y_)):\n            self._write_tensorboard_hparams(self.X_[i], self.y_[i])\n        self._write_tensorboard_scalars()\n\n    # Initial best\n    best_idx = np.argmin(self.y_)\n    self.best_x_ = self.X_[best_idx].copy()\n    self.best_y_ = self.y_[best_idx]\n\n    if self.verbose:\n        if self.noise:\n            print(\n                f\"Initial best: f(x) = {self.best_y_:.6f}, mean best: f(x) = {self.min_mean_y:.6f}\"\n            )\n        else:\n            print(f\"Initial best: f(x) = {self.best_y_:.6f}\")\n\n    # Start timer for max_time check\n    timeout_start = time.time()\n\n    # Main optimization loop\n    # Termination: continue while (total_evals &lt; max_iter) AND (elapsed_time &lt; max_time)\n    while (len(self.y_) &lt; self.max_iter) and (\n        time.time() &lt; timeout_start + self.max_time * 60\n    ):\n        self.n_iter_ += 1\n\n        # Fit surrogate (use mean_y if noise, otherwise y_)\n        if self.noise:\n            self._fit_surrogate(self.mean_X, self.mean_y)\n        else:\n            self._fit_surrogate(self.X_, self.y_)\n\n        # OCBA: Compute optimal budget allocation for noisy functions\n        # This determines which existing design points should be re-evaluated\n        X_ocba = None\n        if self.noise and self.ocba_delta &gt; 0:\n            # Check conditions for OCBA (need variance &gt; 0 and at least 3 points)\n            if not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n                if self.verbose:\n                    print(\n                        f\"Warning: OCBA skipped (need &gt;2 points with variance &gt; 0)\"\n                    )\n            elif np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n                # Get OCBA allocation\n                X_ocba = self._get_ocba_X(\n                    self.mean_X, self.mean_y, self.var_y, self.ocba_delta\n                )\n                if self.verbose and X_ocba is not None:\n                    print(f\"  OCBA: Adding {X_ocba.shape[0]} re-evaluation(s)\")\n\n        # Suggest next point\n        x_next = self._suggest_next_point()\n\n        # Repeat next point if repeats_surrogate &gt; 1\n        if self.repeats_surrogate &gt; 1:\n            x_next_repeated = np.repeat(\n                x_next.reshape(1, -1), self.repeats_surrogate, axis=0\n            )\n        else:\n            x_next_repeated = x_next.reshape(1, -1)\n\n        # Append OCBA points to new design points (if applicable)\n        if X_ocba is not None:\n            x_next_repeated = append(X_ocba, x_next_repeated, axis=0)\n\n        # Evaluate next point(s) including OCBA points\n        y_next = self._evaluate_function(x_next_repeated)\n\n        # Update success rate BEFORE updating storage (so it compares against previous best)\n        self._update_success_rate(y_next)\n\n        # Update storage\n        self.X_ = np.vstack([self.X_, x_next_repeated])\n        self.y_ = np.append(self.y_, y_next)\n\n        # Update stats\n        self.update_stats()\n\n        # Log to TensorBoard\n        if self.tb_writer is not None:\n            # Log each new evaluation\n            for i in range(len(y_next)):\n                self._write_tensorboard_hparams(x_next_repeated[i], y_next[i])\n            self._write_tensorboard_scalars()\n\n        # Update best\n        current_best = np.min(y_next)\n        if current_best &lt; self.best_y_:\n            best_idx_in_new = np.argmin(y_next)\n            self.best_x_ = x_next_repeated[best_idx_in_new].copy()\n            self.best_y_ = current_best\n\n            if self.verbose:\n                if self.noise:\n                    print(\n                        f\"Iteration {self.n_iter_}: New best f(x) = {self.best_y_:.6f}, mean best: f(x) = {self.min_mean_y:.6f}\"\n                    )\n                else:\n                    print(\n                        f\"Iteration {self.n_iter_}: New best f(x) = {self.best_y_:.6f}\"\n                    )\n        elif self.verbose:\n            if self.noise:\n                mean_y_new = np.mean(y_next)\n                print(f\"Iteration {self.n_iter_}: mean f(x) = {mean_y_new:.6f}\")\n            else:\n                print(f\"Iteration {self.n_iter_}: f(x) = {y_next[0]:.6f}\")\n\n    # Expand results to full dimensions if needed\n    best_x_full = (\n        self.to_all_dim(self.best_x_.reshape(1, -1))[0]\n        if self.red_dim\n        else self.best_x_\n    )\n    X_full = self.to_all_dim(self.X_) if self.red_dim else self.X_\n\n    # Determine termination reason\n    elapsed_time = time.time() - timeout_start\n    if len(self.y_) &gt;= self.max_iter:\n        message = f\"Optimization terminated: maximum evaluations ({self.max_iter}) reached\"\n    elif elapsed_time &gt;= self.max_time * 60:\n        message = (\n            f\"Optimization terminated: time limit ({self.max_time:.2f} min) reached\"\n        )\n    else:\n        message = \"Optimization finished successfully\"\n\n    # Close TensorBoard writer\n    self._close_tensorboard_writer()\n\n    # Return scipy-style result\n    return OptimizeResult(\n        x=best_x_full,\n        fun=self.best_y_,\n        nfev=len(self.y_),\n        nit=self.n_iter_,\n        success=True,\n        message=message,\n        X=X_full,\n        y=self.y_,\n    )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.plot_surrogate","title":"<code>plot_surrogate(i=0, j=1, show=True, alpha=0.8, var_name=None, cmap='jet', num=100, vmin=None, vmax=None, add_points=True, grid_visible=True, contour_levels=30, figsize=(12, 10))</code>","text":"<p>Plot the surrogate model for two dimensions.</p> <p>Creates a 2x2 plot showing: - Top left: 3D surface of predictions - Top right: 3D surface of prediction uncertainty - Bottom left: Contour plot of predictions with evaluated points - Bottom right: Contour plot of prediction uncertainty</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the first dimension to plot. Defaults to 0.</p> <code>0</code> <code>j</code> <code>int</code> <p>Index of the second dimension to plot. Defaults to 1.</p> <code>1</code> <code>show</code> <code>bool</code> <p>If True, displays the plot immediately. Defaults to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency of the 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.</p> <code>0.8</code> <code>var_name</code> <code>list of str</code> <p>Names for each dimension. If None, uses instance var_name. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to \u2018jet\u2019.</p> <code>'jet'</code> <code>num</code> <code>int</code> <p>Number of grid points per dimension for mesh grid. Defaults to 100.</p> <code>100</code> <code>vmin</code> <code>float</code> <p>Minimum value for color scale. If None, determined from data. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>Maximum value for color scale. If None, determined from data. Defaults to None.</p> <code>None</code> <code>add_points</code> <code>bool</code> <p>If True, overlay evaluated points on contour plots. Defaults to True.</p> <code>True</code> <code>grid_visible</code> <code>bool</code> <p>If True, show grid lines on contour plots. Defaults to True.</p> <code>True</code> <code>contour_levels</code> <code>int</code> <p>Number of contour levels. Defaults to 30.</p> <code>30</code> <code>figsize</code> <code>tuple of int</code> <p>Figure size in inches (width, height). Defaults to (12, 10).</p> <code>(12, 10)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimization hasn\u2019t been run yet, or if i, j are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt; # Example 1: Using var_name in constructor\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n...                 max_iter=10, n_initial=5, var_name=['x1', 'x2'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; opt.plot_surrogate(i=0, j=1)  # Uses instance var_name\n&gt;&gt;&gt; # Example 2: Override var_name for this plot\n&gt;&gt;&gt; opt.plot_surrogate(i=0, j=1, var_name=['custom1', 'custom2'])\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def plot_surrogate(\n    self,\n    i: int = 0,\n    j: int = 1,\n    show: bool = True,\n    alpha: float = 0.8,\n    var_name: Optional[List[str]] = None,\n    cmap: str = \"jet\",\n    num: int = 100,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    add_points: bool = True,\n    grid_visible: bool = True,\n    contour_levels: int = 30,\n    figsize: Tuple[int, int] = (12, 10),\n) -&gt; None:\n    \"\"\"Plot the surrogate model for two dimensions.\n\n    Creates a 2x2 plot showing:\n    - Top left: 3D surface of predictions\n    - Top right: 3D surface of prediction uncertainty\n    - Bottom left: Contour plot of predictions with evaluated points\n    - Bottom right: Contour plot of prediction uncertainty\n\n    Args:\n        i (int, optional): Index of the first dimension to plot. Defaults to 0.\n        j (int, optional): Index of the second dimension to plot. Defaults to 1.\n        show (bool, optional): If True, displays the plot immediately. Defaults to True.\n        alpha (float, optional): Transparency of the 3D surface plots (0=transparent, 1=opaque).\n            Defaults to 0.8.\n        var_name (list of str, optional): Names for each dimension. If None, uses instance var_name.\n            Defaults to None.\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'jet'.\n        num (int, optional): Number of grid points per dimension for mesh grid. Defaults to 100.\n        vmin (float, optional): Minimum value for color scale. If None, determined from data.\n            Defaults to None.\n        vmax (float, optional): Maximum value for color scale. If None, determined from data.\n            Defaults to None.\n        add_points (bool, optional): If True, overlay evaluated points on contour plots.\n            Defaults to True.\n        grid_visible (bool, optional): If True, show grid lines on contour plots. Defaults to True.\n        contour_levels (int, optional): Number of contour levels. Defaults to 30.\n        figsize (tuple of int, optional): Figure size in inches (width, height). Defaults to (12, 10).\n\n    Raises:\n        ValueError: If optimization hasn't been run yet, or if i, j are invalid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; def sphere(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt; # Example 1: Using var_name in constructor\n        &gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n        ...                 max_iter=10, n_initial=5, var_name=['x1', 'x2'])\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; opt.plot_surrogate(i=0, j=1)  # Uses instance var_name\n        &gt;&gt;&gt; # Example 2: Override var_name for this plot\n        &gt;&gt;&gt; opt.plot_surrogate(i=0, j=1, var_name=['custom1', 'custom2'])\n    \"\"\"\n    # Validation\n    if self.X_ is None or self.y_ is None:\n        raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n    k = self.n_dim\n    if i &gt;= k or j &gt;= k:\n        raise ValueError(f\"Dimensions i={i} and j={j} must be less than n_dim={k}.\")\n    if i == j:\n        raise ValueError(\"Dimensions i and j must be different.\")\n\n    # Use instance var_name if not provided\n    if var_name is None:\n        var_name = self.var_name\n\n    # Generate mesh grid\n    X_i, X_j, grid_points = self._generate_mesh_grid(i, j, num)\n\n    # Predict on grid\n    y_pred, y_std = self.surrogate.predict(grid_points, return_std=True)\n    Z_pred = y_pred.reshape(X_i.shape)\n    Z_std = y_std.reshape(X_i.shape)\n\n    # Create figure\n    fig = plt.figure(figsize=figsize)\n\n    # Plot 1: 3D surface of predictions\n    ax1 = fig.add_subplot(221, projection=\"3d\")\n    ax1.plot_surface(X_i, X_j, Z_pred, cmap=cmap, alpha=alpha, vmin=vmin, vmax=vmax)\n    ax1.set_title(\"Prediction Surface\")\n    ax1.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax1.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax1.set_zlabel(\"Prediction\")\n\n    # Plot 2: 3D surface of prediction uncertainty\n    ax2 = fig.add_subplot(222, projection=\"3d\")\n    ax2.plot_surface(X_i, X_j, Z_std, cmap=cmap, alpha=alpha)\n    ax2.set_title(\"Prediction Uncertainty Surface\")\n    ax2.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax2.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax2.set_zlabel(\"Std. Dev.\")\n\n    # Plot 3: Contour of predictions\n    ax3 = fig.add_subplot(223)\n    contour3 = ax3.contourf(\n        X_i, X_j, Z_pred, levels=contour_levels, cmap=cmap, vmin=vmin, vmax=vmax\n    )\n    plt.colorbar(contour3, ax=ax3)\n    if add_points:\n        ax3.scatter(\n            self.X_[:, i],\n            self.X_[:, j],\n            c=\"red\",\n            s=30,\n            edgecolors=\"black\",\n            zorder=5,\n            label=\"Evaluated points\",\n        )\n        ax3.legend()\n    ax3.set_title(\"Prediction Contour\")\n    ax3.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax3.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax3.grid(visible=grid_visible)\n\n    # Plot 4: Contour of prediction uncertainty\n    ax4 = fig.add_subplot(224)\n    contour4 = ax4.contourf(X_i, X_j, Z_std, levels=contour_levels, cmap=cmap)\n    plt.colorbar(contour4, ax=ax4)\n    if add_points:\n        ax4.scatter(\n            self.X_[:, i],\n            self.X_[:, j],\n            c=\"red\",\n            s=30,\n            edgecolors=\"black\",\n            zorder=5,\n            label=\"Evaluated points\",\n        )\n        ax4.legend()\n    ax4.set_title(\"Uncertainty Contour\")\n    ax4.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax4.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax4.grid(visible=grid_visible)\n\n    plt.tight_layout()\n\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.save_experiment","title":"<code>save_experiment(filename=None, prefix='experiment', path=None, overwrite=True, unpickleables='all', verbosity=0)</code>","text":"<p>Save the experiment configuration to a pickle file.</p> <p>An experiment contains the optimizer configuration needed to run optimization, but excludes the results. This is useful for defining experiments locally and executing them on remote machines.</p> <p>The experiment includes: - Bounds, variable types, variable names - Optimization parameters (max_iter, n_initial, etc.) - Surrogate and acquisition settings - Random seed</p> <p>The experiment excludes: - Function evaluations (X_, y_) - Optimization results - Objective function (must be re-attached after loading)</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename for the experiment file. If None, generates from prefix. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for auto-generated filename. Defaults to \u201cexperiment\u201d.</p> <code>'experiment'</code> <code>path</code> <code>str</code> <p>Directory path to save the file. If None, saves in current directory. Creates directory if it doesn\u2019t exist. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrites existing file. If False, raises error if file exists. Defaults to True.</p> <code>True</code> <code>unpickleables</code> <code>str</code> <p>Components to exclude for pickling: - \u201call\u201d: Excludes fun, surrogate, lhs_sampler, tb_writer (experiment only) - \u201cfile_io\u201d: Excludes only tb_writer (lighter exclusion) Defaults to \u201call\u201d.</p> <code>'all'</code> <code>verbosity</code> <code>int</code> <p>Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define experiment locally\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Save experiment (without results)\n&gt;&gt;&gt; opt.save_experiment(prefix=\"sphere_opt\")\nExperiment saved to sphere_opt_exp.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # On remote machine: load and run\n&gt;&gt;&gt; # opt_remote = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n&gt;&gt;&gt; # opt_remote.fun = objective_function  # Re-attach function\n&gt;&gt;&gt; # result = opt_remote.optimize()\n&gt;&gt;&gt; # opt_remote.save_result(prefix=\"sphere_opt\")  # Save results\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def save_experiment(\n    self,\n    filename: Optional[str] = None,\n    prefix: str = \"experiment\",\n    path: Optional[str] = None,\n    overwrite: bool = True,\n    unpickleables: str = \"all\",\n    verbosity: int = 0,\n) -&gt; None:\n    \"\"\"Save the experiment configuration to a pickle file.\n\n    An experiment contains the optimizer configuration needed to run optimization,\n    but excludes the results. This is useful for defining experiments locally and\n    executing them on remote machines.\n\n    The experiment includes:\n    - Bounds, variable types, variable names\n    - Optimization parameters (max_iter, n_initial, etc.)\n    - Surrogate and acquisition settings\n    - Random seed\n\n    The experiment excludes:\n    - Function evaluations (X_, y_)\n    - Optimization results\n    - Objective function (must be re-attached after loading)\n\n    Args:\n        filename (str, optional): Filename for the experiment file. If None, generates\n            from prefix. Defaults to None.\n        prefix (str): Prefix for auto-generated filename. Defaults to \"experiment\".\n        path (str, optional): Directory path to save the file. If None, saves in current\n            directory. Creates directory if it doesn't exist. Defaults to None.\n        overwrite (bool): If True, overwrites existing file. If False, raises error if\n            file exists. Defaults to True.\n        unpickleables (str): Components to exclude for pickling:\n            - \"all\": Excludes fun, surrogate, lhs_sampler, tb_writer (experiment only)\n            - \"file_io\": Excludes only tb_writer (lighter exclusion)\n            Defaults to \"all\".\n        verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define experiment locally\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Save experiment (without results)\n        &gt;&gt;&gt; opt.save_experiment(prefix=\"sphere_opt\")\n        Experiment saved to sphere_opt_exp.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # On remote machine: load and run\n        &gt;&gt;&gt; # opt_remote = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n        &gt;&gt;&gt; # opt_remote.fun = objective_function  # Re-attach function\n        &gt;&gt;&gt; # result = opt_remote.optimize()\n        &gt;&gt;&gt; # opt_remote.save_result(prefix=\"sphere_opt\")  # Save results\n    \"\"\"\n    # Close TensorBoard writer before pickling\n    self._close_and_del_tensorboard_writer()\n\n    # Create pickle-safe copy\n    optimizer_copy = self._get_pickle_safe_optimizer(\n        unpickleables=unpickleables, verbosity=verbosity\n    )\n\n    # Determine filename\n    if filename is None:\n        filename = self._get_experiment_filename(prefix)\n\n    # Add path if provided\n    if path is not None:\n        if not os.path.exists(path):\n            os.makedirs(path)\n        filename = os.path.join(path, filename)\n\n    # Check for existing file\n    if os.path.exists(filename) and not overwrite:\n        raise FileExistsError(\n            f\"File {filename} already exists. Use overwrite=True to overwrite.\"\n        )\n\n    # Save to pickle file\n    try:\n        with open(filename, \"wb\") as handle:\n            pickle.dump(optimizer_copy, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        print(f\"Experiment saved to {filename}\")\n    except Exception as e:\n        print(f\"Error during pickling: {e}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.save_result","title":"<code>save_result(filename=None, prefix='result', path=None, overwrite=True, verbosity=0)</code>","text":"<p>Save the complete optimization results to a pickle file.</p> <p>A result contains all information from a completed optimization run, including the experiment configuration and all evaluation results. This is useful for saving completed runs for later analysis.</p> <p>The result includes everything in an experiment plus: - All evaluated points (X_) - All function values (y_) - Best point and best value - Iteration count - Success rate statistics - Noise statistics (if applicable)</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename for the result file. If None, generates from prefix. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for auto-generated filename. Defaults to \u201cresult\u201d.</p> <code>'result'</code> <code>path</code> <code>str</code> <p>Directory path to save the file. If None, saves in current directory. Creates directory if it doesn\u2019t exist. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrites existing file. If False, raises error if file exists. Defaults to True.</p> <code>True</code> <code>verbosity</code> <code>int</code> <p>Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run optimization\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Save complete results\n&gt;&gt;&gt; opt.save_result(prefix=\"sphere_opt\")\nResult saved to sphere_opt_res.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Later: load and analyze\n&gt;&gt;&gt; # opt_loaded = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n&gt;&gt;&gt; # print(\"Best value:\", opt_loaded.best_y_)\n&gt;&gt;&gt; # opt_loaded.plot_surrogate()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def save_result(\n    self,\n    filename: Optional[str] = None,\n    prefix: str = \"result\",\n    path: Optional[str] = None,\n    overwrite: bool = True,\n    verbosity: int = 0,\n) -&gt; None:\n    \"\"\"Save the complete optimization results to a pickle file.\n\n    A result contains all information from a completed optimization run, including\n    the experiment configuration and all evaluation results. This is useful for\n    saving completed runs for later analysis.\n\n    The result includes everything in an experiment plus:\n    - All evaluated points (X_)\n    - All function values (y_)\n    - Best point and best value\n    - Iteration count\n    - Success rate statistics\n    - Noise statistics (if applicable)\n\n    Args:\n        filename (str, optional): Filename for the result file. If None, generates\n            from prefix. Defaults to None.\n        prefix (str): Prefix for auto-generated filename. Defaults to \"result\".\n        path (str, optional): Directory path to save the file. If None, saves in current\n            directory. Creates directory if it doesn't exist. Defaults to None.\n        overwrite (bool): If True, overwrites existing file. If False, raises error if\n            file exists. Defaults to True.\n        verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run optimization\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Save complete results\n        &gt;&gt;&gt; opt.save_result(prefix=\"sphere_opt\")\n        Result saved to sphere_opt_res.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Later: load and analyze\n        &gt;&gt;&gt; # opt_loaded = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n        &gt;&gt;&gt; # print(\"Best value:\", opt_loaded.best_y_)\n        &gt;&gt;&gt; # opt_loaded.plot_surrogate()\n    \"\"\"\n    # Use save_experiment with file_io unpickleables to preserve results\n    if filename is None:\n        filename = self._get_result_filename(prefix)\n\n    self.save_experiment(\n        filename=filename,\n        path=path,\n        overwrite=overwrite,\n        unpickleables=\"file_io\",\n        verbosity=verbosity,\n    )\n\n    # Update message\n    if path is not None:\n        full_path = os.path.join(path, filename)\n    else:\n        full_path = filename\n    print(f\"Result saved to {full_path}\")\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.to_all_dim","title":"<code>to_all_dim(X_red)</code>","text":"<p>Expand reduced-dimensional points to full-dimensional representation.</p> <p>This method restores points from the reduced optimization space to the full-dimensional space by inserting fixed values for constant dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>X_red</code> <code>ndarray</code> <p>Points in reduced space, shape (n_samples, n_reduced_dims).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Points in full space, shape (n_samples, n_original_dims).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Create problem with one fixed dimension\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n...     max_iter=1,\n...     n_initial=3\n... )\n&gt;&gt;&gt; X_red = np.array([[1.0, 3.0], [2.0, 4.0]])  # Only x0 and x2\n&gt;&gt;&gt; X_full = opt.to_all_dim(X_red)\n&gt;&gt;&gt; X_full.shape\n(2, 3)\n&gt;&gt;&gt; X_full[:, 1]  # Middle dimension should be 2.0\narray([2., 2.])\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def to_all_dim(self, X_red: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Expand reduced-dimensional points to full-dimensional representation.\n\n    This method restores points from the reduced optimization space to the\n    full-dimensional space by inserting fixed values for constant dimensions.\n\n    Args:\n        X_red (ndarray): Points in reduced space, shape (n_samples, n_reduced_dims).\n\n    Returns:\n        ndarray: Points in full space, shape (n_samples, n_original_dims).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Create problem with one fixed dimension\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n        ...     max_iter=1,\n        ...     n_initial=3\n        ... )\n        &gt;&gt;&gt; X_red = np.array([[1.0, 3.0], [2.0, 4.0]])  # Only x0 and x2\n        &gt;&gt;&gt; X_full = opt.to_all_dim(X_red)\n        &gt;&gt;&gt; X_full.shape\n        (2, 3)\n        &gt;&gt;&gt; X_full[:, 1]  # Middle dimension should be 2.0\n        array([2., 2.])\n    \"\"\"\n    if not self.red_dim:\n        # No reduction occurred, return as-is\n        return X_red\n\n    # Number of samples and full dimensions\n    n_samples = X_red.shape[0]\n    n_full_dims = len(self.ident)\n\n    # Initialize full-dimensional array\n    X_full = np.zeros((n_samples, n_full_dims))\n\n    # Track index in reduced array\n    red_idx = 0\n\n    # Fill in values dimension by dimension\n    for i in range(n_full_dims):\n        if self.ident[i]:\n            # Fixed dimension: use stored value\n            X_full[:, i] = self.all_lower[i]\n        else:\n            # Varying dimension: use value from reduced array\n            X_full[:, i] = X_red[:, red_idx]\n            red_idx += 1\n\n    return X_full\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.to_red_dim","title":"<code>to_red_dim(X_full)</code>","text":"<p>Reduce full-dimensional points to optimization space.</p> <p>This method removes fixed dimensions from full-dimensional points, extracting only the varying dimensions used in optimization.</p> <p>Parameters:</p> Name Type Description Default <code>X_full</code> <code>ndarray</code> <p>Points in full space, shape (n_samples, n_original_dims).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Points in reduced space, shape (n_samples, n_reduced_dims).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Create problem with one fixed dimension\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n...     max_iter=1,\n...     n_initial=3\n... )\n&gt;&gt;&gt; X_full = np.array([[1.0, 2.0, 3.0], [4.0, 2.0, 5.0]])\n&gt;&gt;&gt; X_red = opt.to_red_dim(X_full)\n&gt;&gt;&gt; X_red.shape\n(2, 2)\n&gt;&gt;&gt; np.array_equal(X_red, np.array([[1.0, 3.0], [4.0, 5.0]]))\nTrue\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def to_red_dim(self, X_full: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Reduce full-dimensional points to optimization space.\n\n    This method removes fixed dimensions from full-dimensional points,\n    extracting only the varying dimensions used in optimization.\n\n    Args:\n        X_full (ndarray): Points in full space, shape (n_samples, n_original_dims).\n\n    Returns:\n        ndarray: Points in reduced space, shape (n_samples, n_reduced_dims).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Create problem with one fixed dimension\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n        ...     max_iter=1,\n        ...     n_initial=3\n        ... )\n        &gt;&gt;&gt; X_full = np.array([[1.0, 2.0, 3.0], [4.0, 2.0, 5.0]])\n        &gt;&gt;&gt; X_red = opt.to_red_dim(X_full)\n        &gt;&gt;&gt; X_red.shape\n        (2, 2)\n        &gt;&gt;&gt; np.array_equal(X_red, np.array([[1.0, 3.0], [4.0, 5.0]]))\n        True\n    \"\"\"\n    if not self.red_dim:\n        # No reduction occurred, return as-is\n        return X_full\n\n    # Select only non-fixed dimensions\n    return X_full[:, ~self.ident]\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.update_stats","title":"<code>update_stats()</code>","text":"<p>Update optimization statistics.</p> <p>Updates: 1. <code>min_y</code>: Minimum y value found so far 2. <code>min_X</code>: X value corresponding to minimum y 3. <code>counter</code>: Total number of function evaluations</p> <p>Note: <code>success_rate</code> is updated separately via <code>_update_success_rate()</code> method, which is called after each batch of function evaluations.</p> <p>If <code>noise</code> is True (repeats &gt; 1), additionally computes: 1. <code>mean_X</code>: Unique design points (aggregated from repeated evaluations) 2. <code>mean_y</code>: Mean y values per design point 3. <code>var_y</code>: Variance of y values per design point 4. <code>min_mean_X</code>: X value of the best mean y value 5. <code>min_mean_y</code>: Best mean y value 6. <code>min_var_y</code>: Variance of the best mean y value</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Without noise\n&gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n...                 bounds=[(-5, 5), (-5, 5)],\n...                 max_iter=10, n_initial=5)\n&gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n&gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0, 1.0])\n&gt;&gt;&gt; opt.update_stats()\n&gt;&gt;&gt; opt.min_y\n1.0\n&gt;&gt;&gt; opt.min_X\narray([0, 1])\n&gt;&gt;&gt; opt.counter\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With noise\n&gt;&gt;&gt; opt_noise = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n...                       bounds=[(-5, 5), (-5, 5)],\n...                       repeats_initial=2)\n&gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [1, 2], [3, 4]])\n&gt;&gt;&gt; opt_noise.y_ = np.array([4.0, 6.0, 25.0])\n&gt;&gt;&gt; opt_noise.update_stats()\n&gt;&gt;&gt; opt_noise.min_y\n4.0\n&gt;&gt;&gt; opt_noise.mean_y\narray([ 5., 25.])\n&gt;&gt;&gt; opt_noise.var_y\narray([1., 0.])\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def update_stats(self) -&gt; None:\n    \"\"\"Update optimization statistics.\n\n    Updates:\n    1. `min_y`: Minimum y value found so far\n    2. `min_X`: X value corresponding to minimum y\n    3. `counter`: Total number of function evaluations\n\n    Note: `success_rate` is updated separately via `_update_success_rate()` method,\n    which is called after each batch of function evaluations.\n\n    If `noise` is True (repeats &gt; 1), additionally computes:\n    1. `mean_X`: Unique design points (aggregated from repeated evaluations)\n    2. `mean_y`: Mean y values per design point\n    3. `var_y`: Variance of y values per design point\n    4. `min_mean_X`: X value of the best mean y value\n    5. `min_mean_y`: Best mean y value\n    6. `min_var_y`: Variance of the best mean y value\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Without noise\n        &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n        ...                 bounds=[(-5, 5), (-5, 5)],\n        ...                 max_iter=10, n_initial=5)\n        &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n        &gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0, 1.0])\n        &gt;&gt;&gt; opt.update_stats()\n        &gt;&gt;&gt; opt.min_y\n        1.0\n        &gt;&gt;&gt; opt.min_X\n        array([0, 1])\n        &gt;&gt;&gt; opt.counter\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With noise\n        &gt;&gt;&gt; opt_noise = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n        ...                       bounds=[(-5, 5), (-5, 5)],\n        ...                       repeats_initial=2)\n        &gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [1, 2], [3, 4]])\n        &gt;&gt;&gt; opt_noise.y_ = np.array([4.0, 6.0, 25.0])\n        &gt;&gt;&gt; opt_noise.update_stats()\n        &gt;&gt;&gt; opt_noise.min_y\n        4.0\n        &gt;&gt;&gt; opt_noise.mean_y\n        array([ 5., 25.])\n        &gt;&gt;&gt; opt_noise.var_y\n        array([1., 0.])\n    \"\"\"\n    if self.y_ is None or len(self.y_) == 0:\n        return\n\n    # Basic stats\n    self.min_y = np.min(self.y_)\n    self.min_X = self.X_[np.argmin(self.y_)]\n    self.counter = len(self.y_)\n\n    # Aggregated stats for noisy functions\n    if self.noise:\n        self.mean_X, self.mean_y, self.var_y = self._aggregate_mean_var(\n            self.X_, self.y_\n        )\n        # X value of the best mean y value so far\n        best_mean_idx = np.argmin(self.mean_y)\n        self.min_mean_X = self.mean_X[best_mean_idx]\n        # Best mean y value so far\n        self.min_mean_y = self.mean_y[best_mean_idx]\n        # Variance of the best mean y value so far\n        self.min_var_y = self.var_y[best_mean_idx]\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/","title":"diabetes","text":"<p>Diabetes dataset for regression tasks.</p> <p>Provides PyTorch Dataset and DataLoader utilities for the sklearn diabetes dataset.</p>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.DiabetesDataset","title":"<code>DiabetesDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch Dataset for the diabetes dataset from sklearn.</p> <p>The diabetes dataset contains 10 baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements) for 442 diabetes patients. The target is a quantitative measure of disease progression one year after baseline.</p> <p>This dataset is useful for testing regression algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target values of shape (n_samples,) or (n_samples, 1).</p> required <code>transform</code> <code>callable</code> <p>Optional transform to be applied to features.</p> <code>None</code> <code>target_transform</code> <code>callable</code> <p>Optional transform to be applied to targets.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>X</code> <code>Tensor</code> <p>Feature tensor of shape (n_samples, n_features).</p> <code>y</code> <code>Tensor</code> <p>Target tensor of shape (n_samples, 1).</p> <code>n_features</code> <code>int</code> <p>Number of features (10 for diabetes dataset).</p> <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; from spotoptim.data import DiabetesDataset\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create dataset\n&gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n&gt;&gt;&gt; print(f\"Dataset size: {len(dataset)}\")\n&gt;&gt;&gt; print(f\"Features shape: {dataset.X.shape}\")\n&gt;&gt;&gt; print(f\"Targets shape: {dataset.y.shape}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get a sample\n&gt;&gt;&gt; features, target = dataset[0]\n&gt;&gt;&gt; print(f\"Sample features: {features.shape}\")\n&gt;&gt;&gt; print(f\"Sample target: {target.shape}\")\n</code></pre> <p>With data splitting and scaling:</p> <pre><code>&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and split data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Scale features\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create datasets\n&gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n&gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n</code></pre> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>class DiabetesDataset(Dataset):\n    \"\"\"PyTorch Dataset for the diabetes dataset from sklearn.\n\n    The diabetes dataset contains 10 baseline variables (age, sex, body mass index,\n    average blood pressure, and six blood serum measurements) for 442 diabetes\n    patients. The target is a quantitative measure of disease progression one year\n    after baseline.\n\n    This dataset is useful for testing regression algorithms.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target values of shape (n_samples,) or (n_samples, 1).\n        transform (callable, optional): Optional transform to be applied to features.\n        target_transform (callable, optional): Optional transform to be applied to targets.\n\n    Attributes:\n        X (torch.Tensor): Feature tensor of shape (n_samples, n_features).\n        y (torch.Tensor): Target tensor of shape (n_samples, 1).\n        n_features (int): Number of features (10 for diabetes dataset).\n        n_samples (int): Number of samples in the dataset.\n\n    Examples:\n        Basic usage:\n\n        &gt;&gt;&gt; from spotoptim.data import DiabetesDataset\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create dataset\n        &gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n        &gt;&gt;&gt; print(f\"Dataset size: {len(dataset)}\")\n        &gt;&gt;&gt; print(f\"Features shape: {dataset.X.shape}\")\n        &gt;&gt;&gt; print(f\"Targets shape: {dataset.y.shape}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get a sample\n        &gt;&gt;&gt; features, target = dataset[0]\n        &gt;&gt;&gt; print(f\"Sample features: {features.shape}\")\n        &gt;&gt;&gt; print(f\"Sample target: {target.shape}\")\n\n        With data splitting and scaling:\n\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and split data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, test_size=0.2, random_state=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Scale features\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n        &gt;&gt;&gt; X_test = scaler.transform(X_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create datasets\n        &gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n        &gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n    \"\"\"\n\n    def __init__(self, X, y, transform=None, target_transform=None):\n        \"\"\"Initialize the DiabetesDataset.\n\n        Args:\n            X (np.ndarray): Feature matrix.\n            y (np.ndarray): Target values.\n            transform (callable, optional): Transform for features.\n            target_transform (callable, optional): Transform for targets.\n        \"\"\"\n        # Ensure y is 2D\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n\n        self.X = torch.FloatTensor(X)\n        self.y = torch.FloatTensor(y)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.n_features = self.X.shape[1]\n        self.n_samples = self.X.shape[0]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples in the dataset.\n\n        Returns:\n            int: Number of samples.\n        \"\"\"\n        return len(self.X)\n\n    def __getitem__(self, idx: int) -&gt; tuple:\n        \"\"\"Get a sample from the dataset.\n\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: (features, target) where features is a tensor of shape (n_features,)\n                   and target is a tensor of shape (1,).\n        \"\"\"\n        features = self.X[idx]\n        target = self.y[idx]\n\n        if self.transform:\n            features = self.transform(features)\n        if self.target_transform:\n            target = self.target_transform(target)\n\n        return features, target\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.DiabetesDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(features, target) where features is a tensor of shape (n_features,)    and target is a tensor of shape (1,).</p> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; tuple:\n    \"\"\"Get a sample from the dataset.\n\n    Args:\n        idx (int): Index of the sample to retrieve.\n\n    Returns:\n        tuple: (features, target) where features is a tensor of shape (n_features,)\n               and target is a tensor of shape (1,).\n    \"\"\"\n    features = self.X[idx]\n    target = self.y[idx]\n\n    if self.transform:\n        features = self.transform(features)\n    if self.target_transform:\n        target = self.target_transform(target)\n\n    return features, target\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.DiabetesDataset.__init__","title":"<code>__init__(X, y, transform=None, target_transform=None)</code>","text":"<p>Initialize the DiabetesDataset.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix.</p> required <code>y</code> <code>ndarray</code> <p>Target values.</p> required <code>transform</code> <code>callable</code> <p>Transform for features.</p> <code>None</code> <code>target_transform</code> <code>callable</code> <p>Transform for targets.</p> <code>None</code> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>def __init__(self, X, y, transform=None, target_transform=None):\n    \"\"\"Initialize the DiabetesDataset.\n\n    Args:\n        X (np.ndarray): Feature matrix.\n        y (np.ndarray): Target values.\n        transform (callable, optional): Transform for features.\n        target_transform (callable, optional): Transform for targets.\n    \"\"\"\n    # Ensure y is 2D\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    self.X = torch.FloatTensor(X)\n    self.y = torch.FloatTensor(y)\n    self.transform = transform\n    self.target_transform = target_transform\n    self.n_features = self.X.shape[1]\n    self.n_samples = self.X.shape[0]\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.DiabetesDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of samples.</p> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples in the dataset.\n\n    Returns:\n        int: Number of samples.\n    \"\"\"\n    return len(self.X)\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.get_diabetes_dataloaders","title":"<code>get_diabetes_dataloaders(test_size=0.2, batch_size=32, shuffle_train=True, shuffle_test=False, random_state=42, scale_features=True, num_workers=0, pin_memory=False)</code>","text":"<p>Get train and test DataLoaders for the diabetes dataset.</p> <p>Convenience function that loads the diabetes dataset, splits it into train/test, optionally scales features, creates Dataset objects, and returns DataLoaders.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Proportion of dataset to include in test split. Defaults to 0.2 (20%).</p> <code>0.2</code> <code>batch_size</code> <code>int</code> <p>Number of samples per batch. Defaults to 32.</p> <code>32</code> <code>shuffle_train</code> <code>bool</code> <p>Whether to shuffle training data. Defaults to True.</p> <code>True</code> <code>shuffle_test</code> <code>bool</code> <p>Whether to shuffle test data. Defaults to False.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>scale_features</code> <code>bool</code> <p>Whether to standardize features using StandardScaler. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of worker processes for data loading. Defaults to 0 (load in main process).</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>If True, DataLoader will copy tensors into CUDA pinned memory before returning them. Useful when using GPU. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(train_loader, test_loader, scaler) where: - train_loader (DataLoader): DataLoader for training data - test_loader (DataLoader): DataLoader for test data - scaler (StandardScaler or None): Fitted scaler if scale_features=True,   otherwise None</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; from spotoptim.data import get_diabetes_dataloaders\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get dataloaders with default settings\n&gt;&gt;&gt; train_loader, test_loader, scaler = get_diabetes_dataloaders()\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"Training batches: {len(train_loader)}\")\n&gt;&gt;&gt; print(f\"Test batches: {len(test_loader)}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Iterate over batches\n&gt;&gt;&gt; for batch_X, batch_y in train_loader:\n...     print(f\"Batch features shape: {batch_X.shape}\")\n...     print(f\"Batch targets shape: {batch_y.shape}\")\n...     break\n</code></pre> <p>Custom configuration:</p> <pre><code>&gt;&gt;&gt; # Larger batches, no scaling\n&gt;&gt;&gt; train_loader, test_loader, scaler = get_diabetes_dataloaders(\n...     batch_size=64,\n...     scale_features=False,\n...     random_state=123\n... )\n</code></pre> <p>Complete training example:</p> <pre><code>&gt;&gt;&gt; from spotoptim.data import get_diabetes_dataloaders\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get data\n&gt;&gt;&gt; train_loader, test_loader, scaler = get_diabetes_dataloaders(\n...     batch_size=32,\n...     random_state=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; model = LinearRegressor(\n...     input_dim=10,\n...     output_dim=1,\n...     l1=32,\n...     num_hidden_layers=2,\n...     activation=\"ReLU\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get optimizer and loss\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop\n&gt;&gt;&gt; for epoch in range(100):\n...     model.train()\n...     for batch_X, batch_y in train_loader:\n...         optimizer.zero_grad()\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...         loss.backward()\n...         optimizer.step()\n...\n...     # Validation\n...     if (epoch + 1) % 20 == 0:\n...         model.eval()\n...         val_loss = 0.0\n...         with torch.no_grad():\n...             for batch_X, batch_y in test_loader:\n...                 predictions = model(batch_X)\n...                 val_loss += criterion(predictions, batch_y).item()\n...         val_loss /= len(test_loader)\n...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n</code></pre> Note <ul> <li>Features are automatically converted to float32 tensors</li> <li>Targets are reshaped to (n_samples, 1) for compatibility with PyTorch</li> <li>The scaler is fitted only on training data to prevent data leakage</li> <li>Set num_workers &gt; 0 for parallel data loading (may speed up training)</li> </ul> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>def get_diabetes_dataloaders(\n    test_size: float = 0.2,\n    batch_size: int = 32,\n    shuffle_train: bool = True,\n    shuffle_test: bool = False,\n    random_state: int = 42,\n    scale_features: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n) -&gt; tuple:\n    \"\"\"Get train and test DataLoaders for the diabetes dataset.\n\n    Convenience function that loads the diabetes dataset, splits it into train/test,\n    optionally scales features, creates Dataset objects, and returns DataLoaders.\n\n    Args:\n        test_size (float, optional): Proportion of dataset to include in test split.\n            Defaults to 0.2 (20%).\n        batch_size (int, optional): Number of samples per batch. Defaults to 32.\n        shuffle_train (bool, optional): Whether to shuffle training data. Defaults to True.\n        shuffle_test (bool, optional): Whether to shuffle test data. Defaults to False.\n        random_state (int, optional): Random seed for reproducibility. Defaults to 42.\n        scale_features (bool, optional): Whether to standardize features using\n            StandardScaler. Defaults to True.\n        num_workers (int, optional): Number of worker processes for data loading.\n            Defaults to 0 (load in main process).\n        pin_memory (bool, optional): If True, DataLoader will copy tensors into\n            CUDA pinned memory before returning them. Useful when using GPU.\n            Defaults to False.\n\n    Returns:\n        tuple: (train_loader, test_loader, scaler) where:\n            - train_loader (DataLoader): DataLoader for training data\n            - test_loader (DataLoader): DataLoader for test data\n            - scaler (StandardScaler or None): Fitted scaler if scale_features=True,\n              otherwise None\n\n    Examples:\n        Basic usage:\n\n        &gt;&gt;&gt; from spotoptim.data import get_diabetes_dataloaders\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get dataloaders with default settings\n        &gt;&gt;&gt; train_loader, test_loader, scaler = get_diabetes_dataloaders()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; print(f\"Training batches: {len(train_loader)}\")\n        &gt;&gt;&gt; print(f\"Test batches: {len(test_loader)}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Iterate over batches\n        &gt;&gt;&gt; for batch_X, batch_y in train_loader:\n        ...     print(f\"Batch features shape: {batch_X.shape}\")\n        ...     print(f\"Batch targets shape: {batch_y.shape}\")\n        ...     break\n\n        Custom configuration:\n\n        &gt;&gt;&gt; # Larger batches, no scaling\n        &gt;&gt;&gt; train_loader, test_loader, scaler = get_diabetes_dataloaders(\n        ...     batch_size=64,\n        ...     scale_features=False,\n        ...     random_state=123\n        ... )\n\n        Complete training example:\n\n        &gt;&gt;&gt; from spotoptim.data import get_diabetes_dataloaders\n        &gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get data\n        &gt;&gt;&gt; train_loader, test_loader, scaler = get_diabetes_dataloaders(\n        ...     batch_size=32,\n        ...     random_state=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model\n        &gt;&gt;&gt; model = LinearRegressor(\n        ...     input_dim=10,\n        ...     output_dim=1,\n        ...     l1=32,\n        ...     num_hidden_layers=2,\n        ...     activation=\"ReLU\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get optimizer and loss\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training loop\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     model.train()\n        ...     for batch_X, batch_y in train_loader:\n        ...         optimizer.zero_grad()\n        ...         predictions = model(batch_X)\n        ...         loss = criterion(predictions, batch_y)\n        ...         loss.backward()\n        ...         optimizer.step()\n        ...\n        ...     # Validation\n        ...     if (epoch + 1) % 20 == 0:\n        ...         model.eval()\n        ...         val_loss = 0.0\n        ...         with torch.no_grad():\n        ...             for batch_X, batch_y in test_loader:\n        ...                 predictions = model(batch_X)\n        ...                 val_loss += criterion(predictions, batch_y).item()\n        ...         val_loss /= len(test_loader)\n        ...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n\n    Note:\n        - Features are automatically converted to float32 tensors\n        - Targets are reshaped to (n_samples, 1) for compatibility with PyTorch\n        - The scaler is fitted only on training data to prevent data leakage\n        - Set num_workers &gt; 0 for parallel data loading (may speed up training)\n    \"\"\"\n    # Load diabetes dataset\n    diabetes = load_diabetes()\n    X, y = diabetes.data, diabetes.target\n\n    # Split into train and test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    # Optionally scale features\n    scaler = None\n    if scale_features:\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n    # Ensure targets are 2D\n    y_train = y_train.reshape(-1, 1)\n    y_test = y_test.reshape(-1, 1)\n\n    # Create datasets\n    train_dataset = DiabetesDataset(X_train, y_train)\n    test_dataset = DiabetesDataset(X_test, y_test)\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=shuffle_train,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=shuffle_test,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    return train_loader, test_loader, scaler\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/","title":"linear_regressor","text":""},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor","title":"<code>LinearRegressor</code>","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch neural network for regression with configurable architecture.</p> <p>A flexible regression model that supports: - Pure linear regression (no hidden layers) - Deep neural networks with multiple hidden layers - Various activation functions (ReLU, Tanh, Sigmoid, etc.) - Easy optimizer selection (Adam, SGD, RMSprop, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>output_dim</code> <code>int</code> <p>Number of output features/targets.</p> required <code>l1</code> <code>int</code> <p>Number of neurons in each hidden layer. Defaults to 64.</p> <code>64</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers. Set to 0 for pure linear regression. Defaults to 0.</p> <code>0</code> <code>activation</code> <code>str</code> <p>Name of activation function from torch.nn to use between layers. Common options: \u201cReLU\u201d, \u201cSigmoid\u201d, \u201cTanh\u201d, \u201cLeakyReLU\u201d, \u201cELU\u201d, \u201cSELU\u201d, \u201cGELU\u201d, \u201cSoftplus\u201d, \u201cSoftsign\u201d, \u201cMish\u201d. Defaults to \u201cReLU\u201d.</p> <code>'ReLU'</code> <code>optimizer</code> <code>str</code> <p>Name of the optimizer to use. Common options: \u201cAdam\u201d, \u201cSGD\u201d, \u201cRMSprop\u201d. Defaults to \u201cAdam\u201d.</p> required <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>Number of input features.</p> <code>output_dim</code> <code>int</code> <p>Number of output features.</p> <code>l1</code> <code>int</code> <p>Number of neurons per hidden layer.</p> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers in the network.</p> <code>activation_name</code> <code>str</code> <p>Name of the activation function.</p> <code>activation</code> <code>Module</code> <p>Instance of the activation function.</p> <code>network</code> <code>Sequential</code> <p>The complete neural network architecture.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified activation function is not found in torch.nn.</p> <p>Examples:</p> <p>Basic usage with pure linear regression:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Pure linear regression (no hidden layers)\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt; x = torch.randn(32, 10)  # Batch of 32 samples\n&gt;&gt;&gt; y_pred = model(x)\n&gt;&gt;&gt; print(y_pred.shape)\ntorch.Size([32, 1])\n</code></pre> <p>Single hidden layer with custom neurons:</p> <pre><code>&gt;&gt;&gt; # Single hidden layer with 64 neurons and ReLU activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=64, num_hidden_layers=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.001)\n</code></pre> <p>Deep network with custom activation:</p> <pre><code>&gt;&gt;&gt; # Three hidden layers with 128 neurons each and Tanh activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=128,\n...                         num_hidden_layers=3, activation=\"Tanh\")\n</code></pre> <p>Complete example using diabetes dataset:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Split and scale data\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to PyTorch tensors\n&gt;&gt;&gt; X_train = torch.FloatTensor(X_train)\n&gt;&gt;&gt; y_train = torch.FloatTensor(y_train)\n&gt;&gt;&gt; X_test = torch.FloatTensor(X_test)\n&gt;&gt;&gt; y_test = torch.FloatTensor(y_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model with 2 hidden layers\n&gt;&gt;&gt; model = LinearRegressor(\n...     input_dim=10,  # diabetes dataset has 10 features\n...     output_dim=1,\n...     l1=32,\n...     num_hidden_layers=2,\n...     activation=\"ReLU\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get optimizer and loss function\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop\n&gt;&gt;&gt; for epoch in range(100):\n...     # Forward pass\n...     y_pred = model(X_train)\n...     loss = criterion(y_pred, y_train)\n...\n...     # Backward pass and optimization\n...     optimizer.zero_grad()\n...     loss.backward()\n...     optimizer.step()\n...\n...     if (epoch + 1) % 20 == 0:\n...         print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate on test set\n&gt;&gt;&gt; model.eval()\n&gt;&gt;&gt; with torch.no_grad():\n...     y_pred = model(X_test)\n...     test_loss = criterion(y_pred, y_test)\n...     print(f'Test Loss: {test_loss.item():.4f}')\n</code></pre> <p>Using PyTorch Dataset and DataLoader (recommended for larger datasets):</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom Dataset class for diabetes data\n&gt;&gt;&gt; class DiabetesDataset(Dataset):\n...     def __init__(self, X, y):\n...         self.X = torch.FloatTensor(X)\n...         self.y = torch.FloatTensor(y)\n...\n...     def __len__(self):\n...         return len(self.X)\n...\n...     def __getitem__(self, idx):\n...         return self.X[idx], self.y[idx]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Scale data\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Dataset and DataLoader\n&gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n&gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n&gt;&gt;&gt; train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n&gt;&gt;&gt; test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=32,\n...                         num_hidden_layers=2, activation=\"ReLU\")\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop with DataLoader\n&gt;&gt;&gt; for epoch in range(100):\n...     model.train()\n...     for batch_X, batch_y in train_loader:\n...         # Forward pass\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...\n...         # Backward pass\n...         optimizer.zero_grad()\n...         loss.backward()\n...         optimizer.step()\n...\n...     # Validation\n...     if (epoch + 1) % 20 == 0:\n...         model.eval()\n...         val_loss = 0.0\n...         with torch.no_grad():\n...             for batch_X, batch_y in test_loader:\n...                 predictions = model(batch_X)\n...                 val_loss += criterion(predictions, batch_y).item()\n...         val_loss /= len(test_loader)\n...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n</code></pre> Note <ul> <li>When num_hidden_layers=0, the model performs pure linear regression</li> <li>Activation functions are only applied between hidden layers, not on output</li> <li>Use get_optimizer() method for convenient optimizer instantiation</li> <li>For large datasets, use PyTorch Dataset and DataLoader for efficient batch processing</li> </ul> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>class LinearRegressor(nn.Module):\n    \"\"\"PyTorch neural network for regression with configurable architecture.\n\n    A flexible regression model that supports:\n    - Pure linear regression (no hidden layers)\n    - Deep neural networks with multiple hidden layers\n    - Various activation functions (ReLU, Tanh, Sigmoid, etc.)\n    - Easy optimizer selection (Adam, SGD, RMSprop, etc.)\n\n    Args:\n        input_dim (int): Number of input features.\n        output_dim (int): Number of output features/targets.\n        l1 (int, optional): Number of neurons in each hidden layer. Defaults to 64.\n        num_hidden_layers (int, optional): Number of hidden layers. Set to 0 for pure\n            linear regression. Defaults to 0.\n        activation (str, optional): Name of activation function from torch.nn to use\n            between layers. Common options: \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\",\n            \"ELU\", \"SELU\", \"GELU\", \"Softplus\", \"Softsign\", \"Mish\". Defaults to \"ReLU\".\n        optimizer (str, optional): Name of the optimizer to use. Common options: \"Adam\", \"SGD\", \"RMSprop\". Defaults to \"Adam\".\n\n\n    Attributes:\n        input_dim (int): Number of input features.\n        output_dim (int): Number of output features.\n        l1 (int): Number of neurons per hidden layer.\n        num_hidden_layers (int): Number of hidden layers in the network.\n        activation_name (str): Name of the activation function.\n        activation (nn.Module): Instance of the activation function.\n        network (nn.Sequential): The complete neural network architecture.\n\n\n    Raises:\n        ValueError: If the specified activation function is not found in torch.nn.\n\n    Examples:\n        Basic usage with pure linear regression:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Pure linear regression (no hidden layers)\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n        &gt;&gt;&gt; x = torch.randn(32, 10)  # Batch of 32 samples\n        &gt;&gt;&gt; y_pred = model(x)\n        &gt;&gt;&gt; print(y_pred.shape)\n        torch.Size([32, 1])\n\n        Single hidden layer with custom neurons:\n\n        &gt;&gt;&gt; # Single hidden layer with 64 neurons and ReLU activation\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=64, num_hidden_layers=1)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.001)\n\n        Deep network with custom activation:\n\n        &gt;&gt;&gt; # Three hidden layers with 128 neurons each and Tanh activation\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=128,\n        ...                         num_hidden_layers=3, activation=\"Tanh\")\n\n        Complete example using diabetes dataset:\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Split and scale data\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, test_size=0.2, random_state=42\n        ... )\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n        &gt;&gt;&gt; X_test = scaler.transform(X_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert to PyTorch tensors\n        &gt;&gt;&gt; X_train = torch.FloatTensor(X_train)\n        &gt;&gt;&gt; y_train = torch.FloatTensor(y_train)\n        &gt;&gt;&gt; X_test = torch.FloatTensor(X_test)\n        &gt;&gt;&gt; y_test = torch.FloatTensor(y_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model with 2 hidden layers\n        &gt;&gt;&gt; model = LinearRegressor(\n        ...     input_dim=10,  # diabetes dataset has 10 features\n        ...     output_dim=1,\n        ...     l1=32,\n        ...     num_hidden_layers=2,\n        ...     activation=\"ReLU\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get optimizer and loss function\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training loop\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     # Forward pass\n        ...     y_pred = model(X_train)\n        ...     loss = criterion(y_pred, y_train)\n        ...\n        ...     # Backward pass and optimization\n        ...     optimizer.zero_grad()\n        ...     loss.backward()\n        ...     optimizer.step()\n        ...\n        ...     if (epoch + 1) % 20 == 0:\n        ...         print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Evaluate on test set\n        &gt;&gt;&gt; model.eval()\n        &gt;&gt;&gt; with torch.no_grad():\n        ...     y_pred = model(X_test)\n        ...     test_loss = criterion(y_pred, y_test)\n        ...     print(f'Test Loss: {test_loss.item():.4f}')\n\n        Using PyTorch Dataset and DataLoader (recommended for larger datasets):\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Custom Dataset class for diabetes data\n        &gt;&gt;&gt; class DiabetesDataset(Dataset):\n        ...     def __init__(self, X, y):\n        ...         self.X = torch.FloatTensor(X)\n        ...         self.y = torch.FloatTensor(y)\n        ...\n        ...     def __len__(self):\n        ...         return len(self.X)\n        ...\n        ...     def __getitem__(self, idx):\n        ...         return self.X[idx], self.y[idx]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, test_size=0.2, random_state=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Scale data\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n        &gt;&gt;&gt; X_test = scaler.transform(X_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create Dataset and DataLoader\n        &gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n        &gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n        &gt;&gt;&gt; train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n        &gt;&gt;&gt; test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=32,\n        ...                         num_hidden_layers=2, activation=\"ReLU\")\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training loop with DataLoader\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     model.train()\n        ...     for batch_X, batch_y in train_loader:\n        ...         # Forward pass\n        ...         predictions = model(batch_X)\n        ...         loss = criterion(predictions, batch_y)\n        ...\n        ...         # Backward pass\n        ...         optimizer.zero_grad()\n        ...         loss.backward()\n        ...         optimizer.step()\n        ...\n        ...     # Validation\n        ...     if (epoch + 1) % 20 == 0:\n        ...         model.eval()\n        ...         val_loss = 0.0\n        ...         with torch.no_grad():\n        ...             for batch_X, batch_y in test_loader:\n        ...                 predictions = model(batch_X)\n        ...                 val_loss += criterion(predictions, batch_y).item()\n        ...         val_loss /= len(test_loader)\n        ...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n\n    Note:\n        - When num_hidden_layers=0, the model performs pure linear regression\n        - Activation functions are only applied between hidden layers, not on output\n        - Use get_optimizer() method for convenient optimizer instantiation\n        - For large datasets, use PyTorch Dataset and DataLoader for efficient batch processing\n    \"\"\"\n\n    def __init__(\n        self, input_dim, output_dim, l1=64, num_hidden_layers=0, activation=\"ReLU\"\n    ):\n        super(LinearRegressor, self).__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.l1 = l1\n        self.num_hidden_layers = num_hidden_layers\n        self.activation_name = activation\n\n        # Get activation function class from string\n        if hasattr(nn, activation):\n            activation_class = getattr(nn, activation)\n            self.activation = activation_class()\n        else:\n            raise ValueError(\n                f\"Activation function '{activation}' not found in torch.nn. \"\n                f\"Please use a valid PyTorch activation function name like 'ReLU', 'Sigmoid', 'Tanh', etc.\"\n            )\n\n        # Build the network layers\n        layers = []\n\n        if num_hidden_layers == 0:\n            # Pure linear regression (no hidden layers)\n            layers.append(nn.Linear(input_dim, output_dim))\n        else:\n            # Input layer to first hidden layer\n            layers.append(nn.Linear(input_dim, l1))\n            layers.append(self.activation)\n\n            # Additional hidden layers\n            for _ in range(num_hidden_layers - 1):\n                layers.append(nn.Linear(l1, l1))\n                # Create a new instance of the activation for each layer\n                layers.append(getattr(nn, self.activation_name)())\n\n            # Final hidden layer to output\n            layers.append(nn.Linear(l1, output_dim))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"Forward pass through the network.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            torch.Tensor: Output predictions of shape (batch_size, output_dim).\n\n        Examples:\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=5, output_dim=1)\n            &gt;&gt;&gt; x = torch.randn(10, 5)  # 10 samples, 5 features\n            &gt;&gt;&gt; output = model(x)\n            &gt;&gt;&gt; print(output.shape)\n            torch.Size([10, 1])\n        \"\"\"\n        return self.network(x)\n\n    def get_optimizer(self, optimizer_name=\"Adam\", lr=0.001, **kwargs):\n        \"\"\"Get a PyTorch optimizer configured for this model.\n\n        Convenience method to instantiate optimizers using string names instead of\n        importing optimizer classes. Automatically configures the optimizer with the\n        model's parameters.\n\n        Args:\n            optimizer_name (str, optional): Name of the optimizer from torch.optim.\n                Common options: \"Adam\", \"AdamW\", \"Adamax\", \"SGD\", \"RMSprop\", \"Adagrad\",\n                \"Adadelta\", \"NAdam\", \"RAdam\", \"ASGD\", \"LBFGS\", \"Rprop\".\n                Defaults to \"Adam\".\n            lr (float, optional): Learning rate for the optimizer. Defaults to 0.001.\n            **kwargs: Additional optimizer-specific parameters (e.g., momentum for SGD,\n                weight_decay for AdamW, alpha for RMSprop).\n\n        Returns:\n            torch.optim.Optimizer: Configured optimizer instance ready for training.\n\n        Raises:\n            ValueError: If the specified optimizer name is not found in torch.optim.\n\n        Examples:\n            Basic usage with default Adam:\n\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n            &gt;&gt;&gt; optimizer = model.get_optimizer()  # Uses Adam with lr=0.001\n\n            SGD with momentum:\n\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.01, momentum=0.9)\n\n            AdamW with weight decay for regularization:\n\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"AdamW\", lr=0.001, weight_decay=0.01)\n\n            RMSprop with custom alpha:\n\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"RMSprop\", lr=0.01, alpha=0.99)\n\n            Complete training example with diabetes dataset:\n\n            &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Prepare data\n            &gt;&gt;&gt; diabetes = load_diabetes()\n            &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n            &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n            &gt;&gt;&gt; X_tensor = torch.FloatTensor(X)\n            &gt;&gt;&gt; y_tensor = torch.FloatTensor(y)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create model and optimizer\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16, num_hidden_layers=1)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n            &gt;&gt;&gt; criterion = nn.MSELoss()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Training\n            &gt;&gt;&gt; for epoch in range(50):\n            ...     optimizer.zero_grad()\n            ...     predictions = model(X_tensor)\n            ...     loss = criterion(predictions, y_tensor)\n            ...     loss.backward()\n            ...     optimizer.step()\n\n            Using with DataLoader for mini-batch training:\n\n            &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n            &gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Custom Dataset\n            &gt;&gt;&gt; class DiabetesDataset(Dataset):\n            ...     def __init__(self, X, y):\n            ...         self.X = torch.FloatTensor(X)\n            ...         self.y = torch.FloatTensor(y)\n            ...\n            ...     def __len__(self):\n            ...         return len(self.X)\n            ...\n            ...     def __getitem__(self, idx):\n            ...         return self.X[idx], self.y[idx]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Prepare data\n            &gt;&gt;&gt; diabetes = load_diabetes()\n            &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n            &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create DataLoader\n            &gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n            &gt;&gt;&gt; dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create model and optimizer\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16, num_hidden_layers=1)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.01, momentum=0.9)\n            &gt;&gt;&gt; criterion = nn.MSELoss()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Training with mini-batches\n            &gt;&gt;&gt; for epoch in range(100):\n            ...     for batch_X, batch_y in dataloader:\n            ...         optimizer.zero_grad()\n            ...         predictions = model(batch_X)\n            ...         loss = criterion(predictions, batch_y)\n            ...         loss.backward()\n            ...         optimizer.step()\n\n        Note:\n            The optimizer is automatically initialized with self.parameters(), so you\n            don't need to manually pass the model parameters. Using DataLoader enables\n            efficient mini-batch training, shuffling, and parallel data loading.\n        \"\"\"\n        # Check if optimizer exists in torch.optim\n        if hasattr(optim, optimizer_name):\n            optimizer_class = getattr(optim, optimizer_name)\n            # Create optimizer with model parameters, learning rate, and additional kwargs\n            return optimizer_class(self.parameters(), lr=lr, **kwargs)\n        else:\n            raise ValueError(\n                f\"Optimizer '{optimizer_name}' not found in torch.optim. \"\n                f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n            )\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output predictions of shape (batch_size, output_dim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = LinearRegressor(input_dim=5, output_dim=1)\n&gt;&gt;&gt; x = torch.randn(10, 5)  # 10 samples, 5 features\n&gt;&gt;&gt; output = model(x)\n&gt;&gt;&gt; print(output.shape)\ntorch.Size([10, 1])\n</code></pre> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the network.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n    Returns:\n        torch.Tensor: Output predictions of shape (batch_size, output_dim).\n\n    Examples:\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=5, output_dim=1)\n        &gt;&gt;&gt; x = torch.randn(10, 5)  # 10 samples, 5 features\n        &gt;&gt;&gt; output = model(x)\n        &gt;&gt;&gt; print(output.shape)\n        torch.Size([10, 1])\n    \"\"\"\n    return self.network(x)\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor.get_optimizer","title":"<code>get_optimizer(optimizer_name='Adam', lr=0.001, **kwargs)</code>","text":"<p>Get a PyTorch optimizer configured for this model.</p> <p>Convenience method to instantiate optimizers using string names instead of importing optimizer classes. Automatically configures the optimizer with the model\u2019s parameters.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_name</code> <code>str</code> <p>Name of the optimizer from torch.optim. Common options: \u201cAdam\u201d, \u201cAdamW\u201d, \u201cAdamax\u201d, \u201cSGD\u201d, \u201cRMSprop\u201d, \u201cAdagrad\u201d, \u201cAdadelta\u201d, \u201cNAdam\u201d, \u201cRAdam\u201d, \u201cASGD\u201d, \u201cLBFGS\u201d, \u201cRprop\u201d. Defaults to \u201cAdam\u201d.</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer. Defaults to 0.001.</p> <code>0.001</code> <code>**kwargs</code> <p>Additional optimizer-specific parameters (e.g., momentum for SGD, weight_decay for AdamW, alpha for RMSprop).</p> <code>{}</code> <p>Returns:</p> Type Description <p>torch.optim.Optimizer: Configured optimizer instance ready for training.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified optimizer name is not found in torch.optim.</p> <p>Examples:</p> <p>Basic usage with default Adam:</p> <pre><code>&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer()  # Uses Adam with lr=0.001\n</code></pre> <p>SGD with momentum:</p> <pre><code>&gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.01, momentum=0.9)\n</code></pre> <p>AdamW with weight decay for regularization:</p> <pre><code>&gt;&gt;&gt; optimizer = model.get_optimizer(\"AdamW\", lr=0.001, weight_decay=0.01)\n</code></pre> <p>RMSprop with custom alpha:</p> <pre><code>&gt;&gt;&gt; optimizer = model.get_optimizer(\"RMSprop\", lr=0.01, alpha=0.99)\n</code></pre> <p>Complete training example with diabetes dataset:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n&gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_tensor = torch.FloatTensor(X)\n&gt;&gt;&gt; y_tensor = torch.FloatTensor(y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model and optimizer\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16, num_hidden_layers=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training\n&gt;&gt;&gt; for epoch in range(50):\n...     optimizer.zero_grad()\n...     predictions = model(X_tensor)\n...     loss = criterion(predictions, y_tensor)\n...     loss.backward()\n...     optimizer.step()\n</code></pre> <p>Using with DataLoader for mini-batch training:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom Dataset\n&gt;&gt;&gt; class DiabetesDataset(Dataset):\n...     def __init__(self, X, y):\n...         self.X = torch.FloatTensor(X)\n...         self.y = torch.FloatTensor(y)\n...\n...     def __len__(self):\n...         return len(self.X)\n...\n...     def __getitem__(self, idx):\n...         return self.X[idx], self.y[idx]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n&gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create DataLoader\n&gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n&gt;&gt;&gt; dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model and optimizer\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16, num_hidden_layers=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.01, momentum=0.9)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training with mini-batches\n&gt;&gt;&gt; for epoch in range(100):\n...     for batch_X, batch_y in dataloader:\n...         optimizer.zero_grad()\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...         loss.backward()\n...         optimizer.step()\n</code></pre> Note <p>The optimizer is automatically initialized with self.parameters(), so you don\u2019t need to manually pass the model parameters. Using DataLoader enables efficient mini-batch training, shuffling, and parallel data loading.</p> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>def get_optimizer(self, optimizer_name=\"Adam\", lr=0.001, **kwargs):\n    \"\"\"Get a PyTorch optimizer configured for this model.\n\n    Convenience method to instantiate optimizers using string names instead of\n    importing optimizer classes. Automatically configures the optimizer with the\n    model's parameters.\n\n    Args:\n        optimizer_name (str, optional): Name of the optimizer from torch.optim.\n            Common options: \"Adam\", \"AdamW\", \"Adamax\", \"SGD\", \"RMSprop\", \"Adagrad\",\n            \"Adadelta\", \"NAdam\", \"RAdam\", \"ASGD\", \"LBFGS\", \"Rprop\".\n            Defaults to \"Adam\".\n        lr (float, optional): Learning rate for the optimizer. Defaults to 0.001.\n        **kwargs: Additional optimizer-specific parameters (e.g., momentum for SGD,\n            weight_decay for AdamW, alpha for RMSprop).\n\n    Returns:\n        torch.optim.Optimizer: Configured optimizer instance ready for training.\n\n    Raises:\n        ValueError: If the specified optimizer name is not found in torch.optim.\n\n    Examples:\n        Basic usage with default Adam:\n\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n        &gt;&gt;&gt; optimizer = model.get_optimizer()  # Uses Adam with lr=0.001\n\n        SGD with momentum:\n\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.01, momentum=0.9)\n\n        AdamW with weight decay for regularization:\n\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"AdamW\", lr=0.001, weight_decay=0.01)\n\n        RMSprop with custom alpha:\n\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"RMSprop\", lr=0.01, alpha=0.99)\n\n        Complete training example with diabetes dataset:\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n        &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt; X_tensor = torch.FloatTensor(X)\n        &gt;&gt;&gt; y_tensor = torch.FloatTensor(y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model and optimizer\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16, num_hidden_layers=1)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training\n        &gt;&gt;&gt; for epoch in range(50):\n        ...     optimizer.zero_grad()\n        ...     predictions = model(X_tensor)\n        ...     loss = criterion(predictions, y_tensor)\n        ...     loss.backward()\n        ...     optimizer.step()\n\n        Using with DataLoader for mini-batch training:\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Custom Dataset\n        &gt;&gt;&gt; class DiabetesDataset(Dataset):\n        ...     def __init__(self, X, y):\n        ...         self.X = torch.FloatTensor(X)\n        ...         self.y = torch.FloatTensor(y)\n        ...\n        ...     def __len__(self):\n        ...         return len(self.X)\n        ...\n        ...     def __getitem__(self, idx):\n        ...         return self.X[idx], self.y[idx]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n        &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create DataLoader\n        &gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n        &gt;&gt;&gt; dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model and optimizer\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16, num_hidden_layers=1)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.01, momentum=0.9)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training with mini-batches\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     for batch_X, batch_y in dataloader:\n        ...         optimizer.zero_grad()\n        ...         predictions = model(batch_X)\n        ...         loss = criterion(predictions, batch_y)\n        ...         loss.backward()\n        ...         optimizer.step()\n\n    Note:\n        The optimizer is automatically initialized with self.parameters(), so you\n        don't need to manually pass the model parameters. Using DataLoader enables\n        efficient mini-batch training, shuffling, and parallel data loading.\n    \"\"\"\n    # Check if optimizer exists in torch.optim\n    if hasattr(optim, optimizer_name):\n        optimizer_class = getattr(optim, optimizer_name)\n        # Create optimizer with model parameters, learning rate, and additional kwargs\n        return optimizer_class(self.parameters(), lr=lr, **kwargs)\n    else:\n        raise ValueError(\n            f\"Optimizer '{optimizer_name}' not found in torch.optim. \"\n            f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n        )\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/","title":"kriging","text":"<p>Simplified Kriging surrogate model for SpotOptim.</p> <p>This is a streamlined version adapted from spotpython.surrogate.kriging for use with the SpotOptim optimizer.</p>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging","title":"<code>Kriging</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A simplified Kriging (Gaussian Process) surrogate model for SpotOptim.</p> <p>This class provides a scikit-learn compatible interface with fit() and predict() methods, making it suitable for use as a surrogate in SpotOptim.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>float</code> <p>Regularization parameter (nugget effect). If None, uses sqrt(eps). Defaults to None.</p> <code>None</code> <code>kernel</code> <code>str</code> <p>Kernel type. Currently only \u2018gauss\u2019 (Gaussian/RBF) is supported. Defaults to \u2018gauss\u2019.</p> <code>'gauss'</code> <code>n_theta</code> <code>int</code> <p>Number of theta parameters. If None, uses k (number of dimensions). Defaults to None.</p> <code>None</code> <code>min_theta</code> <code>float</code> <p>Minimum log10(theta) bound for optimization. Defaults to -3.0.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>Maximum log10(theta) bound for optimization. Defaults to 2.0.</p> <code>2.0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>X_</code> <code>ndarray</code> <p>Training data, shape (n_samples, n_features).</p> <code>y_</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> <code>theta_</code> <code>ndarray</code> <p>Optimized theta parameters (log10 scale).</p> <code>mu_</code> <code>float</code> <p>Mean of the Kriging predictor.</p> <code>sigma2_</code> <code>float</code> <p>Variance of the Kriging predictor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; X = np.array([[0.0], [0.5], [1.0]])\n&gt;&gt;&gt; y = np.array([0.0, 0.25, 1.0])\n&gt;&gt;&gt; model = Kriging()\n&gt;&gt;&gt; model.fit(X, y)\n&gt;&gt;&gt; predictions = model.predict(np.array([[0.25], [0.75]]))\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>class Kriging(BaseEstimator, RegressorMixin):\n    \"\"\"A simplified Kriging (Gaussian Process) surrogate model for SpotOptim.\n\n    This class provides a scikit-learn compatible interface with fit() and predict()\n    methods, making it suitable for use as a surrogate in SpotOptim.\n\n    Args:\n        noise (float, optional): Regularization parameter (nugget effect). If None, uses sqrt(eps).\n            Defaults to None.\n        kernel (str, optional): Kernel type. Currently only 'gauss' (Gaussian/RBF) is supported.\n            Defaults to 'gauss'.\n        n_theta (int, optional): Number of theta parameters. If None, uses k (number of dimensions).\n            Defaults to None.\n        min_theta (float, optional): Minimum log10(theta) bound for optimization. Defaults to -3.0.\n        max_theta (float, optional): Maximum log10(theta) bound for optimization. Defaults to 2.0.\n        seed (int, optional): Random seed for reproducibility. Defaults to None.\n\n    Attributes:\n        X_ (ndarray): Training data, shape (n_samples, n_features).\n        y_ (ndarray): Training targets, shape (n_samples,).\n        theta_ (ndarray): Optimized theta parameters (log10 scale).\n        mu_ (float): Mean of the Kriging predictor.\n        sigma2_ (float): Variance of the Kriging predictor.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; X = np.array([[0.0], [0.5], [1.0]])\n        &gt;&gt;&gt; y = np.array([0.0, 0.25, 1.0])\n        &gt;&gt;&gt; model = Kriging()\n        &gt;&gt;&gt; model.fit(X, y)\n        &gt;&gt;&gt; predictions = model.predict(np.array([[0.25], [0.75]]))\n    \"\"\"\n\n    def __init__(\n        self,\n        noise: Optional[float] = None,\n        kernel: str = \"gauss\",\n        n_theta: Optional[int] = None,\n        min_theta: float = -3.0,\n        max_theta: float = 2.0,\n        seed: Optional[int] = None,\n    ):\n        self.noise = noise\n        self.kernel = kernel\n        self.n_theta = n_theta\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.seed = seed\n\n        # Fitted attributes\n        self.X_ = None\n        self.y_ = None\n        self.theta_ = None\n        self.mu_ = None\n        self.sigma2_ = None\n        self.U_ = None  # Cholesky factor\n        self.Rinv_one_ = None\n        self.Rinv_r_ = None\n\n    def _get_noise(self) -&gt; float:\n        \"\"\"Get the noise/regularization parameter.\n\n        Returns:\n            float: Noise/regularization value.\n        \"\"\"\n        if self.noise is None:\n            return np.sqrt(np.finfo(float).eps)\n        return self.noise\n\n    def _correlation(self, D: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute correlation from distance matrix using Gaussian kernel.\n\n        Args:\n            D (ndarray): Squared distance matrix.\n\n        Returns:\n            ndarray: Correlation matrix.\n        \"\"\"\n        if self.kernel == \"gauss\":\n            return np.exp(-D)\n        else:\n            raise ValueError(f\"Unsupported kernel: {self.kernel}\")\n\n    def _build_correlation_matrix(self, X: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build correlation matrix R for training data.\n\n        Args:\n            X (ndarray): Input data, shape (n, k).\n            theta (ndarray): Theta parameters (10^theta used as weights), shape (k,).\n\n        Returns:\n            ndarray: Correlation matrix with noise on diagonal, shape (n, n).\n        \"\"\"\n        n = X.shape[0]\n        theta10 = 10.0**theta\n\n        # Compute weighted squared distances\n        R = np.zeros((n, n))\n        for i in range(n):\n            for j in range(i + 1, n):\n                diff = X[i] - X[j]\n                dist = np.sum(theta10 * diff**2)\n                R[i, j] = dist\n                R[j, i] = dist\n\n        # Apply correlation function\n        R = self._correlation(R)\n\n        # Add noise to diagonal\n        noise_val = self._get_noise()\n        np.fill_diagonal(R, 1.0 + noise_val)\n\n        return R\n\n    def _build_correlation_vector(\n        self, x: np.ndarray, X: np.ndarray, theta: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Build correlation vector between new point x and training data X.\n\n        Args:\n            x (ndarray): New point, shape (k,).\n            X (ndarray): Training data, shape (n, k).\n            theta (ndarray): Theta parameters, shape (k,).\n\n        Returns:\n            ndarray: Correlation vector, shape (n,).\n        \"\"\"\n        theta10 = 10.0**theta\n        diff = X - x.reshape(1, -1)\n        D = np.sum(theta10 * diff**2, axis=1)\n        return self._correlation(D)\n\n    def _neg_log_likelihood(self, log_theta: np.ndarray) -&gt; float:\n        \"\"\"Compute negative concentrated log-likelihood.\n\n        Args:\n            log_theta (ndarray): Log10(theta) parameters.\n\n        Returns:\n            float: Negative log-likelihood (to be minimized).\n        \"\"\"\n        try:\n            n = self.X_.shape[0]\n            y = self.y_.flatten()\n            one = np.ones(n)\n\n            # Build correlation matrix\n            R = self._build_correlation_matrix(self.X_, log_theta)\n\n            # Cholesky decomposition\n            try:\n                U = cholesky(R)\n            except LinAlgError:\n                return 1e10  # Penalty for ill-conditioned matrix\n\n            # Solve for mean and variance\n            Uy = solve(U, y)\n            Uone = solve(U, one)\n\n            Rinv_y = solve(U.T, Uy)\n            Rinv_one = solve(U.T, Uone)\n\n            mu = (one @ Rinv_y) / (one @ Rinv_one)\n            r = y - one * mu\n\n            Ur = solve(U, r)\n            Rinv_r = solve(U.T, Ur)\n\n            sigma2 = (r @ Rinv_r) / n\n\n            if sigma2 &lt;= 0:\n                return 1e10\n\n            # Concentrated log-likelihood\n            log_det_R = 2.0 * np.sum(np.log(np.abs(np.diag(U))))\n            neg_log_like = (n / 2.0) * np.log(sigma2) + 0.5 * log_det_R\n\n            return neg_log_like\n\n        except (LinAlgError, ValueError):\n            return 1e10\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"Kriging\":\n        \"\"\"Fit the Kriging model to training data.\n\n        Args:\n            X (ndarray): Training input data, shape (n_samples, n_features).\n            y (ndarray): Training target values, shape (n_samples,).\n\n        Returns:\n            Kriging: Fitted estimator (self).\n        \"\"\"\n        X = np.atleast_2d(X)\n        y = np.asarray(y).flatten()\n\n        if X.ndim != 2:\n            raise ValueError(f\"X must be 2-dimensional, got shape {X.shape}\")\n        if y.ndim != 1:\n            raise ValueError(f\"y must be 1-dimensional, got shape {y.shape}\")\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(f\"X and y must have same number of samples\")\n\n        self.X_ = X\n        self.y_ = y\n        n, k = X.shape\n\n        # Set number of theta parameters\n        if self.n_theta is None:\n            self.n_theta = k\n\n        # Optimize theta via maximum likelihood\n        bounds = [(self.min_theta, self.max_theta)] * self.n_theta\n\n        result = differential_evolution(\n            func=self._neg_log_likelihood,\n            bounds=bounds,\n            seed=self.seed,\n            maxiter=100,\n            atol=1e-6,\n            tol=0.01,\n        )\n\n        self.theta_ = result.x\n\n        # Compute final model parameters\n        one = np.ones(n)\n        R = self._build_correlation_matrix(X, self.theta_)\n\n        try:\n            self.U_ = cholesky(R)\n        except LinAlgError:\n            # Add more regularization if needed\n            R = self._build_correlation_matrix(X, self.theta_)\n            R += np.eye(n) * 1e-8\n            self.U_ = cholesky(R)\n\n        Uy = solve(self.U_, y)\n        Uone = solve(self.U_, one)\n\n        Rinv_y = solve(self.U_.T, Uy)\n        Rinv_one = solve(self.U_.T, Uone)\n\n        self.mu_ = float((one @ Rinv_y) / (one @ Rinv_one))\n\n        r = y - one * self.mu_\n        Ur = solve(self.U_, r)\n        Rinv_r = solve(self.U_.T, Ur)\n\n        self.sigma2_ = float((r @ Rinv_r) / n)\n\n        # Store for prediction\n        self.Rinv_one_ = Rinv_one\n        self.Rinv_r_ = Rinv_r\n\n        return self\n\n    def predict(self, X: np.ndarray, return_std: bool = False) -&gt; np.ndarray:\n        \"\"\"Predict using the Kriging model.\n\n        Args:\n            X (ndarray): Points to predict at, shape (n_samples, n_features).\n            return_std (bool, optional): If True, return standard deviations as well.\n                Defaults to False.\n\n        Returns:\n            ndarray or tuple: If return_std is False, returns predicted values (n_samples,).\n                If return_std is True, returns tuple of (predictions, std_devs) both shape (n_samples,).\n        \"\"\"\n        X = np.atleast_2d(X)\n\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        if X.shape[1] != self.X_.shape[1]:\n            raise ValueError(\n                f\"X has {X.shape[1]} features, expected {self.X_.shape[1]}\"\n            )\n\n        n_pred = X.shape[0]\n        predictions = np.zeros(n_pred)\n\n        if return_std:\n            std_devs = np.zeros(n_pred)\n\n        for i, x in enumerate(X):\n            # Build correlation vector\n            psi = self._build_correlation_vector(x, self.X_, self.theta_)\n\n            # Predict mean\n            predictions[i] = self.mu_ + psi @ self.Rinv_r_\n\n            if return_std:\n                # Predict variance\n                Upsi = solve(self.U_, psi)\n                psi_Rinv_psi = psi @ solve(self.U_.T, Upsi)\n\n                variance = self.sigma2_ * (1.0 + self._get_noise() - psi_Rinv_psi)\n                std_devs[i] = np.sqrt(max(0.0, variance))\n\n        if return_std:\n            return predictions, std_devs\n        return predictions\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the Kriging model to training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training input data, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training target values, shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>Kriging</code> <code>Kriging</code> <p>Fitted estimator (self).</p> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"Kriging\":\n    \"\"\"Fit the Kriging model to training data.\n\n    Args:\n        X (ndarray): Training input data, shape (n_samples, n_features).\n        y (ndarray): Training target values, shape (n_samples,).\n\n    Returns:\n        Kriging: Fitted estimator (self).\n    \"\"\"\n    X = np.atleast_2d(X)\n    y = np.asarray(y).flatten()\n\n    if X.ndim != 2:\n        raise ValueError(f\"X must be 2-dimensional, got shape {X.shape}\")\n    if y.ndim != 1:\n        raise ValueError(f\"y must be 1-dimensional, got shape {y.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(f\"X and y must have same number of samples\")\n\n    self.X_ = X\n    self.y_ = y\n    n, k = X.shape\n\n    # Set number of theta parameters\n    if self.n_theta is None:\n        self.n_theta = k\n\n    # Optimize theta via maximum likelihood\n    bounds = [(self.min_theta, self.max_theta)] * self.n_theta\n\n    result = differential_evolution(\n        func=self._neg_log_likelihood,\n        bounds=bounds,\n        seed=self.seed,\n        maxiter=100,\n        atol=1e-6,\n        tol=0.01,\n    )\n\n    self.theta_ = result.x\n\n    # Compute final model parameters\n    one = np.ones(n)\n    R = self._build_correlation_matrix(X, self.theta_)\n\n    try:\n        self.U_ = cholesky(R)\n    except LinAlgError:\n        # Add more regularization if needed\n        R = self._build_correlation_matrix(X, self.theta_)\n        R += np.eye(n) * 1e-8\n        self.U_ = cholesky(R)\n\n    Uy = solve(self.U_, y)\n    Uone = solve(self.U_, one)\n\n    Rinv_y = solve(self.U_.T, Uy)\n    Rinv_one = solve(self.U_.T, Uone)\n\n    self.mu_ = float((one @ Rinv_y) / (one @ Rinv_one))\n\n    r = y - one * self.mu_\n    Ur = solve(self.U_, r)\n    Rinv_r = solve(self.U_.T, Ur)\n\n    self.sigma2_ = float((r @ Rinv_r) / n)\n\n    # Store for prediction\n    self.Rinv_one_ = Rinv_one\n    self.Rinv_r_ = Rinv_r\n\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.predict","title":"<code>predict(X, return_std=False)</code>","text":"<p>Predict using the Kriging model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Points to predict at, shape (n_samples, n_features).</p> required <code>return_std</code> <code>bool</code> <p>If True, return standard deviations as well. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray or tuple: If return_std is False, returns predicted values (n_samples,). If return_std is True, returns tuple of (predictions, std_devs) both shape (n_samples,).</p> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def predict(self, X: np.ndarray, return_std: bool = False) -&gt; np.ndarray:\n    \"\"\"Predict using the Kriging model.\n\n    Args:\n        X (ndarray): Points to predict at, shape (n_samples, n_features).\n        return_std (bool, optional): If True, return standard deviations as well.\n            Defaults to False.\n\n    Returns:\n        ndarray or tuple: If return_std is False, returns predicted values (n_samples,).\n            If return_std is True, returns tuple of (predictions, std_devs) both shape (n_samples,).\n    \"\"\"\n    X = np.atleast_2d(X)\n\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    if X.shape[1] != self.X_.shape[1]:\n        raise ValueError(\n            f\"X has {X.shape[1]} features, expected {self.X_.shape[1]}\"\n        )\n\n    n_pred = X.shape[0]\n    predictions = np.zeros(n_pred)\n\n    if return_std:\n        std_devs = np.zeros(n_pred)\n\n    for i, x in enumerate(X):\n        # Build correlation vector\n        psi = self._build_correlation_vector(x, self.X_, self.theta_)\n\n        # Predict mean\n        predictions[i] = self.mu_ + psi @ self.Rinv_r_\n\n        if return_std:\n            # Predict variance\n            Upsi = solve(self.U_, psi)\n            psi_Rinv_psi = psi @ solve(self.U_.T, Upsi)\n\n            variance = self.sigma2_ * (1.0 + self._get_noise() - psi_Rinv_psi)\n            std_devs[i] = np.sqrt(max(0.0, variance))\n\n    if return_std:\n        return predictions, std_devs\n    return predictions\n</code></pre>"}]}